{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def read_emaxx(input_path: str, start_date: pd.Period, end_date: pd.Period):\n",
    "    \n",
    "    period_range = pd.period_range(start_date, end_date, freq='Q')\n",
    "    prefix = 'eMAXX\\\\ASCII_NA_Mkt_All_Pipe_RY_'\n",
    "    \n",
    "    holding_cols = [\n",
    "        'CUSIP',\n",
    "        'CUSIPSUFF',\n",
    "        'FUNDID',\n",
    "        'PARAMT',\n",
    "        'FIRMID',\n",
    "    ]\n",
    "    df_emaxx_holding = []\n",
    "    \n",
    "    fund_cols = [\n",
    "        'FUNDID',\n",
    "        'FUNDCLASS',\n",
    "        # 'FUNDNAME',\n",
    "        # 'SECTOR',\n",
    "        # 'FIRMID',\n",
    "    ]\n",
    "    df_emaxx_fund = []\n",
    "    \n",
    "    secmast_cols = [\n",
    "        'ISSUERCUS',\n",
    "        'ISSUESUF',\n",
    "        'CPNRATE',\n",
    "        'MATDATE',\n",
    "    ]\n",
    "    df_emaxx_secmast = []\n",
    "\n",
    "    issuer_cols = [\n",
    "        'ISSUERCUS',\n",
    "        # 'ISSUERNAM',\n",
    "        'GEOCODE',\n",
    "        'ENTITY',\n",
    "        'STATE'\n",
    "    ]\n",
    "    df_emaxx_issuer = []\n",
    "    \n",
    "    broktran_cols = [\n",
    "        'CUSIP',\n",
    "        'TRANDATE',\n",
    "        'ACTCOST',\n",
    "        'PARAMT',\n",
    "    ]\n",
    "    df_emaxx_broktran = []\n",
    "    \n",
    "    for q in period_range:\n",
    "        path = os.path.join(input_path, prefix + str(q))\n",
    "        \n",
    "        holding_path = os.path.join(path, 'HOLDING.txt')\n",
    "        holding_data = (pd.read_csv(filepath_or_buffer=holding_path,\n",
    "                                    delimiter='|',\n",
    "                                    usecols=holding_cols,\n",
    "                                    low_memory=False)\n",
    "                        .assign(date=q))\n",
    "        df_emaxx_holding.append(holding_data)\n",
    "        \n",
    "        fund_path = os.path.join(path, 'FUND.txt')\n",
    "        fund_data = (pd.read_csv(filepath_or_buffer=fund_path,\n",
    "                                 delimiter='|',\n",
    "                                 usecols=fund_cols,\n",
    "                                 low_memory=False)\n",
    "                     .assign(date=q))\n",
    "        df_emaxx_fund.append(fund_data)\n",
    "        \n",
    "        # secmast_path = os.path.join(path, 'SECMAST.txt')\n",
    "        # secmast_data = (pd.read_csv(filepath_or_buffer=secmast_path,\n",
    "        #                             delimiter='|',\n",
    "        #                             usecols=secmast_cols,\n",
    "        #                             low_memory=False)\n",
    "        #                 .assign(date=q))\n",
    "        # df_emaxx_secmast.append(secmast_data)\n",
    "        \n",
    "        # issuer_path = os.path.join(path, 'ISSUERS.txt')\n",
    "        # issuer_data = (pd.read_csv(filepath_or_buffer=issuer_path,\n",
    "        #                          delimiter='|',\n",
    "        #                          usecols=issuer_cols,\n",
    "        #                          low_memory=False)\n",
    "        #              .assign(date=q))\n",
    "        # df_emaxx_issuer.append(issuer_data)\n",
    "        \n",
    "        broktran_path = os.path.join(path, 'BROKTRAN.txt')\n",
    "        broktran_data = (pd.read_csv(filepath_or_buffer=broktran_path,\n",
    "                                     delimiter='|',\n",
    "                                     usecols=broktran_cols,\n",
    "                                     low_memory=False)\n",
    "                         .assign(date=q)\n",
    "                         .sort_values('TRANDATE')\n",
    "                         .drop_duplicates('CUSIP', keep='last')\n",
    "                         .drop(columns='TRANDATE'))\n",
    "        df_emaxx_broktran.append(broktran_data)\n",
    "        print('Imported  ', q)\n",
    "    \n",
    "    df_emaxx_holding = pd.concat(df_emaxx_holding)\n",
    "    df_emaxx_fund = pd.concat(df_emaxx_fund)\n",
    "    # df_emaxx_secmast = pd.concat(df_emaxx_secmast)\n",
    "    # df_emaxx_issuer = pd.concat(df_emaxx_issuer)\n",
    "    df_emaxx_broktran = pd.concat(df_emaxx_broktran)\n",
    "    \n",
    "    return df_emaxx_holding, df_emaxx_fund, df_emaxx_secmast, df_emaxx_issuer, df_emaxx_broktran\n",
    "\n",
    "\n",
    "def pandas_read_bond(path: str, start_date: pd.Period, end_date: pd.Period):\n",
    "    df_emaxx_holding, df_emaxx_fund, df_emaxx_secmast, df_emaxx_issuer, df_emaxx_broktran = read_emaxx(path, start_date, end_date)\n",
    "    log_import_emaxx_holding(df_emaxx_holding)\n",
    "    log_import_emaxx_fund(df_emaxx_fund)\n",
    "    # log_import_emaxx_secmast(df_emaxx_secmast)\n",
    "    # log_import_emaxx_issuer(df_emaxx_issuer)\n",
    "    log_import_emaxx_broktran(df_emaxx_broktran)\n",
    "    \n",
    "    df_fed = pd.read_csv(filepath_or_buffer=os.path.join(path, 'FED', 'fed_treasury_holding.csv'))\n",
    "    log_import_fed(df_fed)\n",
    "    \n",
    "    df_treasury = pd.read_csv(filepath_or_buffer=os.path.join(path, 'Auctions_TreasurySecurities.csv'))\n",
    "    log_import_treasury(df_treasury)\n",
    "    \n",
    "    columns = [\n",
    "        'DATE',\n",
    "        'CUSIP',\n",
    "        'AMOUNT_OUTSTANDING',\n",
    "        'MATURITY',\n",
    "        'DURATION',\n",
    "        'COUPON',\n",
    "        'YIELD',\n",
    "        'PRICE_EOM',\n",
    "        'RET_EOM',\n",
    "        'T_Spread'\n",
    "    ]\n",
    "    df_wrds_bond = pd.read_csv(filepath_or_buffer=os.path.join(path, 'wrds_bond.csv'),\n",
    "                               usecols=columns,\n",
    "                               low_memory=False)\n",
    "    log_import_wrds_bond(df_wrds_bond)\n",
    "\n",
    "    return df_emaxx_holding, df_emaxx_fund, df_emaxx_secmast, df_emaxx_issuer, df_emaxx_broktran, df_fed, df_treasury, df_wrds_bond\n",
    "\n",
    "\n",
    "def clean_imports_bond(df_emaxx_holding: pd.DataFrame,\n",
    "                       df_emaxx_fund: pd.DataFrame,\n",
    "                       df_emaxx_secmast: pd.DataFrame,\n",
    "                       df_emaxx_issuer: pd.DataFrame,\n",
    "                       df_emaxx_broktran: pd.DataFrame,\n",
    "                       df_fed: pd.DataFrame,\n",
    "                       df_treasury: pd.DataFrame,\n",
    "                       df_wrds_bond: pd.DataFrame,\n",
    "                       start_date: pd.Period,\n",
    "                       end_date: pd.Period) -> tuple:\n",
    "\n",
    "    df_emaxx_holding_clean = clean_emaxx_holding(df_emaxx_holding)\n",
    "    log_clean_emaxx_holding(df_emaxx_holding_clean)\n",
    "\n",
    "    df_emaxx_fund_clean = clean_emaxx_fund(df_emaxx_fund)\n",
    "    log_clean_emaxx_fund(df_emaxx_fund_clean)\n",
    "    \n",
    "    # df_emaxx_secmast_clean = clean_emaxx_secmast(df_emaxx_secmast)\n",
    "    # log_clean_emaxx_secmast(df_emaxx_secmast_clean)\n",
    "    \n",
    "    # df_emaxx_issuer_clean = clean_emaxx_issuer(df_emaxx_issuer)\n",
    "    # log_clean_emaxx_issuer(df_emaxx_issuer_clean)\n",
    "    \n",
    "    df_emaxx_broktran_clean = clean_emaxx_broktran(df_emaxx_broktran)\n",
    "    log_clean_emaxx_broktran(df_emaxx_broktran_clean)\n",
    "    \n",
    "    df_fed_clean = clean_fed_holding(df_fed)\n",
    "    log_clean_fed(df_fed_clean)\n",
    "    \n",
    "    df_treasury_clean = clean_treasury(df_treasury)\n",
    "    log_clean_treasury(df_treasury_clean)\n",
    "    \n",
    "    df_wrds_bond_clean = clean_wrds_bond(df_wrds_bond, start_date, end_date)\n",
    "    log_clean_wrds_bond(df_wrds_bond_clean)\n",
    "    \n",
    "    return df_emaxx_holding_clean, df_emaxx_fund_clean, df_emaxx_broktran_clean, df_fed_clean, df_treasury_clean, df_wrds_bond_clean\n",
    "\n",
    "\n",
    "def clean_emaxx_holding(df_emaxx_holding: pd.DataFrame) -> pd.DataFrame:    \n",
    "    df_emaxx_holding.columns = df_emaxx_holding.columns.str.lower()\n",
    "    return (df_emaxx_holding\n",
    "            .rename(columns={'cusip': 'issuer_id',\n",
    "                             'cusipsuff': 'issue',\n",
    "                             'fundid':'inv_id',\n",
    "                             't_spread': 'spread'})\n",
    "            .astype({'issuer_id': str})\n",
    "            .assign(asset_id=lambda x: x['issuer_id'] + x['issue'])\n",
    "            .dropna(subset=['asset_id'])\n",
    "            .drop(columns=['issue'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_fund(df_emaxx_fund: pd.DataFrame) -> pd.DataFrame:  \n",
    "    df_emaxx_fund.columns = df_emaxx_fund.columns.str.lower()\n",
    "    return (df_emaxx_fund\n",
    "            .rename(columns={'fundid': 'inv_id'})\n",
    "            .dropna(subset=['fundclass'])\n",
    "            .assign(typecode=lambda x: x['fundclass'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_secmast(df_emaxx_secmast: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_emaxx_secmast.columns = df_emaxx_secmast.columns.str.lower()\n",
    "    return (df_emaxx_secmast\n",
    "            .rename(columns={'cpnrate': 'coupon'})\n",
    "            .loc[lambda x: x['matdate'] != 99990101]\n",
    "            .dropna(subset=['issuercus', 'issuesuf'])\n",
    "            .assign(asset_id=lambda x: x['issuercus'] + x['issuesuf'],\n",
    "                    matdate=lambda x: pd.to_datetime(x['matdate'], format='%Y%m%d').dt.to_period('Q'),\n",
    "                    maturity=lambda x: (x['date'] - x['matdate']).apply(lambda y: y.n))\n",
    "            .drop(columns=['cusip', 'cusipsuff'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_issuer(df_emaxx_issuer: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_emaxx_issuer.columns = df_emaxx_issuer.columns.str.lower()\n",
    "    return (df_emaxx_issuer\n",
    "            .dropna(subset=['issuercus', 'geocode', 'entity'])\n",
    "            .rename(columns={'issuercus': 'issuer_id'})\n",
    "            .astype({'issuer_id': str})\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_broktran(df_emaxx_broktran: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_emaxx_broktran.columns = df_emaxx_broktran.columns.str.lower()\n",
    "    return (df_emaxx_broktran\n",
    "            .rename(columns={'cusip': 'asset_id'})\n",
    "            .assign(prc=lambda x: x['actcost'] / x['paramt'])\n",
    "            .drop(columns=['actcost', 'paramt'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_fed_holding(df_fed_holding: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_fed_holding\n",
    "            .rename(columns={'cusip': 'asset_id',\n",
    "                             'maturityDate': 'maturity',\n",
    "                             'parValue': 'paramt'})\n",
    "            .assign(inv_id=-1,\n",
    "                    typecode='FED'))\n",
    "\n",
    "\n",
    "def clean_treasury(df_treasury: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_treasury.columns = df_treasury.columns.str.lower()\n",
    "    df_treasury.columns = df_treasury.columns.str.replace(' ', '_')\n",
    "    return (df_treasury\n",
    "            .rename(columns={'cusip':'asset_id',\n",
    "                             'interest_rate': 'coupon'})\n",
    "            .loc[lambda x: x['floating_rate'] == 'No']\n",
    "            .assign(coupon=lambda x: x['coupon'].fillna(0),\n",
    "                    amtout=lambda x: x['currently_outstanding'].fillna(x['total_accepted']))\n",
    "            .dropna(subset=['amtout', 'coupon'])\n",
    "            .sort_values('issue_date')\n",
    "            .drop_duplicates('asset_id', keep='last')\n",
    "            .drop(columns=['floating_rate']))\n",
    "\n",
    "\n",
    "def clean_wrds_bond(df_wrds_bond: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    df_wrds_bond.columns = df_wrds_bond.columns.str.lower()\n",
    "    return (df_wrds_bond\n",
    "            .rename(columns={'cusip': 'asset_id',\n",
    "                             'fundid': 'inv_id',\n",
    "                             'price_eom': 'prc',\n",
    "                             'ret_eom': 'ret',\n",
    "                             'amount_outstanding': 'amtout'})\n",
    "            .assign(date=lambda x: fix_date_quarterly(x['date']),\n",
    "                    asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .dropna(subset=['asset_id', 'prc'])\n",
    "            .drop_duplicates(subset=['date', 'asset_id'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def fix_date_quarterly(dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(dates).dt.to_period(freq='Q')\n",
    "\n",
    "\n",
    "def fix_date_monthly(dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(dates).dt.to_period(freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bonds Monthly\n",
    "\n",
    "def calc_off_the_run(df_treasury_clean: pd.DataFrame, end_date: pd.Period) -> pd.DataFrame:\n",
    "    return (df_treasury_clean\n",
    "            .sort_values('issue_date')\n",
    "            .assign(otf_date=(df_treasury_clean\n",
    "                              .groupby(['security_term'])\n",
    "                              ['issue_date']\n",
    "                              .shift(periods=-1,\n",
    "                                     fill_value=end_date + 1))))\n",
    "\n",
    "def calc_factors(df_wrds_bond_clean: pd.DataFrame, df_emaxx_secmast_clean: pd.DataFrame, df_otr: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_emaxx_secmast_clean\n",
    "            .merge(right=df_otr,\n",
    "                   how='left',\n",
    "                   on='asset_id')\n",
    "            .assign(on_the_run=lambda x: x['otr_date'] > x['date'],\n",
    "                    maturity=lambda x: x['maturity'] - x['date']))\n",
    "\n",
    "\n",
    "def merge_price_factor(df_emaxx_broktran_clean: pd.DataFrame, df_factor:pd.DataFrame) -> pd.DataFrame:\n",
    "    df_asset = pd.merge(\n",
    "        left=df_emaxx_broktran_clean,\n",
    "        right=df_factor,\n",
    "        how='left',\n",
    "        on='asset_id')\n",
    "\n",
    "    log_price_factor(df_asset)\n",
    "    return df_asset\n",
    "\n",
    "\n",
    "def merge_fed_holding(df_emaxx_holding_clean: pd.DataFrame, df_fed_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fed_holding = pd.concat([df_emaxx_holding_clean, df_fed_clean])\n",
    "    log_fed_holding(df_fed_holding)\n",
    "    return df_fed_holding\n",
    "\n",
    "def merge_holding_fund(df_emaxx_holding_clean: pd.DataFrame, df_emaxx_fund_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_holding_fund = pd.merge(\n",
    "        left=df_emaxx_holding_clean,\n",
    "        right=df_emaxx_fund_clean,\n",
    "        how='left',\n",
    "        on=['date', 'inv_id'])\n",
    "\n",
    "    log_holding_fund_merge(df_holding_fund)\n",
    "    return df_holding_fund\n",
    "\n",
    "\n",
    "def construct_zero_holdings(df_fund_manager: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    \n",
    "    def calc_inv_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.groupby('inv_id')['date'].transform('max') - df['date']).apply(lambda x: x.n)\n",
    "        min_diff = np.minimum(date_diff, n_quarters) + 1\n",
    "        return min_diff\n",
    "    \n",
    "    def calc_asset_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.sort_values('date')\n",
    "                     .groupby(['inv_id', 'asset_id'])\n",
    "                     ['date']\n",
    "                     .diff(periods=1)\n",
    "                     .fillna(pd.DateOffset(n=1))\n",
    "                     .apply(lambda x: x.n))\n",
    "        min_diff = np.minimum(date_diff, df['inv_obs'])\n",
    "        return min_diff.apply(lambda x: list(range(x)))\n",
    "    \n",
    "    \n",
    "    df_holding = (df_fund_manager\n",
    "                  .assign(inv_obs=lambda x: calc_inv_obs(x),\n",
    "                          asset_obs=lambda x: calc_asset_obs(x))\n",
    "                  .explode('asset_obs')\n",
    "                  .assign(asset_obs=lambda x: x['asset_obs'].astype('int8'),\n",
    "                          mask=lambda x: x['asset_obs'] == 0,\n",
    "                          paramt=lambda x: x['paramt'] * x['mask'],\n",
    "                          date=lambda x: x['date'] + x['asset_obs'])\n",
    "                  .drop(columns=['inv_obs', 'asset_obs', 'mask']))\n",
    "    \n",
    "    log_zero_holdings(df_holding)\n",
    "    return df_holding\n",
    "\n",
    "\n",
    "def merge_holding_factor(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_holding,\n",
    "        right=df_asset,\n",
    "        how='left',\n",
    "        on=['date', 'asset_id'])\n",
    "\n",
    "    df_holding_factor = (df_merged\n",
    "                         .assign(holding=lambda x: x['prc'] * x['paramt'],\n",
    "                                 me=lambda x: x['prc'] * x['amtout'])\n",
    "                         .dropna(subset='holding'))\n",
    "    \n",
    "    log_holding_factor_merge(df_holding_factor)\n",
    "    return df_holding_factor\n",
    "\n",
    "\n",
    "def drop_unsused_inv_asset(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_dropped = (df_holding_factor\n",
    "                  .assign(Tinv_paramt=lambda x: x.groupby(['inv_id', 'date'])['paramt'].transform('sum'),\n",
    "                          Tasset_paramt=lambda x: x.groupby(['date', 'asset_id'])['paramt'].transform('sum'),\n",
    "                          mask=lambda x: (x['Tinv_paramt'] == 0) | (x['Tasset_paramt'] == 0))\n",
    "                  .loc[lambda x: ~x['mask']]\n",
    "                  .drop(columns=['Tinv_paramt', 'Tasset_paramt', 'mask']))\n",
    "    \n",
    "    log_drop_unused(df_dropped)\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def create_household_sector(df_dropped: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_dropped\n",
    "                      .groupby(['date', 'asset_id'], as_index=False)\n",
    "                      .agg({\n",
    "                        'paramt': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'amtout': 'last',\n",
    "                        # 'paramt': 'sum',\n",
    "                        'holding': 'sum',\n",
    "                        'me': 'last'})\n",
    "                      .assign(paramt=lambda x: np.maximum(x['amtout'] - x['paramt'], 0),\n",
    "                              holding=lambda x: np.maximum(x['me'] - x['holding'], 0),\n",
    "                              inv_id=0,\n",
    "                              typecode='HOU'))\n",
    "    \n",
    "    log_household_sector(df_household)\n",
    "    df_concat = pd.concat([df_dropped, df_household])\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def get_treasury_id(df_emaxx_issuer_clean: pd.DataFrame) -> np.array:\n",
    "    df_t = (df_emaxx_issuer_clean\n",
    "            .assign(usa_mask=lambda x: x['geocode'] == 'USA',\n",
    "                    gov_mask=lambda x: x['entity'] == 'GT',\n",
    "                    treasury_mask=lambda x: x['usa_mask'] & x['gov_mask'])\n",
    "            .loc[lambda x: x['treasury_mask']])\n",
    "    return df_t['issuer_id'].unique()\n",
    "\n",
    "\n",
    "def partition_outside_asset(df_household: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df_outside = (df_household\n",
    "                  .assign(out_mask=lambda x: x.isna().any(axis=1),\n",
    "                          out_holding=lambda x: x['holding'] * x['out_mask']))\n",
    "    \n",
    "    log_outside_asset(df_outside)\n",
    "    return df_outside\n",
    "\n",
    "\n",
    "def calc_inv_aum(df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_inv_aum = (df_outside\n",
    "                  .assign(aum=lambda x: x.groupby(['inv_id', 'date'])['holding'].transform('sum'))\n",
    "                  .loc[lambda x: x['aum'] > 0]\n",
    "                  .assign(out_aum=lambda x: x.groupby(['inv_id', 'date'])['out_holding'].transform('sum'),\n",
    "                          out_weight=lambda x: x['out_aum'] / x['aum']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def agg_small_inv(df_inv_aum: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_agg = (df_inv_aum\n",
    "              .assign(small_mask=lambda x: (x['aum'] < 10) | (x['out_weight'] == 0) | (x['out_weight'] == 1),\n",
    "                      inv_id=lambda x: ~x['small_mask'] * x['inv_id'],\n",
    "                      hh_mask=lambda x: x['inv_id'] == 0))\n",
    "\n",
    "    df_grouped = df_agg.groupby(['inv_id', 'date', 'asset_id'])\n",
    "    \n",
    "    df_agg = (df_agg\n",
    "              .assign(Tparamt=lambda x: df_grouped['paramt'].transform('sum'),\n",
    "                      Tholding=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                      paramt=lambda x: np.where(x['hh_mask'], x['Tparamt'], x['paramt']),\n",
    "                      holding=lambda x: np.where(x['hh_mask'], x['Tholding'], x['holding']))\n",
    "              .drop(columns=['Tparamt', 'Tholding']))\n",
    "    \n",
    "    df_grouped = df_agg.groupby(['inv_id', 'date'])\n",
    "    \n",
    "    df_agg = (df_agg\n",
    "              .assign(Taum=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                      Tout_aum=lambda x: df_grouped['out_holding'].transform('sum'),\n",
    "                      aum=lambda x: np.where(x['hh_mask'], x['Taum'], x['aum']),\n",
    "                      out_aum=lambda x: np.where(x['hh_mask'], x['Tout_aum'], x['out_aum']),\n",
    "                      x_holding=lambda x: df_grouped['holding'].transform('count'),\n",
    "                      n_holding=lambda x: x['x_holding'] - df_grouped['out_mask'].transform('sum'),\n",
    "                      equal_alloc=lambda x: ~x['hh_mask'] * x['aum'] / (1 + x['x_holding']),\n",
    "                      drop_mask=lambda x: x['small_mask'] | x['out_mask'])\n",
    "              .loc[lambda x: ~x['drop_mask']]\n",
    "              .drop(columns=['Taum', 'Tout_aum', 'out_mask', 'out_holding', 'out_weight', 'small_mask', 'drop_mask', 'hh_mask']))\n",
    "    \n",
    "    log_agg_small_inv(df_agg)\n",
    "    return df_agg\n",
    "    \n",
    "\n",
    "def bin_inv(df_inv_aum: pd.DataFrame, min_n_holding: int) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    def calc_bin(df_type_date: pd.DataFrame) -> pd.Series:\n",
    "        typecode = df_type_date['typecode'].iloc[0]\n",
    "        n_holding = (df_type_date['holding'] > 0).sum()\n",
    "        n_bins = np.floor(n_holding / (2 * min_n_holding)).astype(int)\n",
    "        idx = df_type_date.index\n",
    "        \n",
    "        if (n_bins <= 1) | (n_holding >= min_n_holding) | (typecode == 0) | (typecode == 'FED'):\n",
    "            return pd.Series(0, index=idx)\n",
    "        else:\n",
    "            return pd.Series(pd.qcut(x=df_type_date['aum'], q=n_bins, labels=False), index=idx)\n",
    "    \n",
    "    \n",
    "    df_binned = (df_inv_aum\n",
    "                 .reset_index(drop=True)\n",
    "                 .assign(bin=lambda x: x.groupby(['typecode', 'date']).apply(calc_bin).reset_index(drop=True))\n",
    "                 .assign(bin=lambda x: x.groupby(['date', 'typecode', 'bin']).ngroup()))\n",
    "    \n",
    "    log_bins(df_binned)\n",
    "    return df_binned\n",
    "\n",
    "\n",
    "def calc_instrument(df_binned: pd.DataFrame) -> pd.DataFrame:    \n",
    "    df_instrument = (df_binned\n",
    "                     .assign(total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum'),\n",
    "                             iv_me=lambda x: x['total_alloc'] - x['equal_alloc'])\n",
    "                     .drop(columns=['total_alloc', 'equal_alloc']))\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def dask_calc_instrument(df_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_binned = dd.from_pandas(df_binned.set_index('date'), npartitions=32)\n",
    "    \n",
    "    df_instrument = (df_binned\n",
    "                     .assign(total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum', meta=pd.Series(dtype='float64')),\n",
    "                             iv_me=lambda x: x['total_alloc'] - x['equal_alloc'])\n",
    "                     .drop(columns=['total_alloc', 'equal_alloc'])\n",
    "                     .compute()\n",
    "                     .reset_index())\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = (df_instrument['iv_me'] > 0)\n",
    "\n",
    "    df_weights = (df_instrument.loc[mask]\n",
    "                 .assign(ln_me=lambda x: np.log(x['me']),\n",
    "                         ln_iv_me=lambda x: np.log(x['iv_me']),\n",
    "                         weight=lambda x: x['holding'] / x['aum'],\n",
    "                         rweight=lambda x: x['holding'] / x['out_aum'],\n",
    "                         ln_rweight=lambda x: np.log(x['rweight'].mask(x['rweight'] == 0, np.NaN)),\n",
    "                         mean_ln_rweight=lambda x: x.groupby(['inv_id', 'date'])['ln_rweight'].transform('mean'),\n",
    "                         const=1,\n",
    "                         pct_uni_held=lambda x: np.log(x['n_holding']) - np.log(x['x_holding']))\n",
    "                .drop(columns=['me', 'iv_me']))\n",
    "    \n",
    "    log_holding_weights(df_weights)\n",
    "    return df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def bound_float(arr: np.array) -> float:\n",
    "    ln_bound = 709.7827\n",
    "    return np.minimum(np.maximum(arr, -ln_bound), ln_bound)\n",
    "\n",
    "\n",
    "def momcond_L(params: np.array, exog: np.array) -> np.array:\n",
    "    upper_bound = 0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    mean_ln_rweight = exog[1]\n",
    "    arr_characteristics = exog[2:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = ln_me_term + characteristics_term + mean_ln_rweight\n",
    "    \n",
    "    return pred_weight\n",
    "\n",
    "\n",
    "def fit_inv_date_L(df_inv_date: pd.DataFrame, characteristics: list, params: list,) -> gmm.GMMResults:\n",
    "    df_inv_date = df_inv_date.dropna(subset='ln_rweight')\n",
    "    \n",
    "    exog = np.asarray(df_inv_date[['ln_me', 'mean_ln_rweight'] + characteristics])\n",
    "    instrument = np.asarray(df_inv_date[['ln_iv_me', 'mean_ln_rweight'] + characteristics])\n",
    "    n = exog.shape[0]\n",
    "    endog = np.asarray(df_inv_date['ln_rweight'] - df_inv_date['pct_uni_held'])\n",
    "    start_params = np.zeros(len(params))\n",
    "    w0inv = np.dot(instrument.T, instrument) / n\n",
    "    \n",
    "    try:\n",
    "        model = gmm.NonlinearIVGMM(\n",
    "            endog=endog,\n",
    "            exog=exog,\n",
    "            instrument=instrument, \n",
    "            func=momcond_L)\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=0,\n",
    "            inv_weights=w0inv)\n",
    "        # log_results(result, params)\n",
    "        return result\n",
    "    \n",
    "    except np.linalg.LinAlgError:\n",
    "        print('Linear Algebra Error')\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def estimate_model_L(df_weights: pd.DataFrame, characteristics: list, params: list, min_n_holding: int) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_weights['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_institutions = (df_weights\n",
    "                       .loc[mask]\n",
    "                       .set_index(['inv_id', 'date'])\n",
    "                       .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                       .reset_index())\n",
    "    \n",
    "    df_bins = (df_weights\n",
    "               .loc[~mask]\n",
    "               .set_index(['bin', 'date'])\n",
    "               .assign(gmm_result=lambda x: x.groupby(['bin', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "               .reset_index())\n",
    "    \n",
    "    df_model = pd.concat([df_institutions, df_bins])\n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def momcond_NL(params: np.array, exog: np.array) -> np.array:\n",
    "    upper_bound = 0.9999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = bound_float(params[0])\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    rweight = exog[1]\n",
    "    mean_ln_rweight = exog[2]\n",
    "    arr_characteristics = exog[3:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_ln_rweight = bound_float(ln_me_term + characteristics_term + mean_ln_rweight)\n",
    "    pred_rweight = np.exp(-1 * pred_ln_rweight)\n",
    "    \n",
    "    return rweight * pred_rweight\n",
    "\n",
    "\n",
    "def fit_inv_date_NL(df_inv_date: pd.DataFrame, characteristics: list, params: list) -> gmm.GMMResults:\n",
    "    exog = np.asarray(df_inv_date[['ln_me', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "    instrument = np.asarray(df_inv_date[['ln_iv_me', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "    n = exog.shape[0]\n",
    "    endog = np.ones(n)\n",
    "    \n",
    "    try:\n",
    "        model = gmm.NonlinearIVGMM(\n",
    "            endog=endog,\n",
    "            exog=exog,\n",
    "            instrument=instrument, \n",
    "            func=momcond_NL)\n",
    "        w0inv = np.dot(instrument.T, instrument) / n\n",
    "        start_params = np.zeros(len(params))\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=100,\n",
    "            inv_weights=w0inv)\n",
    "        # log_results(result, params)\n",
    "        return result\n",
    "    \n",
    "    except np.linalg.LinAlgError:\n",
    "        print('Linear Algebra Error')\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def estimate_model_NL(df_weights: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_weights['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_institutions = (df_weights\n",
    "                       .loc[mask]\n",
    "                       .set_index(['inv_id', 'date'])\n",
    "                       .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                       .reset_index())\n",
    "    \n",
    "    df_bins = (df_weights\n",
    "               .loc[~mask]\n",
    "               .set_index(['bin', 'date'])\n",
    "               .assign(gmm_result=lambda x: x.groupby(['bin', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "               .reset_index())\n",
    "    \n",
    "    df_model = pd.concat([df_institutions, df_bins])\n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def calc_latent_demand_L(df_model: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    def unpack_result(result: gmm.GMMResults) -> list:\n",
    "        return result.params\n",
    "\n",
    "    def unpack_params(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[params] = pd.DataFrame(df['lst_params'].tolist(), index=df.index)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    upper_bound = 0.9999\n",
    "    \n",
    "    df_results = (df_model\n",
    "                  .dropna()\n",
    "                  .assign(lst_params=lambda x: x['gmm_result'].apply(unpack_result))\n",
    "                  .pipe(unpack_params)\n",
    "                  .drop(columns='lst_params')\n",
    "                  .assign(beta_ln_me=lambda x: upper_bound - np.exp(-1 * x['beta_ln_me']),\n",
    "                          beta_const=lambda x: x['beta_const'] + x['mean_ln_rweight'],\n",
    "                          pred_ln_rweight=lambda x: np.einsum('ij,ij->i', x[['ln_me'] + characteristics], x[params]),\n",
    "                          latent_demand=lambda x: x['ln_rweight'] - x['pred_ln_rweight'])\n",
    "                  .drop(columns=['pct_uni_held', 'gmm_result', 'pred_ln_rweight']))\n",
    "    \n",
    "    log_latent_demand(df_results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Liquidity\n",
    "\n",
    "def calc_coliquidity_matrix(df_date: pd.DataFrame) -> pd.Series:\n",
    "    df_asset_grouped = df_date.groupby('asset_id')\n",
    "    n_assets = len(df_asset_grouped)\n",
    "    print(n_assets)\n",
    "\n",
    "    df_liquid = (df_date\n",
    "                 .assign(first=lambda x: df_asset_grouped.cumcount() == 0,\n",
    "                         total_holding=lambda x: df_asset_grouped['holding'].transform('sum'),\n",
    "                         share_of_holding=lambda x: x['holding'] / x['total_holding'],\n",
    "                         row=lambda x: df_asset_grouped.ngroup())\n",
    "                 .sort_values('row'))\n",
    "    \n",
    "    Zmat = np.empty(shape=(n_assets, n_assets))\n",
    "    Amat = np.empty(shape=(n_assets, n_assets))\n",
    "    \n",
    "    for col in range(n_assets):\n",
    "        df_liquid = df_liquid.assign(mask=lambda x: x['row'] == col,\n",
    "                                     mask_weight=lambda x: x['mask'] * x['weight'],\n",
    "                                     rcweight = lambda x: x['mask'] - x.groupby('inv_id')['mask_weight'].transform('max'),\n",
    "                                     Zcol=lambda x: x['beta_ln_me'] * x['share_of_holding'] * x['rcweight'],\n",
    "                                     Acol=lambda x: x['share_of_holding'] * x['rcweight'])\n",
    "        Zmat[col] = df_liquid.groupby('asset_id')['Zcol'].sum()\n",
    "        Amat[col] = df_liquid.groupby('asset_id')['Acol'].sum()\n",
    "        print('Added asset ', col)\n",
    "\n",
    "    # types = df_date['typecode'].unique()\n",
    "    # for i, t in enumerate(types):\n",
    "    #     Bcol_name = 'Bcol_' + str(t)        \n",
    "    #     mask = df_liquid['typecode'] == t\n",
    "    #     df_liquid[Bcol_name] = mask * df_liquid['Acol']\n",
    "    # \n",
    "    # for t in types:\n",
    "    #     Bcol_name = 'Bcol_' + str(t)\n",
    "    #     \n",
    "    #     n_type = df_asset_grouped.apply(lambda x: (x['typecode'] == t).sum())\n",
    "    #     df_matrix[Bcol_name] = df_asset_grouped[Bcol_name].sum() / np.maximum(n_type, 1)\n",
    "    \n",
    "    I = np.identity(n_assets)\n",
    "    liquidity_matrixA = np.linalg.solve(I - Zmat, Amat)\n",
    "    price_impact = np.diagonal(liquidity_matrixA)\n",
    "    \n",
    "    idx = df_liquid.set_index('row')['asset_id'].to_dict()\n",
    "    sr_liquid = pd.Series(price_impact, index=idx, name='price_impact')\n",
    "    \n",
    "    return sr_liquid\n",
    "\n",
    "\n",
    "def calc_price_elasticity(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = df_results['holding'] > 0\n",
    "    \n",
    "    df_liquidity = (df_results\n",
    "                    .loc[mask]\n",
    "                    .groupby('date')\n",
    "                    .apply(calc_coliquidity_matrix))\n",
    "    \n",
    "    log_liquidity(df_liquidity)\n",
    "    return df_liquidity\n",
    "    \n",
    "\n",
    "def graph_liquidity(df_liquidity: pd.DataFrame):\n",
    "    df_fig = (df_liquidity\n",
    "              .groupby('date')\n",
    "              .agg({'p10': lambda x: np.quantile(x['price_impact'], q=0.1),\n",
    "                    'p50': lambda x: np.quantile(x['price_impact'], q=0.5),\n",
    "                    'p90': lambda x: np.quantile(x['price_impact'], q=0.9)})\n",
    "              .stack()\n",
    "              .reset_index(name='stat'))\n",
    "    \n",
    "    g = sns.relplot(data=df_fig,\n",
    "                    x='date',\n",
    "                    y='p10',\n",
    "                    hue='stat',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Price Impact')\n",
    "    g.legend()\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'price_elasticity.png'))\n",
    "    return df_liquidity"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "\n",
    "def equillibrium_loop(df_loop: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    for i in range(1000):\n",
    "        df_loop = (df_loop\n",
    "                   .assign(weightN=lambda x: np.exp(x['loop_beta_ln_me'] * x['loop_ln_prc']) + x['loop_latent'],\n",
    "                           weightD=lambda x: x.groupby(['inv_id', 'date'])['weightN'].transform('sum'),\n",
    "                           weight=lambda x: x['weightN'] / (1 + x['weightD']),\n",
    "                           holding=lambda x: x['weight'] * x['loop_aum'],\n",
    "                           demand=lambda x: x.groupby(['inv_id', 'date'])['holding'].transform('sum'),\n",
    "                           Ddemand_a=lambda x: x['loop_beta_ln_me'] * x['loop_aum'] * x['weight'] * (1 - x['weight']) / x['demand'],\n",
    "                           Ddemand_b=lambda x: x.groupby(['date', 'asset_id'])['Ddemand_a'].transform('sum'),\n",
    "                           Ddemand_c=lambda x: 1 / (1 - np.minimum(0, x['Ddemand_b'])),\n",
    "                           gap=lambda x: np.log(x['demand']) - x['loop_ln_price'] - x['loop_ln_amtout']))\n",
    "        \n",
    "        gap = max(abs(df_loop['gap'].min()), abs(df_loop['gap'].max()))\n",
    "        \n",
    "        if i == 999:\n",
    "            print(f'Equilibrium did not converge after 1000 iterations')\n",
    "            print(f'Gap:  ', gap)\n",
    "        \n",
    "        if gap < 0.00001:\n",
    "            print(f'Equilibrium converged after {i} iterations')\n",
    "            df_loop = df_loop.assign(loop_ln_prc=lambda x: x.groupby('asset_id')['loop_ln_prc'].transform('min'))\n",
    "    \n",
    "    return df_loop\n",
    "\n",
    "\n",
    "def run_counterfactuals(df_date: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    for i in range(1, 6):\n",
    "        if i <= 4:\n",
    "            df_date = df_date.assign(loop_ln_prc=lambda x: x['lag_ln_prc'],\n",
    "                                     loop_ln_amtout=lambda x: x['lag_ln_amtout'])\n",
    "        else:\n",
    "            df_date = df_date.assign(loop_ln_prc=lambda x: x['prc'],\n",
    "                                     loop_ln_amtout=lambda x: x['amtout'])\n",
    "    \n",
    "        if i == 1:\n",
    "            for char in characteristics:\n",
    "                df_date['loop_' + char] = df_date['lag_' + char]\n",
    "        elif i <= 4:\n",
    "            for char in characteristics:\n",
    "                df_date['loop_' + char] = df_date['lag_' + char]\n",
    "        else:\n",
    "            for char in characteristics:\n",
    "                df_date['loop_' + char] = df_date[char]\n",
    "                \n",
    "        if i <= 2:\n",
    "            df_date['loop_aum'] = df_date['lag_aum']\n",
    "        elif i <= 4:\n",
    "            df_date['loop_aum'] = df_date['aum']\n",
    "        else:\n",
    "            df_date['loop_aum'] = df_date['aum']\n",
    "        \n",
    "        if i <= 3:\n",
    "            for param in params:\n",
    "                df_date['loop_' + param] = df_date['lag_' + param]\n",
    "        elif i <= 4:\n",
    "            for param in params:\n",
    "                df_date['loop_' + param] = df_date['lag_' + param]\n",
    "        else:\n",
    "            for param in params:\n",
    "                df_date['loop_' + param] = df_date[param]\n",
    "        \n",
    "        loop_chars = ['loop_' + char for char in characteristics]\n",
    "        loop_params = ['loop_' + param for param in params]\n",
    "        if i <= 4:\n",
    "            df_date = df_date.assign(loop_latent=lambda x: np.einsum('ij,ij->i', df_date[['loop_ln_amtout'] + loop_chars], df_date[loop_params]) + x['lag_latent_demand']) \n",
    "        else:\n",
    "            df_date = df_date.assign(loop_latent=lambda x: np.einsum('ij,ij->i', df_date[['loop_ln_amtout'] + loop_chars], df_date[loop_params]) + x['latent_demand'])\n",
    "\n",
    "    return df_date\n",
    "\n",
    "\n",
    "def decompose_variance(df_results: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    mask = df_results['holding'] > 0\n",
    "    \n",
    "    df_variance = (df_results\n",
    "                   .loc[mask]\n",
    "                   .assign(ln_prc=lambda x: np.log(x['prc']),\n",
    "                           ln_amtout=lambda x: np.log(x['amtout'])))\n",
    "    \n",
    "    df_grouped = df_variance.groupby(['date', 'asset_id'])\n",
    "    \n",
    "    df_variance = (df_variance\n",
    "                   .assign(lag_ln_prc=lambda x: df_grouped['ln_prc'].shift(4),\n",
    "                           lag_ln_amtout=lambda x: df_grouped['ln_amtout'].shift(4),\n",
    "                           lag_aum=lambda x: df_grouped['aum'].shift(4),\n",
    "                           lag_latent=lambda x: df_grouped['latent_demand'].shift(4)))\n",
    "    \n",
    "    for char in characteristics:\n",
    "        name = 'lag_' + char\n",
    "        df_variance[name] = df_variance.groupby(['date', 'asset_id'])[char].shift(4)\n",
    "    \n",
    "    return df_variance"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predictability\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figures\n",
    "\n",
    "def clean_figures(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_results\n",
    "            .assign(date=lambda x: x['date'].dt.to_timestamp(),\n",
    "                    typecode=lambda x: x['typecode'].apply(get_readable_typecode)))\n",
    "\n",
    "\n",
    "def typecode_share_counts(df_figure: pd.DataFrame, figure_path: str):\n",
    "    df_type_share = (df_figure\n",
    "                     .groupby(['typecode', 'date'])\n",
    "                     .agg({'aum': 'sum', 'n_holding': 'count'})\n",
    "                     .assign(share=lambda x: x['aum'] / x['aum'].groupby('date').transform('sum')))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, \n",
    "                    x='date', \n",
    "                    y='n_holding', \n",
    "                    hue='typecode', \n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Number of Investors')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_count.png'))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, \n",
    "                    x='date', \n",
    "                    y='share', \n",
    "                    hue='typecode', \n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Share of AUM')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_share.png'))\n",
    "    \n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def check_moment_condition(df_figures: pd.DataFrame, min_n_holding: int):\n",
    "    mask = df_figures['n_holding'] >= min_n_holding\n",
    "    df_mom = (df_figures\n",
    "              .loc[mask]\n",
    "              .groupby(['inv_id', 'date'])\n",
    "              .agg({'latent_demand': 'mean'}))\n",
    "    \n",
    "    epsilon = 0.01\n",
    "    mask = (df_mom['latent_demand'] > -epsilon) & (df_mom['latent_demand'] < epsilon)\n",
    "    valid_rate = len(df_mom[mask]) / len(df_mom)\n",
    "    print(f'Percentage of valid moments:  {100*valid_rate:.4f}%')\n",
    "    \n",
    "    g = sns.displot(\n",
    "        data=df_mom,\n",
    "        x='latent_demand'\n",
    "    )\n",
    "    g.set_axis_labels('Log Latent Demand', 'Frequency')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'moment_condition.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def critical_value_test(df_figures: pd.DataFrame, characteristics: list, min_n_holding: int, figure_path: str):\n",
    "    \n",
    "    def iv_reg(df_inv_date: pd.DataFrame):\n",
    "        y = df_inv_date['ln_me']\n",
    "        X = df_inv_date[['ln_iv_me'] + characteristics]\n",
    "        model = OLS(y, X)\n",
    "        result = model.fit()\n",
    "        t_stat = result.tvalues.iloc[0]\n",
    "        \n",
    "        return t_stat\n",
    "    \n",
    "    \n",
    "    mask = df_figures['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_iv_a = (df_figures\n",
    "               .loc[mask]\n",
    "               .groupby(['inv_id', 'date'])\n",
    "               .apply(iv_reg))\n",
    "    \n",
    "    df_iv_b = (df_figures\n",
    "               .loc[~mask]\n",
    "               .groupby(['bin', 'date'])\n",
    "               .apply(iv_reg))\n",
    "    \n",
    "    df_iv = (pd.concat([df_iv_a, df_iv_b])\n",
    "             .to_frame('t_stat')\n",
    "             .groupby('date')\n",
    "             .min())\n",
    "    \n",
    "    g = sns.relplot(data=df_iv,\n",
    "                    x='date',\n",
    "                    y='t_stat',\n",
    "                    kind='line')\n",
    "    g.refline(y=4.05, linestyle='--')\n",
    "    g.set_axis_labels('Date', 'First stage t-statistic')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'instrument_validity.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def test_index_fund(df_figures: pd.DataFrame, characteristics: list, params: list, indexfund_id: int, figure_path: str):\n",
    "    mask = df_figures['inv_id'] == indexfund_id\n",
    "    df_index_fund = (df_figures\n",
    "                     .assign(ln_rweight=lambda x: x['ln_me'] + x['mean_ln_rweight'],\n",
    "                             pct_uni_held=1)\n",
    "                     .loc[mask])\n",
    "    \n",
    "    df_index_fund_model = (df_index_fund\n",
    "                           .set_index(['inv_id', 'date'])\n",
    "                           .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                           .reset_index())\n",
    "    df_index_fund_result = calc_latent_demand_L(df_index_fund_model, characteristics, params)\n",
    "    cols = params + ['latent_demand']\n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_index_fund_result,\n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.despine()\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.savefig(os.path.join(figure_path, f'index_fund_{param}.png'))\n",
    "\n",
    "    return df_figures\n",
    "    \n",
    "\n",
    "def graph_type_params(df_results: pd.DataFrame, params: list, figure_path: str):\n",
    "    df_types = df_results.copy()\n",
    "    df_types[params] = df_types[params].apply(lambda x: x * df_results['aum'])\n",
    "    df_types = (df_types\n",
    "                .groupby(['typecode', 'date'])\n",
    "                [params]\n",
    "                .sum()\n",
    "                .apply(lambda x: x / df_results.groupby(['typecode', 'date'])['aum'].sum()))\n",
    "    \n",
    "    for param in params:\n",
    "        g = sns.relplot(data=df_types, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        hue='typecode',\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.legend.set_title('Institution Type')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'{param}.png'))\n",
    "        \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def graph_std_latent_demand(df_figures: pd.DataFrame, figure_path: str):\n",
    "    df_ld = (df_figures\n",
    "         .groupby(['inv_id', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'std',\n",
    "            'aum': 'last',\n",
    "            'typecode': 'last'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] * x['aum'])\n",
    "         .groupby(['typecode', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'mean',\n",
    "            'aum': 'sum'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] / x['aum']))\n",
    "    \n",
    "    g = sns.relplot(data=df_ld, \n",
    "                    x='date',\n",
    "                    y='latent_demand',\n",
    "                    hue='typecode',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Standard Deviation of Latent Demand')\n",
    "    g.legend.set_title('Institution Type')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'std_latent_demand.png'))\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def summary_tables(df_figures: pd.DataFrame):\n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def get_param_cols(cols: list) -> list:\n",
    "    return ['beta_' + col for col in cols]\n",
    "\n",
    "\n",
    "def get_readable_param(name: str) -> str:\n",
    "    return name.replace('_', ' ').title()\n",
    "\n",
    "\n",
    "def get_readable_typecode(typecode: int):\n",
    "    return dict_typecode[typecode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_quarter(q: pd.Period):\n",
    "    print('Imported:   ', q)\n",
    "\n",
    "\n",
    "def log_import_emaxx_holding(df_emaxx_holding: pd.DataFrame):\n",
    "    print()\n",
    "    print('Finished import eMAXX holdings')\n",
    "\n",
    "\n",
    "def log_import_emaxx_fund(df_emaxx_fund: pd.DataFrame):\n",
    "    print('Finished import eMAXX fund')\n",
    "\n",
    "\n",
    "def log_import_emaxx_secmast(df_emaxx_fund: pd.DataFrame):\n",
    "    print('Finished import eMAXX secmast')\n",
    "\n",
    "\n",
    "def log_import_emaxx_issuer(df_emaxx_issuer: pd.DataFrame):\n",
    "    print('Finished import eMAXX issuer')\n",
    "\n",
    "\n",
    "def log_import_emaxx_broktran(df_emaxx_broktran: pd.DataFrame):\n",
    "    print('Finished import eMAXX broktran')\n",
    "\n",
    "\n",
    "def log_import_fed(df_fed: pd.DataFrame):\n",
    "    print()\n",
    "    print('Imported Federal Reserve treasury holdings')\n",
    "\n",
    "\n",
    "def log_import_treasury(df_treasury: pd.DataFrame):\n",
    "    print('Imported Treasury auction data')\n",
    "\n",
    "\n",
    "def log_import_wrds_bond(df_wrds_bond: pd.DataFrame):\n",
    "    print('Imported WRDS bond returns')\n",
    "    \n",
    "\n",
    "def log_clean_emaxx_holding(df_emaxx_holding_clean):\n",
    "    print()\n",
    "    print('Cleaned eMAXX holdings')\n",
    "    print('Number of holdings:  ', len(df_emaxx_holding_clean))\n",
    "\n",
    "\n",
    "def log_clean_emaxx_fund(df_emaxx_fund_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX fund characteristics')\n",
    "    print('Number of investors:  ', df_emaxx_fund_clean['inv_id'].nunique())\n",
    "    \n",
    "    \n",
    "def log_clean_emaxx_secmast(df_emaxx_secmast: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX secmast characteristics')\n",
    "    \n",
    "    \n",
    "def log_clean_emaxx_issuer(df_emaxx_issuer: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX issuer characteristics')\n",
    "    print('Number of issuers:  ', df_emaxx_issuer['issuer_id'].nunique())\n",
    "    \n",
    "   \n",
    "def log_clean_emaxx_broktran(df_emaxx_broktran: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX broktran')\n",
    "    print('Number of assets:  ', df_emaxx_broktran['asset_id'].nunique())\n",
    "    \n",
    "\n",
    "def log_clean_fed(df_fed_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned Federal Reserve treasury holdings')\n",
    "    print('Number of holdings:  ', len(df_fed_clean))\n",
    "   \n",
    "\n",
    "def log_clean_treasury(df_treasury_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned Treasury auction data')\n",
    "    print('Number of treasuries:  ', df_treasury_clean['asset_id'].nunique())\n",
    "\n",
    "\n",
    "def log_clean_wrds_bond(df_wrds_bond_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned WRDS bond returns')\n",
    "    print('Number of assets:  ', df_wrds_bond_clean['asset_id'].nunique())\n",
    "\n",
    "\n",
    "def log_drop_unused(df_dropped: pd.DataFrame):\n",
    "    print()\n",
    "    print('Dropped unused investors/assets')\n",
    "    print('Number of holdings:  ', len(df_dropped))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_price_factor(df_asset: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged prices and factors')\n",
    "\n",
    "\n",
    "def log_fed_holding(df_holding_fed: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged Federal Reserve and corporate holdings')\n",
    "\n",
    "\n",
    "def log_holding_fund_merge(df_holding_fund: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged holdings and fund typecode')\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of investor/date/asset:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_outside: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created outside asset')\n",
    "    print('Number of outside holdings:  ', df_outside['out_mask'].sum())\n",
    "\n",
    "\n",
    "def log_zero_holdings(df_holding: pd.DataFrame):\n",
    "    print()\n",
    "    print('Constructed zero holdings')\n",
    "    print('Number of non-zero holdings:  ', sum(df_holding['paramt'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_holding['paramt'] == 0))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated investor AUM')\n",
    "    print('Number of investors:  ', df_inv_aum['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_agg_small_inv(df_agg: pd.DataFrame):\n",
    "    print()\n",
    "    print('Aggregated small investors')\n",
    "    print('Number of investors:  ', df_agg['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_bins(df_binned: pd.DataFrame):\n",
    "    print()\n",
    "    print('Binned investors')\n",
    "    print('Number of investors:  ', df_binned['inv_id'].nunique())\n",
    "    print('Number of bins:  ', df_binned['bin'].nunique())\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created investment universe')\n",
    "    print('Number of investors:  ', df_inv_uni['inv_id'].nunique())\n",
    "    print('Average investment universe size:  ', df_inv_uni['uni_size'].mean())\n",
    "    \n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created market equity instrument')\n",
    "    print('Number of valid instruments:  ', sum(df_instrument['iv_me'] > 0))\n",
    "    print('Number of invalid instruments:  ', sum(df_instrument['iv_me'] == 0))\n",
    "    \n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated holding weights')\n",
    "    print('Number of non-zero holdings:  ', sum(df_model['holding'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_model['holding'] == 0))\n",
    "\n",
    "\n",
    "def log_results(result, params):\n",
    "    print()\n",
    "    print(result.summary(yname='Latent demand', xname=params))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_params(df_params: pd.DataFrame):\n",
    "    print()\n",
    "    print('Estimated parameters')\n",
    "    print('Number of converged estimations:  ', sum(df_params['gmm_result'].notna()))\n",
    "    print('Number of unconverged estimations:  ', sum(df_params['gmm_result'].isna()))\n",
    "    \n",
    "    \n",
    "def log_latent_demand(df_results: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated latent demand')\n",
    "    print('Number of investors:  ', df_results['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_liquidity(df_liquidity: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated price elasticity')\n",
    "\n",
    "\n",
    "def log_variance(df_variance: pd.DataFrame):\n",
    "    print()\n",
    "    print('Decomposed variance')\n",
    "    \n",
    "    \n",
    "def log_predictability(df_predictability: pd.DataFrame):\n",
    "    print()\n",
    "    print('Tested return predictability')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    # TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "sns.set_theme(style='ticks', palette=sns.color_palette('hls', 6), context='paper')\n",
    "\n",
    "input_path = 'data'\n",
    "output_path = 'output/'\n",
    "figure_path = 'figures/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(figure_path, exist_ok=True)\n",
    "\n",
    "start_date = pd.Period('2017Q1')\n",
    "end_date = pd.Period('2017Q4')\n",
    "\n",
    "characteristics = [\n",
    "    'coupon',\n",
    "    'maturity',\n",
    "    # 'spread',\n",
    "    'on_the_run',\n",
    "] + ['const']\n",
    "params = ['beta_ln_me'] + get_param_cols(characteristics)\n",
    "dict_typecode = {'TTF':'13F Filer',\n",
    "                 'FOK':'401K',\n",
    "                 'ANN':'Annuity/Variable Annuity',\n",
    "                 'AMM':'Annuity/VA - Money Market',\n",
    "                 'BKP':'Bank-Portfolio',\n",
    "                 'SVG':'Bank-Savings/Bldg Society',\n",
    "                 'BKT':'Bank-Trust',\n",
    "                 'CHU':'Church/Religious Org',\n",
    "                 'CRP':'Corporation',\n",
    "                 'CRU':'Credit Union',\n",
    "                 'FCC':'Finance Company',\n",
    "                 'FED':'Federal Reserve',\n",
    "                 'FEN':'Foundation/Endowment',\n",
    "                 'GVT':'Government',\n",
    "                 'HLC':'Health Care Systems',\n",
    "                 'HFD':'Hedge Fund',\n",
    "                 'HOU':'Households',\n",
    "                 'HSP':'Hospital',\n",
    "                 'INS':'Insurance Co-Diversified',\n",
    "                 'LIN':'Insurance Co-Life/Health',\n",
    "                 'PIN':'Insurance Co-Prop & Cas',\n",
    "                 'INM':'Investment Manager',\n",
    "                 'BAL':'Mutual Fund - Balanced',\n",
    "                 'MMM':'Mutual Fund - Money Mkt',\n",
    "                 'MUT':'MutFd-OE/UnitTr/SICAV/FCP',\n",
    "                 'END':'MutFd-CE/Inv Tr/FCP',\n",
    "                 'QUI':'Mutual Fund-Equity',\n",
    "                 'FOF':'Mutual Fund-Fund of Funds',\n",
    "                 'NDT':'Nuclear De-Comm Trust',\n",
    "                 'OTH':'Other',\n",
    "                 'CPF':'Pension Fund-Corporate',\n",
    "                 'GPE':'Pension Fund-Government',\n",
    "                 'UPE':'Pension Fund-Union',\n",
    "                 'RIN':'Reinsurance Company',\n",
    "                 'SBC':'Small Business Invst Co',\n",
    "                 'SPZ':'Spezial Fund',\n",
    "                 'UIT':'Unit Investment Trust'}\n",
    "\n",
    "min_n_holding = 1000\n",
    "n_quarters = 11\n",
    "indexfund_id = -1"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Starting Imports---------------------------\\n')\n",
    "dfs = pandas_read_bond(input_path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Starting Cleaning---------------------------\\n')\n",
    "df_emaxx_holding_clean, df_emaxx_fund_clean, df_emaxx_broktran_clean, df_fed_clean, df_treasury_clean,  df_wrds_bond_clean = clean_imports_bond(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculate Off-The-Run Factor---------------------------\\n')\n",
    "df_factor = calc_off_the_run(df_treasury_clean, end_date)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Merging Prices/Factors---------------------------\\n')\n",
    "df_asset = merge_price_factor(df_emaxx_broktran_clean, df_factor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Merge Federal Reserve Holdings---------------------------\\n')\n",
    "df_holding_fed = merge_fed_holding(df_fed_clean, df_asset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Merging Fund Class---------------------------\\n')\n",
    "df_holding_fund = merge_holding_fund(df_emaxx_holding_clean, df_emaxx_fund_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print('\\n---------------Constructing Zero Holdings---------------------------\\n')\n",
    "# df_holding = construct_zero_holdings(df_holding_fund, n_quarters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Merging Holdings/Factors---------------------------\\n')\n",
    "df_holding_factor = merge_holding_factor(df_holding_fund, df_asset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Dropping Mangers/Assets Without Holdings---------------------------\\n')\n",
    "df_dropped = drop_unsused_inv_asset(df_holding_factor)\n",
    "\n",
    "print('\\n---------------Creating Household Sector---------------------------\\n')\n",
    "df_household = create_household_sector(df_dropped)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Partitioning Outside Asset---------------------------\\n')\n",
    "df_outside = partition_outside_asset(df_household)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Investor AUM---------------------------\\n')\n",
    "df_inv_aum = calc_inv_aum(df_outside)\n",
    "# df_inv_aum.to_csv(os.path.join(output_path, 'df_inv_aum.csv'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Aggregating Small Investors---------------------------\\n')\n",
    "df_agg = agg_small_inv(df_inv_aum)\n",
    "# df_agg.to_csv(os.path.join(output_path, 'df_agg.csv'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Pooling Investors By Type/Size---------------------------\\n')\n",
    "df_binned = bin_inv(df_agg, min_n_holding)\n",
    "# df_binned.to_csv(os.path.join(output_path, 'df_binned.csv'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Instrument---------------------------\\n')\n",
    "df_instrument = calc_instrument(df_binned)\n",
    "# df_instrument.to_csv(os.path.join(output_path, 'df_instrument.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Holding Weights---------------------------\\n')\n",
    "df_weights = calc_holding_weights(df_instrument)\n",
    "# df_weights.to_csv(os.path.join(output_path, 'df_weights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Estimating Demand System---------------------------\\n')\n",
    "df_model = estimate_model_L(df_weights, characteristics, params, min_n_holding)\n",
    "# df_model.to_csv(os.path.join(output_path, 'df_model.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Latent Demand---------------------------\\n')\n",
    "df_results = calc_latent_demand_L(df_model, characteristics, params)\n",
    "# df_results.to_csv(os.path.join(output_path, 'df_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Moment Condition---------------------------\\n')\n",
    "# df_figures = clean_figures(df_results)\n",
    "# _ = check_moment_condition(df_figures, 0, min_n_holding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Investors by Typecode---------------------------\\n')\n",
    "# _ = typecode_share_counts(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Instrument Validity---------------------------\\n')\n",
    "# _ = critical_value_test(df_figures, characteristics, min_n_holding, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Hypothetical Index Fund---------------------------\\n')\n",
    "# _ = test_index_fund(df_figures, characteristics, params, indexfund_id, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Parameters By Typecode---------------------------\\n')\n",
    "# _ = graph_type_params(df_figures, params, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Standard Deviation of Latent Demand By Typecode---------------------------\\n')\n",
    "# _ = graph_std_latent_demand(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Price Elasticity---------------------------\\n')\n",
    "df_liquidity = calc_price_elasticity(df_results).pipe(graph_liquidity)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Predictability---------------------------\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Simulating Monetary Policy Shock---------------------------\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Finished---------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
