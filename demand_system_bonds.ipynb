{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def read_emaxx(input_path: str, start_date: pd.Period, end_date: pd.Period):\n",
    "    \n",
    "    period_range = pd.period_range(start_date, end_date, freq='Q')\n",
    "    prefix = 'eMAXX\\\\ASCII_NA_Mkt_All_Pipe_RY_'\n",
    "    \n",
    "    holding_cols = [\n",
    "        'CUSIP',\n",
    "        'CUSIPSUFF',\n",
    "        'FUNDID',\n",
    "        'BOOL',\n",
    "        'PARAMT',\n",
    "    ]\n",
    "    df_emaxx_holding = []\n",
    "    \n",
    "    fund_cols = [\n",
    "        'FUNDID',\n",
    "        'FUNDCLASS',\n",
    "    ]\n",
    "    df_emaxx_fund = []\n",
    "    \n",
    "    secmast_cols = [\n",
    "        'ISSUERCUS',\n",
    "        'ISSUESUF',\n",
    "        'CPNRATE',\n",
    "        'MATDATE',\n",
    "    ]\n",
    "    df_emaxx_secmast = []\n",
    "\n",
    "    issuer_cols = [\n",
    "        'ISSUERCUS',\n",
    "        'GEOCODE',\n",
    "        'ENTITY',\n",
    "        'STATE'\n",
    "    ]\n",
    "    df_emaxx_issuer = []\n",
    "    \n",
    "    broktran_cols = [\n",
    "        'CUSIP',\n",
    "        'TRANDATE',\n",
    "        'ACTCOST',\n",
    "        'PARAMT',\n",
    "    ]\n",
    "    df_emaxx_broktran = []\n",
    "    \n",
    "    for q in period_range:\n",
    "        log_quarter(q)\n",
    "        path = os.path.join(input_path, prefix + str(q))\n",
    "        \n",
    "        holding_path = os.path.join(path, 'HOLDING.txt')\n",
    "        holding_data = (pd.read_csv(filepath_or_buffer=holding_path,\n",
    "                                    delimiter='|',\n",
    "                                    usecols=holding_cols,\n",
    "                                    low_memory=False)\n",
    "                        .assign(date=q))\n",
    "        df_emaxx_holding.append(holding_data)\n",
    "        \n",
    "        fund_path = os.path.join(path, 'FUND.txt')\n",
    "        fund_data = (pd.read_csv(filepath_or_buffer=fund_path,\n",
    "                                 delimiter='|',\n",
    "                                 usecols=fund_cols,\n",
    "                                 low_memory=False)\n",
    "                     .assign(date=q))\n",
    "        df_emaxx_fund.append(fund_data)\n",
    "        \n",
    "        # secmast_path = os.path.join(path, 'SECMAST.txt')\n",
    "        # secmast_data = (pd.read_csv(filepath_or_buffer=secmast_path,\n",
    "        #                             delimiter='|',\n",
    "        #                             usecols=secmast_cols,\n",
    "        #                             low_memory=False)\n",
    "        #                 .assign(date=q))\n",
    "        # df_emaxx_secmast.append(secmast_data)\n",
    "        \n",
    "        # issuer_path = os.path.join(path, 'ISSUERS.txt')\n",
    "        # issuer_data = (pd.read_csv(filepath_or_buffer=issuer_path,\n",
    "        #                          delimiter='|',\n",
    "        #                          usecols=issuer_cols,\n",
    "        #                          low_memory=False)\n",
    "        #              .assign(date=q))\n",
    "        # df_emaxx_issuer.append(issuer_data)\n",
    "        \n",
    "        # broktran_path = os.path.join(path, 'BROKTRAN.txt')\n",
    "        # broktran_data = (pd.read_csv(filepath_or_buffer=broktran_path,\n",
    "        #                              delimiter='|',\n",
    "        #                              usecols=broktran_cols,\n",
    "        #                              low_memory=False)\n",
    "        #                  .assign(date=q)\n",
    "        #                  .sort_values('TRANDATE')\n",
    "        #                  .drop_duplicates('CUSIP', keep='last')\n",
    "        #                  .drop(columns='TRANDATE'))\n",
    "        # df_emaxx_broktran.append(broktran_data)\n",
    "        \n",
    "    \n",
    "    df_emaxx_holding = pd.concat(df_emaxx_holding)\n",
    "    df_emaxx_fund = pd.concat(df_emaxx_fund)\n",
    "    # df_emaxx_secmast = pd.concat(df_emaxx_secmast)\n",
    "    # df_emaxx_issuer = pd.concat(df_emaxx_issuer)\n",
    "    # df_emaxx_broktran = pd.concat(df_emaxx_broktran)\n",
    "    \n",
    "    return df_emaxx_holding, df_emaxx_fund, df_emaxx_secmast, df_emaxx_issuer, df_emaxx_broktran\n",
    "\n",
    "\n",
    "def pandas_read_bond(path: str, start_date: pd.Period, end_date: pd.Period):\n",
    "    df_emaxx_holding, df_emaxx_fund, df_emaxx_secmast, df_emaxx_issuer, df_emaxx_broktran = read_emaxx(path, start_date, end_date)\n",
    "    log_import_emaxx_holding(df_emaxx_holding)\n",
    "    log_import_emaxx_fund(df_emaxx_fund)\n",
    "    # log_import_emaxx_secmast(df_emaxx_secmast)\n",
    "    # log_import_emaxx_issuer(df_emaxx_issuer)\n",
    "    log_import_emaxx_broktran(df_emaxx_broktran)\n",
    "    \n",
    "    fed_cols = ['cusip',\n",
    "                'parValue',\n",
    "                'date']\n",
    "    df_fed = pd.read_csv(filepath_or_buffer=os.path.join(path, 'FED', 'fed_treasury_holding.csv'),\n",
    "                         usecols=fed_cols)\n",
    "    log_import_fed(df_fed)\n",
    "    \n",
    "    df_treasury = pd.read_csv(filepath_or_buffer=os.path.join(path, 'Auctions_TreasurySecurities.csv'))\n",
    "    log_import_treasury(df_treasury)\n",
    "    \n",
    "    crsp_cols = ['TCUSIP',\n",
    "                 'TMATDT',\n",
    "                 'TCOUPRT',\n",
    "                 'ITYPE',\n",
    "                 'MCALDT',\n",
    "                 'TMYLD',\n",
    "                 'TMBID',\n",
    "                 'TMASK',\n",
    "                 'TMNOMPRC',\n",
    "                 'TMTOTOUT']\n",
    "    df_crsp = pd.read_csv(filepath_or_buffer=os.path.join(path, 'crsp_treasury.csv'),\n",
    "                          usecols=crsp_cols,\n",
    "                          low_memory=False)\n",
    "    log_import_crsp(df_crsp)\n",
    "    \n",
    "    wrds_cols = ['DATE',\n",
    "                 'CUSIP',\n",
    "                 'BOND_TYPE',\n",
    "                 'AMOUNT_OUTSTANDING',\n",
    "                 'MATURITY',\n",
    "                 'COUPON',\n",
    "                 'T_Yld_Pt',\n",
    "                 'PRICE_EOM',\n",
    "                 'R_SP',\n",
    "                 'N_SP',\n",
    "                 'T_Spread']\n",
    "    df_wrds_bond = pd.read_csv(filepath_or_buffer=os.path.join(path, 'wrds_bond.csv'),\n",
    "                               usecols=wrds_cols,\n",
    "                               low_memory=False)\n",
    "    log_import_wrds_bond(df_wrds_bond)\n",
    "\n",
    "    return df_emaxx_holding, df_emaxx_fund, df_emaxx_secmast, df_emaxx_issuer, df_emaxx_broktran, df_fed, df_treasury, df_crsp, df_wrds_bond\n",
    "\n",
    "\n",
    "def clean_imports_bond(df_emaxx_holding: pd.DataFrame,\n",
    "                       df_emaxx_fund: pd.DataFrame,\n",
    "                       df_emaxx_secmast: pd.DataFrame,\n",
    "                       df_emaxx_issuer: pd.DataFrame,\n",
    "                       df_emaxx_broktran: pd.DataFrame,\n",
    "                       df_fed: pd.DataFrame,\n",
    "                       df_treasury: pd.DataFrame,\n",
    "                       df_crsp: pd.DataFrame,\n",
    "                       df_wrds_bond: pd.DataFrame,\n",
    "                       start_date: pd.Period,\n",
    "                       end_date: pd.Period) -> tuple:\n",
    "\n",
    "    df_emaxx_holding_clean = clean_emaxx_holding(df_emaxx_holding)\n",
    "    log_clean_emaxx_holding(df_emaxx_holding_clean)\n",
    "\n",
    "    df_emaxx_fund_clean = clean_emaxx_fund(df_emaxx_fund)\n",
    "    log_clean_emaxx_fund(df_emaxx_fund_clean)\n",
    "    \n",
    "    # df_emaxx_secmast_clean = clean_emaxx_secmast(df_emaxx_secmast)\n",
    "    # log_clean_emaxx_secmast(df_emaxx_secmast_clean)\n",
    "    \n",
    "    # df_emaxx_issuer_clean = clean_emaxx_issuer(df_emaxx_issuer)\n",
    "    # log_clean_emaxx_issuer(df_emaxx_issuer_clean)\n",
    "    \n",
    "    # df_emaxx_broktran_clean = clean_emaxx_broktran(df_emaxx_broktran)\n",
    "    # log_clean_emaxx_broktran(df_emaxx_broktran_clean)\n",
    "    \n",
    "    df_fed_clean = clean_fed_holding(df_fed, start_date, end_date)\n",
    "    log_clean_fed(df_fed_clean)\n",
    "    \n",
    "    df_treasury_clean = clean_treasury(df_treasury)\n",
    "    log_clean_treasury(df_treasury_clean)\n",
    "    \n",
    "    df_crsp_clean = clean_crsp(df_crsp, start_date, end_date)\n",
    "    log_clean_crsp(df_crsp_clean)\n",
    "    \n",
    "    df_wrds_bond_clean = clean_wrds_bond(df_wrds_bond, start_date, end_date)\n",
    "    log_clean_wrds_bond(df_wrds_bond_clean)\n",
    "    \n",
    "    return df_emaxx_holding_clean, df_emaxx_fund_clean, df_fed_clean, df_treasury_clean, df_crsp_clean, df_wrds_bond_clean\n",
    "\n",
    "\n",
    "def clean_emaxx_holding(df_emaxx_holding: pd.DataFrame) -> pd.DataFrame:    \n",
    "    df_emaxx_holding.columns = df_emaxx_holding.columns.str.lower()\n",
    "    return (df_emaxx_holding\n",
    "            .rename(columns={'cusip': 'issuer_id',\n",
    "                             'cusipsuff': 'issue',\n",
    "                             'fundid': 'inv_id',\n",
    "                             'bool': 'q_date'})\n",
    "            .assign(asset_id=lambda x: x['issuer_id'] + x['issue'])\n",
    "            .dropna(subset=['asset_id'])\n",
    "            .drop(columns=['issue', 'issuer_id'])\n",
    "            .sort_values('paramt')\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_fund(df_emaxx_fund: pd.DataFrame) -> pd.DataFrame:  \n",
    "    df_emaxx_fund.columns = df_emaxx_fund.columns.str.lower()\n",
    "    return (df_emaxx_fund\n",
    "            .rename(columns={'fundid': 'inv_id',\n",
    "                             'fundclass': 'typecode'})\n",
    "            .dropna(subset=['typecode', 'inv_id'])\n",
    "            .assign(typecode=lambda x: x['typecode'].apply(agg_typcode))\n",
    "            .drop_duplicates(subset=['inv_id', 'date'], keep='last')\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_secmast(df_emaxx_secmast: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_emaxx_secmast.columns = df_emaxx_secmast.columns.str.lower()\n",
    "    return (df_emaxx_secmast\n",
    "            .rename(columns={'cpnrate': 'coupon'})\n",
    "            .loc[lambda x: x['matdate'] != 99990101]\n",
    "            .dropna(subset=['issuercus', 'issuesuf'])\n",
    "            .assign(asset_id=lambda x: x['issuercus'] + x['issuesuf'],\n",
    "                    matdate=lambda x: pd.to_datetime(x['matdate'], format='%Y%m%d').dt.to_period('Q'),\n",
    "                    maturity=lambda x: (x['date'] - x['matdate']).apply(lambda y: y.n))\n",
    "            .drop(columns=['cusip', 'cusipsuff'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_issuer(df_emaxx_issuer: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_emaxx_issuer.columns = df_emaxx_issuer.columns.str.lower()\n",
    "    return (df_emaxx_issuer\n",
    "            .dropna(subset=['issuercus', 'geocode', 'entity'])\n",
    "            .rename(columns={'issuercus': 'issuer_id'})\n",
    "            .astype({'issuer_id': str})\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_emaxx_broktran(df_emaxx_broktran: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_emaxx_broktran.columns = df_emaxx_broktran.columns.str.lower()\n",
    "    return (df_emaxx_broktran\n",
    "            .rename(columns={'cusip': 'asset_id'})\n",
    "            .assign(prc=lambda x: x['actcost'] / x['paramt'])\n",
    "            .drop(columns=['actcost', 'paramt'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_fed_holding(df_fed_holding: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    return (df_fed_holding\n",
    "            .rename(columns={'cusip': 'asset_id',\n",
    "                             'maturityDate': 'maturity',\n",
    "                             'parValue': 'paramt'})\n",
    "            .assign(asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]),\n",
    "                    paramt=lambda x: x['paramt'] / 1000000,\n",
    "                    inv_id=-1,\n",
    "                    typecode='FED',\n",
    "                    date=lambda x: x['date'].apply(lambda y: pd.Period(y, freq='Q')))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)])\n",
    "\n",
    "\n",
    "def clean_treasury(df_treasury: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_treasury.columns = df_treasury.columns.str.lower()\n",
    "    df_treasury.columns = df_treasury.columns.str.replace(' ', '_')\n",
    "    return (df_treasury\n",
    "            .rename(columns={'cusip':'asset_id',\n",
    "                             'interest_rate': 'coupon'})\n",
    "            .loc[lambda x: x['floating_rate'] == 'No']\n",
    "            .assign(coupon=lambda x: x['coupon'].fillna(0),\n",
    "                    amtout=lambda x: x['currently_outstanding'].fillna(x['total_accepted']))\n",
    "            .dropna(subset=['amtout', 'coupon'])\n",
    "            .sort_values('issue_date')\n",
    "            .drop_duplicates('asset_id', keep='last')\n",
    "            .drop(columns=['floating_rate'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_crsp(df_crsp: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    df_crsp.columns = df_crsp.columns.str.lower()\n",
    "    return (df_crsp\n",
    "            .rename(columns={'tcusip': 'asset_id',\n",
    "                             'mcaldt': 'date',\n",
    "                             'tmatdt': 'maturity',\n",
    "                             'tcouprt': 'coupon',\n",
    "                             'tmyld': 'ytm',\n",
    "                             'tmnomprc': 'prc',\n",
    "                             'tmtotout': 'amtout'})\n",
    "            .loc[lambda x: (x['itype'] >= 1) & (x['itype'] <= 4)]\n",
    "            .assign(date=lambda x: fix_date_quarterly(x['date']),\n",
    "                    ytm=lambda x: x['ytm'] * 100,\n",
    "                    credit_rating= 1)\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .dropna(subset=['asset_id', 'prc', 'amtout'])\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates(subset=['date', 'asset_id'], keep='last')\n",
    "            .drop(columns=['itype'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def clean_wrds_bond(df_wrds_bond: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    df_wrds_bond.columns = df_wrds_bond.columns.str.lower()\n",
    "    return (df_wrds_bond\n",
    "            .loc[lambda x: x['bond_type'] != 'CCOV']\n",
    "            .rename(columns={'cusip': 'asset_id',\n",
    "                             'price_eom': 'prc',\n",
    "                             't_yld_pt': 'ytm',\n",
    "                             't_spread': 'spread',\n",
    "                             'n_sp': 'credit_rating',\n",
    "                             'amount_outstanding': 'amtout'})\n",
    "            .assign(date=lambda x: fix_date_quarterly(x['date']),\n",
    "                    asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]),\n",
    "                    spread=lambda x: x['spread'].replace('%','', regex=True).astype(float),\n",
    "                    is_treasury=0)\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .dropna(subset=['asset_id', 'prc'])\n",
    "            .drop(columns=['bond_type'])\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates(subset=['date', 'asset_id'])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "\n",
    "def fix_date_quarterly(dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(dates).dt.to_period(freq='Q')\n",
    "\n",
    "\n",
    "def fix_date_monthly(dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(dates).dt.to_period(freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bonds Monthly\n",
    "\n",
    "def calc_factors(df_crsp_clean: pd.DataFrame, df_treasury_clean: pd.DataFrame, end_date: pd.Period) -> pd.DataFrame:\n",
    "    df_otr = (df_treasury_clean\n",
    "              .sort_values('issue_date')\n",
    "              .assign(otf_date=(df_treasury_clean\n",
    "                                .groupby(['security_term'])\n",
    "                                ['issue_date']\n",
    "                               .shift(periods=-1,\n",
    "                                      fill_value=end_date + 1))))\n",
    "    \n",
    "    # df_factor = (df_crsp_clean\n",
    "    #              .merge(right=df_otr,\n",
    "    #                     how='left',\n",
    "    #                     on=['asset_id', 'date']))\n",
    "    df_factor = (df_crsp_clean\n",
    "                 .assign(spread=lambda x: 100 * (x['tmask'] - x['tmbid']) / x['tmask'],\n",
    "                         is_treasury=1)\n",
    "                 .drop(columns=['tmask', 'tmbid']))\n",
    "    \n",
    "    log_factors(df_factor)\n",
    "    return df_factor\n",
    "\n",
    "\n",
    "def merge_corp_treasury(df_factor: pd.DataFrame, df_wrds_bond_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_asset = (pd.concat([df_wrds_bond_clean, df_factor], ignore_index=True)\n",
    "                .assign(maturity=lambda x: pd.to_datetime(x['maturity'], format='%Y-%m-%d'))\n",
    "                .drop_duplicates(['asset_id', 'date'], keep='last'))\n",
    "    log_corp_treasury(df_asset)\n",
    "    return df_asset\n",
    "\n",
    "def merge_holding_fund(df_emaxx_holding_clean: pd.DataFrame, df_emaxx_fund_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_holding_fund = (df_emaxx_holding_clean\n",
    "                       .merge(right=df_emaxx_fund_clean,\n",
    "                              how='left',\n",
    "                              on=['date', 'inv_id']))\n",
    "\n",
    "    log_holding_fund_merge(df_holding_fund)\n",
    "    return df_holding_fund\n",
    "\n",
    "\n",
    "def merge_fed_holding(df_emaxx_holding_clean: pd.DataFrame, df_fed_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fed_holding = pd.concat([df_emaxx_holding_clean, df_fed_clean], ignore_index=True)\n",
    "    log_fed_holding(df_fed_holding)\n",
    "    return df_fed_holding\n",
    "\n",
    "\n",
    "def construct_zero_holdings(df_fund_manager: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    \n",
    "    def calc_inv_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.groupby('inv_id')['date'].transform('max') - df['date']).apply(lambda x: x.n)\n",
    "        min_diff = np.minimum(date_diff, n_quarters) + 1\n",
    "        return min_diff\n",
    "    \n",
    "    def calc_asset_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.sort_values('date')\n",
    "                     .groupby(['inv_id', 'asset_id'])\n",
    "                     ['date']\n",
    "                     .diff(periods=1)\n",
    "                     .fillna(pd.DateOffset(n=1))\n",
    "                     .apply(lambda x: x.n))\n",
    "        min_diff = np.minimum(date_diff, df['inv_obs'])\n",
    "        return min_diff.apply(lambda x: list(range(x)))\n",
    "    \n",
    "    \n",
    "    df_holding = (df_fund_manager\n",
    "                  .assign(inv_obs=lambda x: calc_inv_obs(x),\n",
    "                          asset_obs=lambda x: calc_asset_obs(x))\n",
    "                  .explode('asset_obs')\n",
    "                  .assign(asset_obs=lambda x: x['asset_obs'].astype('int8'),\n",
    "                          mask=lambda x: x['asset_obs'] == 0,\n",
    "                          paramt=lambda x: x['paramt'] * x['mask'],\n",
    "                          date=lambda x: x['date'] + x['asset_obs'])\n",
    "                  .drop(columns=['inv_obs', 'asset_obs', 'mask']))\n",
    "    \n",
    "    log_zero_holdings(df_holding)\n",
    "    return df_holding\n",
    "\n",
    "\n",
    "def merge_holding_factor(df_holding_fed: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_holding_factor = (df_holding_fed\n",
    "                         .merge(right=df_asset,\n",
    "                                how='left',\n",
    "                                on=['date', 'asset_id'])\n",
    "                         .assign(holding=lambda x: x['prc'] * x['paramt'] / 100000,\n",
    "                                 q_date=lambda x: pd.to_datetime(x['q_date'], format='%Y%m%d'),\n",
    "                                 t_maturity=lambda x: (x['maturity'] - x['q_date']).dt.days.fillna(0) / 30.5)\n",
    "                         .loc[lambda x: x['amtout'] > 0]\n",
    "                         .dropna(subset=['holding'])\n",
    "                         .drop(columns=['q_date', 'maturity']))\n",
    "    \n",
    "    log_holding_factor_merge(df_holding_factor)\n",
    "    return df_holding_factor\n",
    "\n",
    "\n",
    "def calc_maturity(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_holding_factor)\n",
    "\n",
    "\n",
    "def drop_unused_inv_asset(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_dropped = (df_holding_factor\n",
    "                  .assign(Tinv_paramt=lambda x: x.groupby(['inv_id', 'date'])['paramt'].transform('sum'),\n",
    "                          Tasset_paramt=lambda x: x.groupby(['date', 'asset_id'])['paramt'].transform('sum'),\n",
    "                          mask=lambda x: (x['Tinv_paramt'] == 0) | (x['Tasset_paramt'] == 0))\n",
    "                  .loc[lambda x: ~x['mask']]\n",
    "                  .drop(columns=['Tinv_paramt', 'Tasset_paramt', 'mask']))\n",
    "    \n",
    "    log_drop_unused(df_dropped)\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def create_household_sector(df_dropped: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_dropped\n",
    "                      .groupby(['date', 'asset_id'], as_index=False)\n",
    "                      .agg({\n",
    "                        'paramt': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'amtout': 'last',\n",
    "                        'holding': 'sum',\n",
    "                        'ytm': 'last'})\n",
    "                      .assign(paramt=lambda x: np.maximum(x['amtout'] - x['paramt'], 0),\n",
    "                              holding=lambda x: x['holding'] * x['prc'],\n",
    "                              inv_id=0,\n",
    "                              typecode='HOU'))\n",
    "    \n",
    "    log_household_sector(df_household)\n",
    "    df_concat = pd.concat([df_dropped, df_household], ignore_index=True)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def get_treasury_id(df_emaxx_issuer_clean: pd.DataFrame) -> np.array:\n",
    "    df_t = (df_emaxx_issuer_clean\n",
    "            .assign(usa_mask=lambda x: x['geocode'] == 'USA',\n",
    "                    gov_mask=lambda x: x['entity'] == 'GT',\n",
    "                    t_mask=lambda x: x['usa_mask'] & x['gov_mask'])\n",
    "            .loc[lambda x: x['treasury_mask']])\n",
    "    return df_t['issuer_id'].unique()\n",
    "\n",
    "\n",
    "def partition_outside_asset(df_household: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df_outside = (df_household\n",
    "                  .assign(out_mask=lambda x: x.isna().any(axis=1),\n",
    "                          out_holding=lambda x: x['holding'] * x['out_mask']))\n",
    "    \n",
    "    log_outside_asset(df_outside)\n",
    "    return df_outside\n",
    "\n",
    "\n",
    "def calc_inv_aum(df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_inv_aum = (df_outside\n",
    "                  .assign(aum=lambda x: x.groupby(['inv_id', 'date'])['holding'].transform('sum'))\n",
    "                  .loc[lambda x: x['aum'] > 0]\n",
    "                  .assign(out_aum=lambda x: x.groupby(['inv_id', 'date'])['out_holding'].transform('sum'))\n",
    "                  .assign(out_aum=lambda x: x['out_aum'].mask(x['typecode'] == 'FED', 1),\n",
    "                          out_weight=lambda x: x['out_aum'] / x['aum']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def agg_small_inv(df_inv_aum: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_agg = (df_inv_aum\n",
    "              .assign(small_mask=lambda x: (x['aum'] < 10) | (x['out_weight'] == 0) | (x['out_weight'] == 1),\n",
    "                      inv_id=lambda x: ~x['small_mask'] * x['inv_id'],\n",
    "                      typecode=lambda x: x['typecode'].mask(x['small_mask'], 'HOU'),\n",
    "                      hh_mask=lambda x: x['inv_id'] == 0))\n",
    "\n",
    "    df_grouped = df_agg.groupby(['inv_id', 'date', 'asset_id'])\n",
    "    \n",
    "    df_agg = (df_agg\n",
    "              .assign(Tparamt=lambda x: df_grouped['paramt'].transform('sum'),\n",
    "                      Tholding=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                      paramt=lambda x: np.where(x['hh_mask'], x['Tparamt'], x['paramt']),\n",
    "                      holding=lambda x: np.where(x['hh_mask'], x['Tholding'], x['holding']))\n",
    "              .drop(columns=['Tparamt', 'Tholding']))\n",
    "    \n",
    "    df_grouped = df_agg.groupby(['inv_id', 'date'])\n",
    "    \n",
    "    df_agg = (df_agg\n",
    "              .assign(Taum=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                      Tout_aum=lambda x: df_grouped['out_holding'].transform('sum'),\n",
    "                      aum=lambda x: np.where(x['hh_mask'], x['Taum'], x['aum']),\n",
    "                      out_aum=lambda x: np.where(x['hh_mask'], x['Tout_aum'], x['out_aum']),\n",
    "                      x_holding=lambda x: df_grouped['holding'].transform('count'),\n",
    "                      n_holding=lambda x: x['x_holding'] - df_grouped['out_mask'].transform('sum'),\n",
    "                      equal_alloc=lambda x: ~x['hh_mask'] * x['aum'] / (1 + x['x_holding']),\n",
    "                      drop_mask=lambda x: x['small_mask'] | x['out_mask'])\n",
    "              .loc[lambda x: ~x['drop_mask']]\n",
    "              .drop(columns=['Taum', 'Tout_aum', 'out_mask', 'out_holding', 'out_weight', 'small_mask', 'drop_mask', 'hh_mask']))\n",
    "    \n",
    "    log_agg_small_inv(df_agg)\n",
    "    return df_agg\n",
    "    \n",
    "\n",
    "def bin_inv(df_inv_aum: pd.DataFrame, min_n_holding: int) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    def calc_bin(df_type_date: pd.DataFrame) -> pd.Series:\n",
    "        typecode = df_type_date['typecode'].iloc[0]\n",
    "        n_holding = (df_type_date['holding'] > 0).sum()\n",
    "        n_bins = np.floor(n_holding / (2 * min_n_holding)).astype(int)\n",
    "        idx = df_type_date.index\n",
    "        \n",
    "        if (n_bins <= 1) | (n_holding >= min_n_holding) | (typecode == 0) | (typecode == 'FED'):\n",
    "            return pd.Series(0, index=idx)\n",
    "        else:\n",
    "            return pd.Series(pd.qcut(x=df_type_date['aum'], q=n_bins, labels=False), index=idx)\n",
    "    \n",
    "    \n",
    "    df_binned = (df_inv_aum\n",
    "                 .reset_index(drop=True)\n",
    "                 .assign(bin=lambda x: x.groupby(['typecode', 'date']).apply(calc_bin).reset_index(drop=True))\n",
    "                 .assign(bin=lambda x: x.groupby(['date', 'typecode', 'bin']).ngroup()))\n",
    "    \n",
    "    log_bins(df_binned)\n",
    "    return df_binned\n",
    "\n",
    "\n",
    "def calc_ytm(coupon_rate: pd.Series, maturity: pd.Series, prc: pd.Series, par: pd.Series) -> pd.Series:\n",
    "    coupon = coupon_rate * par / 200\n",
    "    appreciation = (par - prc) / maturity\n",
    "    average_prc = (par + prc) / 2\n",
    "    return (coupon + appreciation) / average_prc\n",
    "\n",
    "\n",
    "def calc_instrument(df_binned: pd.DataFrame) -> pd.DataFrame:    \n",
    "    df_instrument = (df_binned\n",
    "                     .assign(total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum'),\n",
    "                             iv_me=lambda x: x['total_alloc'] - x['equal_alloc'],\n",
    "                             iv_ytm=lambda x: calc_ytm(x['coupon'], x['t_maturity'], x['iv_me'], x['amtout']))\n",
    "                     .drop(columns=['total_alloc', 'equal_alloc']))\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def dask_calc_instrument(df_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_binned = dd.from_pandas(df_binned.set_index('date'), npartitions=32)\n",
    "    \n",
    "    df_instrument = (df_binned\n",
    "                     .assign(total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum', meta=pd.Series(dtype='float64')),\n",
    "                             iv_me=lambda x: x['total_alloc'] - x['equal_alloc'])\n",
    "                     .drop(columns=['total_alloc', 'equal_alloc'])\n",
    "                     .compute()\n",
    "                     .reset_index())\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = (df_instrument['iv_me'] > 0)\n",
    "\n",
    "    df_weights = (df_instrument.loc[mask]\n",
    "                 .assign(ln_iv_me=lambda x: np.log(x['iv_me']),\n",
    "                         weight=lambda x: x['holding'] / x['aum'],\n",
    "                         rweight=lambda x: x['holding'] / x['out_aum'],\n",
    "                         ln_rweight=lambda x: np.log(x['rweight'].mask(x['rweight'] == 0, np.NaN)),\n",
    "                         mean_ln_rweight=lambda x: x.groupby(['inv_id', 'date'])['ln_rweight'].transform('mean'),\n",
    "                         const=1,\n",
    "                         pct_uni_held=lambda x: np.log(x['n_holding']) - np.log(x['x_holding']))\n",
    "                .drop(columns=['iv_me']))\n",
    "    \n",
    "    log_holding_weights(df_weights)\n",
    "    return df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def bound_float(arr: np.array) -> float:\n",
    "    ln_bound = 709.7827\n",
    "    return np.minimum(np.maximum(arr, -ln_bound), ln_bound)\n",
    "\n",
    "\n",
    "def momcond_L(params: np.array, exog: np.array) -> np.array:\n",
    "    lower_bound = -0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_yield = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ytm = exog[0]\n",
    "    mean_ln_rweight = exog[1]\n",
    "    arr_characteristics = exog[2:]\n",
    "    \n",
    "    yield_term = beta_yield * ytm\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = yield_term + characteristics_term + mean_ln_rweight\n",
    "    \n",
    "    return pred_weight\n",
    "\n",
    "\n",
    "def fit_inv_date_L(df_inv_date: pd.DataFrame, characteristics: list, params: list,) -> gmm.GMMResults:\n",
    "    df_inv_date = df_inv_date.dropna(subset='ln_rweight')\n",
    "    exog = np.asarray(df_inv_date[['ytm', 'mean_ln_rweight'] + characteristics])\n",
    "    instrument = np.asarray(df_inv_date[['iv_ytm', 'mean_ln_rweight'] + characteristics])\n",
    "    n = exog.shape[0]\n",
    "    endog = np.asarray(df_inv_date['ln_rweight'] - df_inv_date['pct_uni_held'])\n",
    "    start_params = np.zeros(len(params))\n",
    "    w0inv = np.dot(instrument.T, instrument) / n\n",
    "    \n",
    "    if len(exog) == 1:\n",
    "        print('One valid holding')\n",
    "        return None\n",
    "    \n",
    "    # try:\n",
    "    model = gmm.NonlinearIVGMM(\n",
    "        endog=endog,\n",
    "        exog=exog,\n",
    "        instrument=instrument, \n",
    "        func=momcond_L)\n",
    "    result = model.fit(\n",
    "        start_params=start_params,\n",
    "        maxiter=0,\n",
    "        inv_weights=w0inv)\n",
    "    # log_results(result, params)\n",
    "    return result\n",
    "    \n",
    "    # except np.linalg.LinAlgError:\n",
    "    #     print('Linear Algebra Error')\n",
    "    #     return None\n",
    "\n",
    "\n",
    "def estimate_model_L(df_weights: pd.DataFrame, characteristics: list, params: list, min_n_holding: int) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_weights['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_institutions = (df_weights\n",
    "                       .loc[mask]\n",
    "                       .set_index(['inv_id', 'date'])\n",
    "                       .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)).reset_index(drop=True))\n",
    "                       .reset_index())\n",
    "    \n",
    "    df_bins = (df_weights\n",
    "               .loc[~mask]\n",
    "               .set_index(['bin', 'date'])\n",
    "               .assign(gmm_result=lambda x: x.groupby(['bin', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "               .reset_index())\n",
    "\n",
    "    df_model = pd.concat([df_institutions, df_bins], ignore_index=True)\n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def momcond_NL(params: np.array, exog: np.array) -> np.array:\n",
    "    upper_bound = 0.9999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ytm = bound_float(params[0])\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ytm = exog[0]\n",
    "    rweight = exog[1]\n",
    "    mean_ln_rweight = exog[2]\n",
    "    arr_characteristics = exog[3:]\n",
    "    \n",
    "    ytm_term = beta_ytm * ytm\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_ln_rweight = bound_float(ytm_term + characteristics_term + mean_ln_rweight)\n",
    "    pred_rweight = np.exp(-1 * pred_ln_rweight)\n",
    "    \n",
    "    return rweight * pred_rweight\n",
    "\n",
    "\n",
    "def fit_inv_date_NL(df_inv_date: pd.DataFrame, characteristics: list, params: list) -> gmm.GMMResults:\n",
    "    exog = np.asarray(df_inv_date[['ytm', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "    instrument = np.asarray(df_inv_date[['iv_ytm', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "    n = exog.shape[0]\n",
    "    endog = np.ones(n)\n",
    "    \n",
    "    try:\n",
    "        model = gmm.NonlinearIVGMM(\n",
    "            endog=endog,\n",
    "            exog=exog,\n",
    "            instrument=instrument, \n",
    "            func=momcond_NL)\n",
    "        w0inv = np.dot(instrument.T, instrument) / n\n",
    "        start_params = np.zeros(len(params))\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=100,\n",
    "            inv_weights=w0inv)\n",
    "        # log_results(result, params)\n",
    "        return result\n",
    "    \n",
    "    except np.linalg.LinAlgError:\n",
    "        print('Linear Algebra Error')\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def estimate_model_NL(df_weights: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_weights['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_institutions = (df_weights\n",
    "                       .loc[mask]\n",
    "                       .set_index(['inv_id', 'date'])\n",
    "                       .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                       .reset_index())\n",
    "    \n",
    "    df_bins = (df_weights\n",
    "               .loc[~mask]\n",
    "               .set_index(['bin', 'date'])\n",
    "               .assign(gmm_result=lambda x: x.groupby(['bin', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "               .reset_index())\n",
    "    \n",
    "    df_model = pd.concat([df_institutions, df_bins], ignore_index=True)\n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def calc_latent_demand(df_model: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    def unpack_result(result: gmm.GMMResults) -> list:\n",
    "        return result.params\n",
    "\n",
    "    def unpack_params(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[params] = pd.DataFrame(df['lst_params'].tolist(), index=df.index)\n",
    "        return df\n",
    "    \n",
    "        \n",
    "    df_results = (df_model\n",
    "                  .dropna()\n",
    "                  .assign(lst_params=lambda x: x['gmm_result'].apply(unpack_result))\n",
    "                  .pipe(unpack_params)\n",
    "                  .drop(columns='lst_params')\n",
    "                  .assign(beta_const=lambda x: x['beta_const'] + x['mean_ln_rweight'],\n",
    "                          pred_ln_rweight=lambda x: np.einsum('ij,ij->i', x[['ytm'] + characteristics], x[params]),\n",
    "                          latent_demand=lambda x: x['ln_rweight'] - x['pred_ln_rweight'],\n",
    "                          moment=lambda x: x['latent_demand'] - x['pct_uni_held'])\n",
    "                  .drop(columns=['pct_uni_held', 'gmm_result', 'pred_ln_rweight']))\n",
    "    \n",
    "    log_latent_demand(df_results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Liquidity\n",
    "\n",
    "def calc_coliquidity_matrix(df_date: pd.DataFrame) -> pd.Series:\n",
    "    df_asset_grouped = df_date.groupby('asset_id')\n",
    "    n_assets = len(df_asset_grouped)\n",
    "    print(n_assets)\n",
    "\n",
    "    df_liquid = (df_date\n",
    "                 .assign(first=lambda x: df_asset_grouped.cumcount() == 0,\n",
    "                         total_holding=lambda x: df_asset_grouped['holding'].transform('sum'),\n",
    "                         share_of_holding=lambda x: x['holding'] / x['total_holding'],\n",
    "                         row=lambda x: df_asset_grouped.ngroup())\n",
    "                 .sort_values('row'))\n",
    "    \n",
    "    Zmat = np.empty(shape=(n_assets, n_assets))\n",
    "    Amat = np.empty(shape=(n_assets, n_assets))\n",
    "    \n",
    "    for col in range(n_assets):\n",
    "        df_liquid = df_liquid.assign(mask=lambda x: x['row'] == col,\n",
    "                                     mask_weight=lambda x: x['mask'] * x['weight'],\n",
    "                                     rcweight = lambda x: x['mask'] - x.groupby('inv_id')['mask_weight'].transform('max'),\n",
    "                                     Zcol=lambda x: x['beta_ln_me'] * x['share_of_holding'] * x['rcweight'],\n",
    "                                     Acol=lambda x: x['share_of_holding'] * x['rcweight'])\n",
    "        Zmat[col] = df_liquid.groupby('asset_id')['Zcol'].sum()\n",
    "        Amat[col] = df_liquid.groupby('asset_id')['Acol'].sum()\n",
    "        print('Added asset ', col)\n",
    "\n",
    "    # types = df_date['typecode'].unique()\n",
    "    # for i, t in enumerate(types):\n",
    "    #     Bcol_name = 'Bcol_' + str(t)        \n",
    "    #     mask = df_liquid['typecode'] == t\n",
    "    #     df_liquid[Bcol_name] = mask * df_liquid['Acol']\n",
    "    # \n",
    "    # for t in types:\n",
    "    #     Bcol_name = 'Bcol_' + str(t)\n",
    "    #     \n",
    "    #     n_type = df_asset_grouped.apply(lambda x: (x['typecode'] == t).sum())\n",
    "    #     df_matrix[Bcol_name] = df_asset_grouped[Bcol_name].sum() / np.maximum(n_type, 1)\n",
    "    \n",
    "    I = np.identity(n_assets)\n",
    "    liquidity_matrixA = np.linalg.solve(I - Zmat, Amat)\n",
    "    price_impact = np.diagonal(liquidity_matrixA)\n",
    "    \n",
    "    idx = df_liquid.set_index('row')['asset_id'].to_dict()\n",
    "    sr_liquid = pd.Series(price_impact, index=idx, name='price_impact')\n",
    "    \n",
    "    return sr_liquid\n",
    "\n",
    "\n",
    "def calc_price_elasticity(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = df_results['holding'] > 0\n",
    "    \n",
    "    df_liquidity = (df_results\n",
    "                    .loc[mask]\n",
    "                    .groupby('date')\n",
    "                    .apply(calc_coliquidity_matrix))\n",
    "    \n",
    "    log_liquidity(df_liquidity)\n",
    "    return df_liquidity\n",
    "    \n",
    "\n",
    "def graph_liquidity(df_liquidity: pd.DataFrame):\n",
    "    df_fig = (df_liquidity\n",
    "              .groupby('date')\n",
    "              .agg({'p10': lambda x: np.quantile(x['price_impact'], q=0.1),\n",
    "                    'p50': lambda x: np.quantile(x['price_impact'], q=0.5),\n",
    "                    'p90': lambda x: np.quantile(x['price_impact'], q=0.9)})\n",
    "              .stack()\n",
    "              .reset_index(name='stat'))\n",
    "    \n",
    "    g = sns.relplot(data=df_fig,\n",
    "                    x='date',\n",
    "                    y='p10',\n",
    "                    hue='stat',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Price Impact')\n",
    "    g.legend()\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'price_elasticity.png'))\n",
    "    return df_liquidity"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "\n",
    "def equillibrium_loop(df_loop: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    for i in range(1000):\n",
    "        df_loop = (df_loop\n",
    "                   .assign(weightN=lambda x: np.exp(x['loop_beta_ln_me'] * x['loop_ln_prc']) + x['loop_latent'],\n",
    "                           weightD=lambda x: x.groupby(['inv_id', 'date'])['weightN'].transform('sum'),\n",
    "                           weight=lambda x: x['weightN'] / (1 + x['weightD']),\n",
    "                           holding=lambda x: x['weight'] * x['loop_aum'],\n",
    "                           demand=lambda x: x.groupby(['inv_id', 'date'])['holding'].transform('sum'),\n",
    "                           Ddemand_a=lambda x: x['loop_beta_ln_me'] * x['loop_aum'] * x['weight'] * (1 - x['weight']) / x['demand'],\n",
    "                           Ddemand_b=lambda x: x.groupby(['date', 'asset_id'])['Ddemand_a'].transform('sum'),\n",
    "                           Ddemand_c=lambda x: 1 / (1 - np.minimum(0, x['Ddemand_b'])),\n",
    "                           gap=lambda x: np.log(x['demand']) - x['loop_ln_price'] - x['loop_ln_amtout']))\n",
    "        \n",
    "        gap = max(abs(df_loop['gap'].min()), abs(df_loop['gap'].max()))\n",
    "        \n",
    "        if i == 999:\n",
    "            print(f'Equilibrium did not converge after 1000 iterations')\n",
    "            print(f'Gap:  ', gap)\n",
    "        \n",
    "        if gap < 0.00001:\n",
    "            print(f'Equilibrium converged after {i} iterations')\n",
    "            df_loop = df_loop.assign(loop_ln_prc=lambda x: x.groupby('asset_id')['loop_ln_prc'].transform('min'))\n",
    "    \n",
    "    return df_loop\n",
    "\n",
    "\n",
    "def run_counterfactuals(df_date: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    for i in range(1, 6):\n",
    "        if i <= 4:\n",
    "            df_date = df_date.assign(loop_ln_prc=lambda x: x['lag_ln_prc'],\n",
    "                                     loop_ln_amtout=lambda x: x['lag_ln_amtout'])\n",
    "        else:\n",
    "            df_date = df_date.assign(loop_ln_prc=lambda x: x['prc'],\n",
    "                                     loop_ln_amtout=lambda x: x['amtout'])\n",
    "    \n",
    "        if i == 1:\n",
    "            for char in characteristics:\n",
    "                df_date['loop_' + char] = df_date['lag_' + char]\n",
    "        elif i <= 4:\n",
    "            for char in characteristics:\n",
    "                df_date['loop_' + char] = df_date['lag_' + char]\n",
    "        else:\n",
    "            for char in characteristics:\n",
    "                df_date['loop_' + char] = df_date[char]\n",
    "                \n",
    "        if i <= 2:\n",
    "            df_date['loop_aum'] = df_date['lag_aum']\n",
    "        elif i <= 4:\n",
    "            df_date['loop_aum'] = df_date['aum']\n",
    "        else:\n",
    "            df_date['loop_aum'] = df_date['aum']\n",
    "        \n",
    "        if i <= 3:\n",
    "            for param in params:\n",
    "                df_date['loop_' + param] = df_date['lag_' + param]\n",
    "        elif i <= 4:\n",
    "            for param in params:\n",
    "                df_date['loop_' + param] = df_date['lag_' + param]\n",
    "        else:\n",
    "            for param in params:\n",
    "                df_date['loop_' + param] = df_date[param]\n",
    "        \n",
    "        loop_chars = ['loop_' + char for char in characteristics]\n",
    "        loop_params = ['loop_' + param for param in params]\n",
    "        if i <= 4:\n",
    "            df_date = df_date.assign(loop_latent=lambda x: np.einsum('ij,ij->i', df_date[['loop_ln_amtout'] + loop_chars], df_date[loop_params]) + x['lag_latent_demand']) \n",
    "        else:\n",
    "            df_date = df_date.assign(loop_latent=lambda x: np.einsum('ij,ij->i', df_date[['loop_ln_amtout'] + loop_chars], df_date[loop_params]) + x['latent_demand'])\n",
    "\n",
    "    return df_date\n",
    "\n",
    "\n",
    "def decompose_variance(df_results: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    mask = df_results['holding'] > 0\n",
    "    \n",
    "    df_variance = (df_results\n",
    "                   .loc[mask]\n",
    "                   .assign(ln_prc=lambda x: np.log(x['prc']),\n",
    "                           ln_amtout=lambda x: np.log(x['amtout'])))\n",
    "    \n",
    "    df_grouped = df_variance.groupby(['date', 'asset_id'])\n",
    "    \n",
    "    df_variance = (df_variance\n",
    "                   .assign(lag_ln_prc=lambda x: df_grouped['ln_prc'].shift(4),\n",
    "                           lag_ln_amtout=lambda x: df_grouped['ln_amtout'].shift(4),\n",
    "                           lag_aum=lambda x: df_grouped['aum'].shift(4),\n",
    "                           lag_latent=lambda x: df_grouped['latent_demand'].shift(4)))\n",
    "    \n",
    "    for char in characteristics:\n",
    "        name = 'lag_' + char\n",
    "        df_variance[name] = df_variance.groupby(['date', 'asset_id'])[char].shift(4)\n",
    "    \n",
    "    return df_variance"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predictability\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T20:37:16.806445200Z",
     "start_time": "2024-04-07T20:37:08.252513Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Figures\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclean_figures\u001B[39m(df_results: pd\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (df_results\n\u001B[0;32m      5\u001B[0m             \u001B[38;5;241m.\u001B[39massign(date\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m p: p\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;28;01mNone\u001B[39;00m))))\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtypecode_share_counts\u001B[39m(df_figure: pd\u001B[38;5;241m.\u001B[39mDataFrame, figure_path: \u001B[38;5;28mstr\u001B[39m):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Figures\n",
    "\n",
    "def clean_figures(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_results\n",
    "            .assign(date=lambda x: x['date'].apply(lambda p: p.strftime(None))))\n",
    "\n",
    "\n",
    "def typecode_share_counts(df_figure: pd.DataFrame, figure_path: str):\n",
    "    df_type_share = (df_figure\n",
    "                     .groupby(['typecode', 'date'])\n",
    "                     .agg({'aum': 'sum', 'n_holding': 'count'})\n",
    "                     .assign(share=lambda x: x['aum'] / x['aum'].groupby('date').transform('sum')))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, \n",
    "                    x='date', \n",
    "                    y='n_holding', \n",
    "                    hue='typecode', \n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Number of Investors')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_count.png'))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, \n",
    "                    x='date', \n",
    "                    y='share', \n",
    "                    hue='typecode', \n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Share of AUM')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_share.png'))\n",
    "    \n",
    "    return df_figure\n",
    "\n",
    "\n",
    "def check_moment_condition(df_figures: pd.DataFrame, min_n_holding: int):\n",
    "    mask = df_figures['n_holding'] >= min_n_holding\n",
    "    df_mom = (df_figures\n",
    "              .groupby(['inv_id', 'date'])\n",
    "              .agg({'moment': 'mean'}))\n",
    "    \n",
    "    epsilon = 0.01\n",
    "    mask = (df_mom['moment'] > -epsilon) & (df_mom['moment'] < epsilon)\n",
    "    valid_rate = len(df_mom[mask]) / len(df_mom)\n",
    "    print(f'Percentage of valid moments:  {100*valid_rate:.4f}%')\n",
    "    \n",
    "    g = sns.displot(\n",
    "        data=df_mom,\n",
    "        x='moment'\n",
    "    )\n",
    "    g.set_axis_labels('Log Latent Demand', 'Frequency')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'moment_condition.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def critical_value_test(df_figures: pd.DataFrame, characteristics: list, min_n_holding: int, figure_path: str):\n",
    "    \n",
    "    def iv_reg(df_inv_date: pd.DataFrame):\n",
    "        y = df_inv_date['ytm']\n",
    "        X = df_inv_date[['iv_ytm'] + characteristics]\n",
    "        model = OLS(y, X)\n",
    "        result = model.fit()\n",
    "        t_stat = result.tvalues.iloc[0]\n",
    "        return abs(t_stat)\n",
    "    \n",
    "    \n",
    "    df_iv = (df_figures\n",
    "             .groupby(['inv_id', 'date'])\n",
    "             .apply(iv_reg)\n",
    "             .to_frame('t_stat')\n",
    "             .groupby('date')\n",
    "             .median()\n",
    "             .reset_index())\n",
    "    \n",
    "    g = sns.relplot(data=df_iv,\n",
    "                    x='date',\n",
    "                    y='t_stat',\n",
    "                    kind='line')\n",
    "    g.refline(y=4.05, linestyle='--')\n",
    "    g.set_axis_labels('Date', 'First stage t-statistic')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'instrument_validity.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def test_index_fund(df_figures: pd.DataFrame, characteristics: list, params: list, indexfund_id: int, figure_path: str):\n",
    "    mask = df_figures['inv_id'] == indexfund_id\n",
    "    df_index_fund = (df_figures\n",
    "                     .assign(ln_rweight=lambda x: x['ytm'] + x['mean_ln_rweight'],\n",
    "                             pct_uni_held=1)\n",
    "                     .loc[mask])\n",
    "    \n",
    "    df_index_fund_model = (df_index_fund\n",
    "                           .set_index(['inv_id', 'date'])\n",
    "                           .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                           .reset_index())\n",
    "    df_index_fund_result = calc_latent_demand(df_index_fund_model, characteristics, params)\n",
    "    cols = params + ['latent_demand']\n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_index_fund_result,\n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.despine()\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.savefig(os.path.join(figure_path, f'index_fund_{param}.png'))\n",
    "\n",
    "    return df_figures\n",
    "    \n",
    "\n",
    "def graph_type_params(df_figures: pd.DataFrame, params: list, figure_path: str):\n",
    "    df_types = df_figures.copy()\n",
    "    df_types[params] = df_types[params].apply(lambda x: x * df_figures['aum'])\n",
    "    df_types = (df_types\n",
    "                .groupby(['typecode', 'date'])\n",
    "                [params]\n",
    "                .sum()\n",
    "                .apply(lambda x: x / df_results.groupby(['typecode', 'date'])['aum'].sum()))\n",
    "    \n",
    "    for param in params:\n",
    "        g = sns.relplot(data=df_types, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        hue='typecode',\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.legend.set_title('Institution Type')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'{param}.png'))\n",
    "        \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def graph_std_latent_demand(df_figures: pd.DataFrame, figure_path: str):\n",
    "    df_ld = (df_figures\n",
    "         .groupby(['inv_id', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'std',\n",
    "            'aum': 'last',\n",
    "            'typecode': 'last'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] * x['aum'])\n",
    "         .groupby(['typecode', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'mean',\n",
    "            'aum': 'sum'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] / x['aum']))\n",
    "    \n",
    "    g = sns.relplot(data=df_ld, \n",
    "                    x='date',\n",
    "                    y='latent_demand',\n",
    "                    hue='typecode',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Standard Deviation of Latent Demand')\n",
    "    g.legend.set_title('Institution Type')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'std_latent_demand.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def summary_tables(df_figures: pd.DataFrame):\n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: str, name: str):\n",
    "    df.to_csv(os.path.join(path, name), index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_param_cols(cols: list) -> list:\n",
    "    return ['beta_' + col for col in cols]\n",
    "\n",
    "\n",
    "def get_readable_param(name: str) -> str:\n",
    "    return name.replace('_', ' ').title()\n",
    "\n",
    "\n",
    "def get_readable_typecode(typecode: str):\n",
    "    dict_typecode = {'TTF':'13F Filer',\n",
    "                  'FOK':'401K',\n",
    "                  'ANN':'Annuity/Variable Annuity',\n",
    "                  'AMM':'Annuity/VA - Money Market',\n",
    "                  'BKP':'Bank-Portfolio',\n",
    "                  'SVG':'Bank-Savings/Bldg Society',\n",
    "                  'BKT':'Bank-Trust',\n",
    "                  'CHU':'Church/Religious Org',\n",
    "                  'CRP':'Corporation',\n",
    "                  'CRU':'Credit Union',\n",
    "                  'FCC':'Finance Company',\n",
    "                  'FED':'Federal Reserve',\n",
    "                  'FEN':'Foundation/Endowment',\n",
    "                  'GVT':'Government',\n",
    "                  'HLC':'Health Care Systems',\n",
    "                  'HFD':'Hedge Fund',\n",
    "                  'HOU':'Households',\n",
    "                  'HSP':'Hospital',\n",
    "                  'INS':'Insurance Co-Diversified',\n",
    "                  'LIN':'Insurance Co-Life/Health',\n",
    "                  'PIN':'Insurance Co-Prop & Cas',\n",
    "                  'INM':'Investment Manager',\n",
    "                  'BAL':'Mutual Fund - Balanced',\n",
    "                  'MMM':'Mutual Fund - Money Mkt',\n",
    "                  'MUT':'MutFd-OE/UnitTr/SICAV/FCP',\n",
    "                  'END':'MutFd-CE/Inv Tr/FCP',\n",
    "                  'QUI':'Mutual Fund-Equity',\n",
    "                  'FOF':'Mutual Fund-Fund of Funds',\n",
    "                  'NDT':'Nuclear De-Comm Trust',\n",
    "                  'OTH':'Other',\n",
    "                  'CPF':'Pension Fund-Corporate',\n",
    "                  'GPE':'Pension Fund-Government',\n",
    "                  'UPE':'Pension Fund-Union',\n",
    "                  'RIN':'Reinsurance Company',\n",
    "                  'SBC':'Small Business Invst Co',\n",
    "                  'SPZ':'Spezial Fund',\n",
    "                  'UIT':'Unit Investment Trust'}\n",
    "    return dict_typecode[typecode]\n",
    "\n",
    "\n",
    "def agg_typcode(typecode: str):\n",
    "    dict_type_agg = {'TTF':'Investment Manager',\n",
    "                 'FOK':'Pension Fund',\n",
    "                 'ANN':'Variable Annuity',\n",
    "                 'AMM':'Variable Annuity',\n",
    "                 'BKP':'Bank',\n",
    "                 'SVG':'Bank',\n",
    "                 'BKT':'Bank',\n",
    "                 'CHU':'Other',\n",
    "                 'CRP':'Corporation',\n",
    "                 'CRU':'Bank',\n",
    "                 'FCC':'Investment Manager',\n",
    "                 'FED':'Federal Reserve',\n",
    "                 'FEN':'Other',\n",
    "                 'GVT':'Government',\n",
    "                 'HLC':'Insurance Company',\n",
    "                 'HFD':'Investment Manager',\n",
    "                 'HOU':'Households',\n",
    "                 'HSP':'Insurance Company',\n",
    "                 'INS':'Insurance Company',\n",
    "                 'LIN':'Insurance Company',\n",
    "                 'PIN':'Insurance Company',\n",
    "                 'INM':'Investment Manager',\n",
    "                 'BAL':'Mutual Fund',\n",
    "                 'MMM':'Mutual Fund',\n",
    "                 'MUT':'Mutual Fund',\n",
    "                 'END':'Mutual Fund',\n",
    "                 'QUI':'Mutual Fund',\n",
    "                 'FOF':'Mutual Fund',\n",
    "                 'NDT':'Other',\n",
    "                 'OTH':'Other',\n",
    "                 'CPF':'Pension Fund',\n",
    "                 'GPE':'Pension Fund',\n",
    "                 'UPE':'Pension Fund',\n",
    "                 'RIN':'Insurance Company',\n",
    "                 'SBC':'Investment Manager',\n",
    "                 'SPZ':'Investment Manager',\n",
    "                 'UIT':'Investment Manager'}\n",
    "    return dict_type_agg[typecode]\n",
    "\n",
    "\n",
    "def get_rating_number(rating_class: str):\n",
    "    lst_rating = ['AAA',\n",
    "                   'AA+',\n",
    "                   'AA',\n",
    "                   'AA-',\n",
    "                   'A+',\n",
    "                   'A',\n",
    "                   'A-',\n",
    "                   'BBB+',\n",
    "                   'BBB',\n",
    "                   'BBB-',\n",
    "                   'BB+',\n",
    "                   'BB',\n",
    "                   'BB-',\n",
    "                   'B+',\n",
    "                   'B',\n",
    "                   'B-',\n",
    "                   'CCC+',\n",
    "                   'CCC',\n",
    "                   'CCC-',\n",
    "                   'CC',\n",
    "                   'C',\n",
    "                   'C-',\n",
    "                   'D']\n",
    "    dict_rating = {lst_rating[i]: i for i in range(len(lst_rating))}\n",
    "    dict_rating['NR'] = np.nan\n",
    "    return dict_rating[rating_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_quarter(q: pd.Period):\n",
    "    print('Importing:   ', q)\n",
    "\n",
    "\n",
    "def log_import_emaxx_holding(df_emaxx_holding: pd.DataFrame):\n",
    "    print()\n",
    "    print('Finished import eMAXX holdings')\n",
    "\n",
    "\n",
    "def log_import_emaxx_fund(df_emaxx_fund: pd.DataFrame):\n",
    "    print('Finished import eMAXX fund')\n",
    "\n",
    "\n",
    "def log_import_emaxx_secmast(df_emaxx_fund: pd.DataFrame):\n",
    "    print('Finished import eMAXX secmast')\n",
    "\n",
    "\n",
    "def log_import_emaxx_issuer(df_emaxx_issuer: pd.DataFrame):\n",
    "    print('Finished import eMAXX issuer')\n",
    "\n",
    "\n",
    "def log_import_emaxx_broktran(df_emaxx_broktran: pd.DataFrame):\n",
    "    print('Finished import eMAXX broktran')\n",
    "\n",
    "\n",
    "def log_import_fed(df_fed: pd.DataFrame):\n",
    "    print()\n",
    "    print('Imported Federal Reserve treasury holdings')\n",
    "\n",
    "\n",
    "def log_import_treasury(df_treasury: pd.DataFrame):\n",
    "    print('Imported Treasury auction data')\n",
    "\n",
    "\n",
    "def log_import_crsp(df_crsp: pd.DataFrame):\n",
    "    print('Imported CRSP Treasury price data')\n",
    "\n",
    "\n",
    "def log_import_wrds_bond(df_wrds_bond: pd.DataFrame):\n",
    "    print('Imported WRDS bond returns')\n",
    "    \n",
    "\n",
    "def log_clean_emaxx_holding(df_emaxx_holding_clean):\n",
    "    print()\n",
    "    print('Cleaned eMAXX holdings')\n",
    "    print('Number of holdings:  ', len(df_emaxx_holding_clean))\n",
    "\n",
    "\n",
    "def log_clean_emaxx_fund(df_emaxx_fund_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX fund characteristics')\n",
    "    print('Number of investors:  ', df_emaxx_fund_clean['inv_id'].nunique())\n",
    "    \n",
    "    \n",
    "def log_clean_emaxx_secmast(df_emaxx_secmast: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX secmast characteristics')\n",
    "    \n",
    "    \n",
    "def log_clean_emaxx_issuer(df_emaxx_issuer: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX issuer characteristics')\n",
    "    print('Number of issuers:  ', df_emaxx_issuer['issuer_id'].nunique())\n",
    "    \n",
    "   \n",
    "def log_clean_emaxx_broktran(df_emaxx_broktran: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned eMAXX broktran')\n",
    "    print('Number of assets:  ', df_emaxx_broktran['asset_id'].nunique())\n",
    "    \n",
    "\n",
    "def log_clean_fed(df_fed_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned Federal Reserve treasury holdings')\n",
    "    print('Number of holdings:  ', len(df_fed_clean))\n",
    "   \n",
    "\n",
    "def log_clean_treasury(df_treasury_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned Treasury auction data')\n",
    "    print('Number of treasuries:  ', df_treasury_clean['asset_id'].nunique())\n",
    "\n",
    "\n",
    "def log_clean_crsp(df_crsp_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned CRSP prices')\n",
    "    print('Number of treasuries:  ', df_crsp_clean['asset_id'].nunique())\n",
    "\n",
    "\n",
    "def log_clean_wrds_bond(df_wrds_bond_clean: pd.DataFrame):\n",
    "    print()\n",
    "    print('Cleaned WRDS bond returns')\n",
    "    print('Number of assets:  ', df_wrds_bond_clean['asset_id'].nunique())\n",
    "\n",
    "\n",
    "def log_factors(df_factor: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated treasury factors')\n",
    "    print('Number of asset/dates:  ', len(df_factor))\n",
    "\n",
    "\n",
    "def log_corp_treasury(df_asset: pd.DataFrame):\n",
    "    print('Merged corporate/treasury prices and factors')\n",
    "    print('Number of asset/dates:  ', len(df_asset))\n",
    "\n",
    "\n",
    "def log_fed_holding(df_holding_fed: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged Federal Reserve and corporate holdings')\n",
    "\n",
    "\n",
    "def log_holding_fund_merge(df_holding_fund: pd.DataFrame):\n",
    "    print('Merged holdings and fund typecode')\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of investor/date/asset:  ', len(df_merged))\n",
    "\n",
    "\n",
    "def log_drop_unused(df_dropped: pd.DataFrame):\n",
    "    print()\n",
    "    print('Dropped unused investors/assets')\n",
    "    print('Number of holdings:  ', len(df_dropped))\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_outside: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created outside asset')\n",
    "    print('Number of outside holdings:  ', df_outside['out_mask'].sum())\n",
    "\n",
    "\n",
    "def log_zero_holdings(df_holding: pd.DataFrame):\n",
    "    print()\n",
    "    print('Constructed zero holdings')\n",
    "    print('Number of non-zero holdings:  ', sum(df_holding['paramt'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_holding['paramt'] == 0))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated investor AUM')\n",
    "    print('Number of investors:  ', df_inv_aum['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_agg_small_inv(df_agg: pd.DataFrame):\n",
    "    print()\n",
    "    print('Aggregated small investors')\n",
    "    print('Number of investors:  ', df_agg['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_bins(df_binned: pd.DataFrame):\n",
    "    print()\n",
    "    print('Binned investors')\n",
    "    print('Number of investors:  ', df_binned['inv_id'].nunique())\n",
    "    print('Number of bins:  ', df_binned['bin'].nunique())\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created investment universe')\n",
    "    print('Number of investors:  ', df_inv_uni['inv_id'].nunique())\n",
    "    print('Average investment universe size:  ', df_inv_uni['uni_size'].mean())\n",
    "    \n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created market equity instrument')\n",
    "    print('Number of valid instruments:  ', sum(df_instrument['iv_me'] > 0))\n",
    "    print('Number of invalid instruments:  ', sum(df_instrument['iv_me'] == 0))\n",
    "    \n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated holding weights')\n",
    "    print('Number of non-zero holdings:  ', sum(df_model['holding'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_model['holding'] == 0))\n",
    "\n",
    "\n",
    "def log_results(result, params):\n",
    "    print()\n",
    "    print(result.summary(yname='Latent demand', xname=params))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_params(df_params: pd.DataFrame):\n",
    "    print()\n",
    "    print('Estimated parameters')\n",
    "    print('Number of converged estimations:  ', sum(df_params['gmm_result'].notna()))\n",
    "    print('Number of unconverged estimations:  ', sum(df_params['gmm_result'].isna()))\n",
    "    \n",
    "    \n",
    "def log_latent_demand(df_results: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated latent demand')\n",
    "    print('Number of investors:  ', df_results['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_liquidity(df_liquidity: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated price elasticity')\n",
    "\n",
    "\n",
    "def log_variance(df_variance: pd.DataFrame):\n",
    "    print()\n",
    "    print('Decomposed variance')\n",
    "    \n",
    "    \n",
    "def log_predictability(df_predictability: pd.DataFrame):\n",
    "    print()\n",
    "    print('Tested return predictability')\n",
    "    \n",
    "    \n",
    "def log_finished(df: pd.DataFrame):\n",
    "    print()\n",
    "    print('\\n---------------------------Finished---------------------------\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "sns.set_theme(style='ticks', palette=sns.color_palette('hls', 6), context='paper')\n",
    "\n",
    "input_path = 'data'\n",
    "output_path = 'output/'\n",
    "figure_path = 'figures/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(figure_path, exist_ok=True)\n",
    "\n",
    "start_date = pd.Period('2017Q1')\n",
    "end_date = pd.Period('2017Q4')\n",
    "\n",
    "characteristics = ['coupon',\n",
    "                   'amtout',\n",
    "                   't_maturity',\n",
    "                   # 'is_treasury',\n",
    "                   'credit_rating',\n",
    "                   'spread',\n",
    "                   'const']\n",
    "params = ['beta_ytm'] + get_param_cols(characteristics)\n",
    "\n",
    "min_n_holding = 1000\n",
    "n_quarters = 11\n",
    "indexfund_id = 22225"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "dfs = pandas_read_bond(input_path, start_date, end_date)\n",
    "\n",
    "df_emaxx_holding_clean, df_emaxx_fund_clean, df_fed_clean, df_treasury_clean, df_crsp_clean, df_wrds_bond_clean = clean_imports_bond(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")\n",
    "\n",
    "df_asset = (df_crsp_clean\n",
    "            .pipe(calc_factors, df_treasury_clean, end_date)\n",
    "            .pipe(merge_corp_treasury, df_wrds_bond_clean))\n",
    "\n",
    "df_holding = (df_emaxx_holding_clean\n",
    "              .pipe(merge_holding_fund, df_emaxx_fund_clean)\n",
    "              .pipe(merge_fed_holding, df_fed_clean)\n",
    "              .pipe(construct_zero_holdings, n_quarters))\n",
    "\n",
    "df_results = (df_holding\n",
    "              .pipe(merge_holding_factor, df_asset)\n",
    "              .pipe(drop_unused_inv_asset)\n",
    "              .pipe(create_household_sector)\n",
    "              .pipe(partition_outside_asset)\n",
    "              .pipe(calc_inv_aum)\n",
    "              .pipe(agg_small_inv)\n",
    "              .pipe(bin_inv, min_n_holding)\n",
    "              .pipe(calc_instrument)\n",
    "              .pipe(calc_holding_weights)\n",
    "              .pipe(estimate_model_L, characteristics, params, min_n_holding)\n",
    "              .pipe(calc_latent_demand, characteristics, params)\n",
    "              .pipe(save_df, output_path, 'df_results.csv')\n",
    "              .pipe(clean_figures)\n",
    "              .pipe(typecode_share_counts, figure_path)\n",
    "              .pipe(check_moment_condition, min_n_holding)\n",
    "              .pipe(critical_value_test, characteristics, min_n_holding, figure_path)\n",
    "              .pipe(test_index_fund, characteristics, params, indexfund_id, figure_path)\n",
    "              .pipe(graph_type_params, params, figure_path)\n",
    "              .pipe(graph_std_latent_demand, figure_path)\n",
    "              .pipe(log_finished))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = pandas_read_bond(input_path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_emaxx_holding_clean, df_emaxx_fund_clean, df_fed_clean, df_treasury_clean,  df_crsp_clean, df_wrds_bond_clean = clean_imports_bond(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_factor = calc_factors(df_crsp_clean, df_treasury_clean, end_date)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_asset = merge_corp_treasury(df_wrds_bond_clean, df_factor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_holding_fund = merge_holding_fund(df_emaxx_holding_clean, df_emaxx_fund_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_holding_fed = merge_fed_holding(df_holding_fund, df_fed_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_holding = construct_zero_holdings(df_holding_fed, n_quarters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_holding_factor = merge_holding_factor(df_holding, df_asset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_dropped = drop_unused_inv_asset(df_holding_factor)\n",
    "df_household = create_household_sector(df_dropped)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_outside = partition_outside_asset(df_household)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_inv_aum = calc_inv_aum(df_outside)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_agg = agg_small_inv(df_inv_aum)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_binned = bin_inv(df_agg, min_n_holding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_instrument = calc_instrument(df_binned)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_weights = calc_holding_weights(df_instrument)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model = estimate_model_L(df_weights, characteristics, params, min_n_holding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_results = calc_latent_demand(df_model, characteristics, params)\n",
    "# df_results.to_csv(os.path.join(output_path, 'df_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_figures = clean_figures(df_results)\n",
    "_ = typecode_share_counts(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = check_moment_condition(df_figures, min_n_holding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = critical_value_test(df_figures, characteristics, min_n_holding, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = test_index_fund(df_figures, characteristics, params, indexfund_id, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = graph_type_params(df_figures, params, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = graph_std_latent_demand(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
