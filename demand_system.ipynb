{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wrds\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.api import OLS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.224360200Z",
     "start_time": "2024-03-22T13:05:04.337497600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def request_wrds(path: str,\n",
    "                 start_date: pd.Timestamp,\n",
    "                 end_date: pd.Timestamp) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    with wrds.Connection() as db:\n",
    "        df_s12 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s12\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s12.to_csv(f'{path}s12.csv')\n",
    "\n",
    "        df_s34 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s34\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s34.to_csv(f'{path}s34.csv')\n",
    "\n",
    "        df_security = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM ff.factors_monthly\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_security.to_csv(f'{path}security.csv')\n",
    "\n",
    "        return df_s12, df_s34, df_security\n",
    "\n",
    "\n",
    "def load_wrds(path: str,\n",
    "              start_date: pd.Timestamp,\n",
    "              end_date: pd.Timestamp\n",
    "              ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    try:\n",
    "        df_s12 = pd.read_csv(f'{path}s12.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12(df_s12)\n",
    "\n",
    "        df_s12type5 = pd.read_csv(f'{path}s12type5.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12type5(df_s12type5)\n",
    "\n",
    "        df_s34 = pd.read_csv(f'{path}s34.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s34(df_s34)\n",
    "\n",
    "        df_beta = pd.read_csv(f'{path}beta.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_beta(df_beta)\n",
    "\n",
    "        df_security = pd.read_csv(f'{path}security.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_security(df_security)\n",
    "\n",
    "        return df_s12, df_s12type5, df_s34, df_beta, df_security\n",
    "    except FileNotFoundError:\n",
    "        return request_wrds(path, start_date, end_date)\n",
    "\n",
    "\n",
    "def clean_imports(df_s12,\n",
    "                  df_s12type5,\n",
    "                  df_s34,\n",
    "                  df_beta,\n",
    "                  df_security,\n",
    "                  start_date,\n",
    "                  end_date\n",
    "                  ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    df_s12_clean = clean_s12(df_s12, start_date, end_date)\n",
    "    log_clean_s12(df_s12_clean)\n",
    "\n",
    "    df_s12type5_clean = clean_s12type5(df_s12type5, start_date, end_date)\n",
    "    log_clean_s12type5(df_s12type5_clean)\n",
    "\n",
    "    df_s34_clean = clean_s34(df_s34, start_date, end_date)\n",
    "    log_clean_s34(df_s34_clean)\n",
    "\n",
    "    df_beta_clean = clean_beta(df_beta, start_date, end_date)\n",
    "    log_clean_beta(df_beta_clean)\n",
    "\n",
    "    df_security_clean = clean_security(df_security, start_date, end_date)\n",
    "    log_clean_security(df_security_clean)\n",
    "\n",
    "    return df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean\n",
    "\n",
    "\n",
    "def clean_s12(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'fundno',\n",
    "        'rdate',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['fundno', 'shares', 'cusip'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'fundno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_s12type5(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    return (df\n",
    "            .rename(columns={'fdate': 'date', 'fundno': 'inv_id'})\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .dropna(how='any', subset=['inv_id', 'date'])\n",
    "            .set_index(['inv_id', 'date']))\n",
    "\n",
    "\n",
    "def clean_s34(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'mgrno',\n",
    "        'rdate',\n",
    "        'typecode',\n",
    "        'cusip',\n",
    "        'prc',\n",
    "        'shrout2',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['cusip', 'shares'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'mgrno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)),\n",
    "                backup_holding=lambda x: x['shares'] * x['prc'],\n",
    "                backup_me=lambda x: x['shrout2'] * x['prc'] * 1000,\n",
    "                typecode=lambda x: x['typecode'].fillna(0).astype(int))\n",
    "            .astype({'inv_id': 'str'})\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .drop(columns=['prc', 'shrout2'])\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_beta(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns=[\n",
    "        'PERMNO',\n",
    "        'DATE',\n",
    "        'b_mkt',\n",
    "        'b_smb',\n",
    "        'b_hml'\n",
    "    ]\n",
    "    offset = pd.DateOffset(months=6)\n",
    "    return (df[columns]\n",
    "            .dropna()\n",
    "            .rename(columns={\n",
    "                'DATE': 'date',\n",
    "                'PERMNO':'permno'})\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)) + offset)\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def clean_security(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'LPERMNO',\n",
    "        'cusip',\n",
    "        'datadate',\n",
    "        'prccm',\n",
    "        'cshoq'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['cusip', 'prccm', 'cshoq'])\n",
    "            .rename(columns={\n",
    "                'LPERMNO': 'permno',\n",
    "                'cusip': 'asset_id',\n",
    "                'prccm': 'prc',\n",
    "                'cshoq': 'shrout',\n",
    "                'datadate': 'date'})\n",
    "            .assign(\n",
    "                shrout=lambda x: x['shrout'] * 1000000,\n",
    "                asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]),\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def fix_date(date: str, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.Timestamp:\n",
    "    date_converted = pd.Timestamp(date) - pd.offsets.MonthEnd()\n",
    "    date_filtered = np.NaN if (date_converted < start_date) or (date_converted > end_date) else date_converted\n",
    "    return date_filtered"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.245934600Z",
     "start_time": "2024-03-22T13:05:05.226677Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Stocks Monthly\n",
    "\n",
    "def merge_assets_factors(df_assets: pd.DataFrame, df_factors: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(left=df_assets,\n",
    "                      right=df_factors,\n",
    "                      how='inner',\n",
    "                      left_index=True,\n",
    "                      right_index=True)\n",
    "    df_merged_indexed = (df_merged\n",
    "                         .reset_index()\n",
    "                         .assign(date=lambda x: x['date'] - pd.offsets.BQuarterEnd())\n",
    "                         .drop_duplicates(subset=['date', 'asset_id'], keep='last')\n",
    "                         .set_index(['date', 'asset_id']))\n",
    "    log_asset_merge(df_merged_indexed)\n",
    "    return df_merged_indexed\n",
    "\n",
    "\n",
    "# Manager / Holdings\n",
    "\n",
    "def match_fund_manager(df_fund: pd.DataFrame, df_manager: pd.DataFrame, df_key: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO\n",
    "    # df_fund_wkey = df_fund.assign(mgrno=lambda x: df_key.loc[x.index.get_level_values(0), 'mgrcocd'])\n",
    "    # df_merged = df_manager.merge(df_fund_wkey, how='outer', on='mgrno')\n",
    "    \n",
    "    df_fund_manager = (df_manager\n",
    "                       .reset_index()\n",
    "                       .assign(date=lambda x: x['date'] - pd.offsets.BQuarterEnd())\n",
    "                       .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "                       .set_index(['inv_id', 'date', 'asset_id']))\n",
    "    log_holding_merge(df_fund_manager)\n",
    "    return df_fund_manager\n",
    "\n",
    "\n",
    "def merge_holding_factor(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_holding,\n",
    "        right=df_asset,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    df_holding_factor = (df_merged\n",
    "                         .assign(\n",
    "                           shares=lambda x: np.minimum(x['shares'], x['shrout']),\n",
    "                           ccm_holding=lambda x: x['prc'] * x['shares'],\n",
    "                           ccm_me=lambda x: x['prc'] * x['shrout'],\n",
    "                           holding=lambda x: x['ccm_holding'].fillna(x['backup_holding']) / 1000000,\n",
    "                           me=lambda x: x['ccm_me'].fillna(x['backup_me']) / 1000000,\n",
    "                           type_code=lambda x: x['typecode'].fillna(0))\n",
    "                         .drop(columns=['ccm_holding', 'backup_holding', 'ccm_me', 'backup_me'])\n",
    "                         .dropna(subset=['holding', 'me'])\n",
    "                         .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    log_holding_factor_merge(df_holding_factor)\n",
    "    return df_holding_factor\n",
    "\n",
    "\n",
    "def create_household_sector(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_holding_factor\n",
    "                      .groupby(['date', 'asset_id'])\n",
    "                      .agg({\n",
    "                        'shares': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'shrout': 'last',\n",
    "                        'b_mkt': 'last',\n",
    "                        'b_smb': 'last',\n",
    "                        'b_hml': 'last',\n",
    "                        'holding': 'sum',\n",
    "                        'me': 'last',\n",
    "                        '_merge': 'last'})\n",
    "                      .assign(\n",
    "                        shares=lambda x: np.maximum(x['shrout'] - x['shares'], 0),\n",
    "                        holding=lambda x: np.maximum(x['me'] - x['holding'], 0),\n",
    "                        inv_id='0',\n",
    "                        typecode=0)\n",
    "                    .set_index('inv_id', append=True)\n",
    "                    .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    log_household_sector(df_household)\n",
    "    df_concat = pd.concat([df_holding_factor, df_household])\n",
    "    return df_concat\n",
    "\n",
    "def create_outside_asset(df_household: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    mask = df_household['_merge'] == 'both'\n",
    "    df_inside = df_household[mask].drop(columns='_merge')\n",
    "    df_outside = (df_household[~mask]\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({\n",
    "                    'typecode': 'last',\n",
    "                    'holding': 'sum'})\n",
    "                  .assign(asset_id='-1')\n",
    "                  .set_index('asset_id', append=True))\n",
    "    log_outside_asset(df_outside)\n",
    "    return df_inside, df_outside"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.277595700Z",
     "start_time": "2024-03-22T13:05:05.252577300Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def calc_inv_aum(df_inside: pd.DataFrame, df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_in_aum = (df_inside\n",
    "                 .groupby(['inv_id', 'date'])\n",
    "                 .agg({\n",
    "                   'holding': 'sum',\n",
    "                   'shares': 'count',\n",
    "                   'typecode': 'last'})\n",
    "                 .rename(columns={\n",
    "                   'holding': 'in_aum',\n",
    "                   'shares': 'n_holding'}))\n",
    "\n",
    "    df_out_aum = (df_outside\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({\n",
    "                    'holding': 'sum',\n",
    "                    'typecode': 'last'})\n",
    "                  .rename(columns={'holding': 'out_aum'}))\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        left=df_in_aum,\n",
    "        right=df_out_aum,\n",
    "        how='outer',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    df_inv_aum = (df_merged\n",
    "                  .assign(\n",
    "                    typecode=lambda x: x['typecode_x'].fillna(x['typecode_y']),\n",
    "                    n_holding=lambda x: x['n_holding'].fillna(0),\n",
    "                    out_aum=lambda x: x['out_aum'].fillna(0),\n",
    "                    in_aum=lambda x: x['in_aum'].fillna(0),\n",
    "                    aum=lambda x: x['out_aum'] + x['in_aum'])\n",
    "                  .drop(columns=['typecode_x', 'typecode_y']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def bin_concentrated_inv(df_inside: pd.DataFrame, df_inv_aum: pd.DataFrame, min_n_holding: int) -> (pd.DataFrame, pd.DataFrame):\n",
    "    household_mask = (df_inv_aum['out_aum'] == 0) | (df_inv_aum['in_aum'] == 0) | (df_inv_aum['aum'] < 10)\n",
    "    df_valid = (df_inv_aum\n",
    "                .reset_index()\n",
    "                .assign(\n",
    "                    inv_id=lambda x: x['inv_id'].mask(household_mask.values, '0'),\n",
    "                    typecode=lambda x: x['typecode'].mask(household_mask.values, 0))\n",
    "                .set_index(['inv_id', 'date']))\n",
    "    \n",
    "    diversified_mask = df_valid['n_holding'] >= min_n_holding\n",
    "    df_aum_diversified = df_valid[diversified_mask]\n",
    "    \n",
    "    def calc_bin(df_date_type: pd.DataFrame) -> pd.Series:\n",
    "        typecode = df_date_type['typecode'].iloc[0]\n",
    "        n_bins = np.ceil(df_date_type['n_holding'].sum() / (2 * min_n_holding)).astype(int)\n",
    "        if (n_bins <= 1) or (typecode == 0):\n",
    "            return pd.Series('0', index=df_date_type.index)\n",
    "        else:\n",
    "            return pd.qcut(x=df_date_type['aum'], q=n_bins, labels=False).apply(fix_qcut_bin, args=(str(typecode),))\n",
    "\n",
    "    def fix_qcut_bin(bin: int, typecode: str) -> str:\n",
    "        return typecode + ':' + str(bin)\n",
    "    \n",
    "    df_aum_concentrated = df_valid[~diversified_mask].assign(\n",
    "        bin = lambda x: x.groupby(['date', 'typecode']).apply(calc_bin).reset_index(drop=True))\n",
    "    df_concentrated_binned = (df_aum_concentrated\n",
    "                              .groupby(['bin', 'date'])\n",
    "                              .agg({\n",
    "                                'in_aum': 'sum',\n",
    "                                'out_aum': 'sum',\n",
    "                                'aum': 'sum',\n",
    "                                'typecode': 'last'})\n",
    "                              .rename_axis(index={'bin': 'inv_id'}))\n",
    "    df_aum_binned = (pd.concat([\n",
    "                        df_aum_diversified, \n",
    "                        df_concentrated_binned])\n",
    "                     .assign(\n",
    "                       out_weight=lambda x: x['out_aum'] / x['aum'],\n",
    "                       in_weight=lambda x: x['in_aum'] / x['aum']))\n",
    "    df_inside_merged = pd.merge(\n",
    "        left=df_inside,\n",
    "        right=df_aum_concentrated['bin'],\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    df_inside_binned = (df_inside_merged\n",
    "                        .reset_index()\n",
    "                        .assign(bin=lambda x: x['bin'].fillna(x['inv_id']))\n",
    "                        .groupby(['bin', 'date', 'asset_id'])\n",
    "                        .agg({\n",
    "                          'holding': 'sum',\n",
    "                          'prc': 'last',\n",
    "                          'shrout': 'last',\n",
    "                          'me': 'last',\n",
    "                          'b_mkt': 'last',\n",
    "                          'b_smb': 'last',\n",
    "                          'b_hml': 'last'})\n",
    "                        .rename_axis(index={'bin': 'inv_id'}))\n",
    "    log_bins(df_inside_binned, df_aum_binned)\n",
    "    return df_inside_binned, df_aum_binned\n",
    "\n",
    "\n",
    "def assetid_byinv(df_holding: pd.DataFrame) -> pd.Series:\n",
    "    return df_holding.reset_index('asset_id')['asset_id']\n",
    "\n",
    "\n",
    "def calc_inv_universe(df_holding: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    df_assetid_byinv = assetid_byinv(df_holding).sort_index()\n",
    "    idx_inv_universe = df_assetid_byinv.index.unique()\n",
    "    df_inv_universe = pd.DataFrame(index=idx_inv_universe, columns=['inv_universe'])\n",
    "    offset = pd.DateOffset(months=3 * n_quarters)\n",
    "\n",
    "    def calc_past_quarters(i: int, d: pd.Timestamp) -> np.array:\n",
    "        prev_date = d - offset\n",
    "        asset_id_within_range = df_assetid_byinv.loc[i].loc[prev_date:d]\n",
    "        inv_uni = asset_id_within_range.unique().tolist()\n",
    "        return inv_uni\n",
    "\n",
    "    for (inv_id, date) in idx_inv_universe.to_flat_index():\n",
    "        inv_uni = calc_past_quarters(inv_id, date)\n",
    "        df_inv_universe.loc[(inv_id, date), 'inv_universe'] = inv_uni\n",
    "\n",
    "    df_inv_universe = df_inv_universe.assign(uni_size=lambda x: x['inv_universe'].apply(len),\n",
    "                                             iweight=lambda x: x['n_holding'] / x['uni_size'],\n",
    "                                             cutoff=lambda x: (x['iweight'] >= .95).astype(int))\n",
    "    log_inv_universe(df_inv_universe)\n",
    "    return df_inv_universe\n",
    "\n",
    "\n",
    "def create_equal_allocation(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_alloc = pd.merge(\n",
    "        left=df_inv_universe,\n",
    "        right=df_aum_binned,\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    return (df_equal_alloc\n",
    "            .assign(allocation=lambda x: x['cutoff'] * x['aum'] / (x['uni_size'] + 1))\n",
    "            .explode('inv_universe')\n",
    "            .rename(columns={'inv_universe':'asset_id'})\n",
    "            .set_index('asset_id', append=True))\n",
    "\n",
    "\n",
    "def create_total_allocation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df['allocation']\n",
    "            .groupby('asset_id')\n",
    "            .sum())\n",
    "\n",
    "\n",
    "def create_instrument(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_allocation = create_equal_allocation(df_inv_universe, df_aum_binned)\n",
    "    total_allocation = create_total_allocation(df_equal_allocation)\n",
    "    df_instrument = df_equal_allocation.assign(iv_me=lambda x: total_allocation - x['allocation'])\n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:43:27.678022700Z",
     "start_time": "2024-03-22T13:43:27.551601300Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame, df_inside_binned: pd.DataFrame, min_holding: int) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_instrument,\n",
    "        right=df_inside_binned,\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    mask = (df_merged['n_holding'] >= min_holding) & (df_merged['out_weight'] > 0) & (df_merged['in_weight'] > 0) & (df_merged['me'] > 0) & (df_merged['iv_me'] > 0)\n",
    "\n",
    "    df_weights = (df_merged.loc[mask]\n",
    "                 .assign(\n",
    "                   ln_me=lambda x: np.log(x['me']),\n",
    "                   ln_iv_me=lambda x: np.log(x['iv_me']),\n",
    "                   weight=lambda x: x['holding'] / x['aum'],\n",
    "                   ln_weight=lambda x: np.log(x['weight']),\n",
    "                   rweight=lambda x: x['weight'] / x['out_weight'],\n",
    "                   ln_rweight=lambda x: np.log(x['rweight']),\n",
    "                   mean_ln_rweight=lambda x: x['ln_rweight'].groupby(['inv_id', 'date']).transform('mean'),\n",
    "                   const=1)\n",
    "                .reset_index('asset_id'))\n",
    "    \n",
    "    log_holding_weights(df_weights)\n",
    "    return df_weights\n",
    "\n",
    "\n",
    "def unpack_result(result: gmm.GMMResults) -> dict:\n",
    "    return result.params\n",
    "\n",
    "\n",
    "def momcond_1(params, exog):\n",
    "    upper_bound = 0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    mean_ln_rweight = exog[1]\n",
    "    arr_characteristics = exog[2:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = ln_me_term + characteristics_term + mean_ln_rweight\n",
    "    \n",
    "    return pred_weight\n",
    "\n",
    "\n",
    "def momcond_2(params, exog):\n",
    "    upper_bound = 0.9999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    rweight = exog[1]\n",
    "    mean_ln_rweight = exog[2]\n",
    "    arr_characteristics = exog[3:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = np.exp(-1 * (ln_me_term + characteristics_term + mean_ln_rweight))\n",
    "    \n",
    "    return rweight * pred_weight\n",
    "        \n",
    "\n",
    "def estimate_model(df_weights: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    def fit_inv_date_1(df_inv_date: pd.DataFrame) -> gmm.GMMResults:\n",
    "        exog = np.asarray(df_inv_date[['ln_me', 'mean_ln_rweight'] + characteristics])\n",
    "        instrument = np.asarray(df_inv_date[['ln_iv_me', 'mean_ln_rweight'] + characteristics])\n",
    "        n = exog.shape[0]\n",
    "        endog = np.asarray(df_inv_date['ln_rweight'])\n",
    "        start_params = np.zeros(len(params))\n",
    "        w0inv = np.dot(instrument.T, instrument) / n\n",
    "        \n",
    "        try:\n",
    "            model = gmm.NonlinearIVGMM(\n",
    "                endog=endog,\n",
    "                exog=exog,\n",
    "                instrument=instrument, \n",
    "                func=momcond_1)\n",
    "            result = model.fit(\n",
    "                start_params=start_params,\n",
    "                maxiter=0,\n",
    "                inv_weights=w0inv)\n",
    "            log_results(result, params)\n",
    "            return result\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Linear Algebra Error')\n",
    "            print(f'Investor Id:  {df_inv_date.index.get_level_values(0).unique()}')\n",
    "            print(f'Date:  {df_inv_date.index.get_level_values(1).unique()}')\n",
    "            return np.NaN\n",
    "    \n",
    "    \n",
    "    def fit_inv_date_2(df_inv_date: pd.DataFrame) -> gmm.GMMResults:\n",
    "        exog = np.asarray(df_inv_date[['ln_me', 'rweight', 'mean_ln_rweight'] + characteristics], dtype=np.longdouble)\n",
    "        instrument = np.asarray(df_inv_date[['ln_iv_me', 'rweight', 'mean_ln_rweight'] + characteristics], dtype=np.longdouble)\n",
    "        n = exog.shape[0]\n",
    "        endog = np.ones(n, dtype=np.longdouble)\n",
    "        \n",
    "        try:\n",
    "            model = gmm.NonlinearIVGMM(\n",
    "                endog=endog,\n",
    "                exog=exog,\n",
    "                instrument=instrument, \n",
    "                func=momcond_1)\n",
    "            w0inv = np.dot(instrument.T, instrument) / n\n",
    "            start_params = np.zeros(len(params))\n",
    "            result = model.fit(\n",
    "                start_params=start_params,\n",
    "                maxiter=0,\n",
    "                inv_weights=w0inv)\n",
    "            log_results(result, params)\n",
    "            return result\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular Matrix Error')\n",
    "            print(f'Investor Id:  {df_inv_date.index.get_level_values(0).unique()}')\n",
    "            print(f'Date:  {df_inv_date.index.get_level_values(1).unique()}')\n",
    "            return np.NaN\n",
    "    \n",
    "    \n",
    "    df_model = df_weights.assign(\n",
    "        gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(fit_inv_date_1),\n",
    "        lst_params=lambda x: x['gmm_result'].apply(unpack_result))\n",
    "    df_model[params] = pd.DataFrame(df_model['lst_params'].tolist(), index=df_model.index)\n",
    "    df_model = df_model.drop(columns='lst_params')\n",
    "    \n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def calc_latent_demand(df_model: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    upper_bound = 0.9999\n",
    "    \n",
    "    df_results = df_model.assign(\n",
    "        beta_ln_me=lambda x: upper_bound - np.exp(-1 * x['beta_ln_me']),\n",
    "        char_demand=lambda x: np.einsum('ij,ij->i', x[['ln_me'] + characteristics], x[params]),\n",
    "        pred_rweight=lambda x: np.exp(-1 * (x['char_demand'] + x['mean_ln_rweight'])),\n",
    "        latent_demand=lambda x: x['rweight'] * x['pred_rweight'])\n",
    "    log_latent_demand(df_results)\n",
    "    return df_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.331315700Z",
     "start_time": "2024-03-22T13:05:05.294873400Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Figures\n",
    "\n",
    "def check_moment_condition(df_results: pd.DataFrame):\n",
    "    df_mom = (df_results\n",
    "              .groupby(['inv_id', 'date'])\n",
    "              .agg({\n",
    "                'latent_demand': 'mean',\n",
    "                'n_holding': 'last',\n",
    "                'uni_size': 'last'})\n",
    "              .assign(\n",
    "                iweight=lambda x: x['n_holding'] / x['uni_size'],\n",
    "                latent_demand=lambda x: x['latent_demand'] * x['iweight']))\n",
    "    \n",
    "    epsilon = .0001\n",
    "    mask = (df_mom['latent_demand'] > 1 - epsilon) & (df_mom['latent_demand'] < 1 + epsilon)\n",
    "    valid_rate = len(df_mom[mask]) / len(df_mom)\n",
    "    print(f'Percentage of valid portfolios:  {100*valid_rate:.4f}')\n",
    "    return df_mom\n",
    "\n",
    "\n",
    "def critical_value_test(df_results: pd.DataFrame, characteristics: list, figure_path: str):\n",
    "    \n",
    "    def iv_reg(df_inv_date: pd.DataFrame):\n",
    "        y = df_inv_date['ln_me']\n",
    "        X = df_inv_date[['ln_iv_me'] + characteristics]\n",
    "        model = OLS(y, X)\n",
    "        result = model.fit()\n",
    "        t_stat = result.tvalues.iloc[0]\n",
    "        return t_stat\n",
    "    \n",
    "    df_iv = (df_results\n",
    "             .groupby(['inv_id', 'date'])\n",
    "             .apply(iv_reg)\n",
    "             .to_frame('t_stat')\n",
    "             .groupby('date')\n",
    "             .min()\n",
    "             .reset_index())\n",
    "    # df_iv['date'] = df_iv['date'].dt.to_timestamp()\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data=df_iv, \n",
    "        x='date', \n",
    "        y='t_stat', \n",
    "        kind='line')\n",
    "    g.refline(\n",
    "        y=4.05,\n",
    "        linestyle='--')\n",
    "    g.set_axis_labels('Date', 'First stage t-statistic')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'instrument_validity.png'))\n",
    "\n",
    "\n",
    "def test_index_fund(df_results: pd.DataFrame, characteristics: list, params: list, figure_path: str):\n",
    "    index_id = '90457'\n",
    "    mask = df_results.index.get_level_values(0) == index_id\n",
    "    df_index_fund = (df_results\n",
    "                     .loc[mask]\n",
    "                     .assign(\n",
    "                        ln_rweight=lambda x: x['ln_me'] + x['mean_ln_rweight']))\n",
    "    \n",
    "    df_index_fund_model = estimate_model(df_index_fund, characteristics, params)\n",
    "    df_index_fund_result = calc_latent_demand(df_index_fund_model, characteristics, params)\n",
    "    # df_index_fund_result['date'] = df_index_fund_result['date'].dt.to_timestamp()\n",
    "    \n",
    "    cols = params + ['latent_demand']\n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_index_fund_result, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'index_fund_{param}.png'))\n",
    "    \n",
    "\n",
    "def graph_type_params(df_results: pd.DataFrame, params: list, figure_path: str):\n",
    "    cols = params + ['latent_demand']\n",
    "\n",
    "    df_types = df_results.copy()\n",
    "    df_types[cols] = df_types[cols].apply(lambda x: x * df_results['aum'])\n",
    "    df_types = (df_types\n",
    "                .groupby(['typecode', 'date'])\n",
    "                [cols]\n",
    "                .sum()\n",
    "                .apply(lambda x: x / df_results.groupby(['typecode', 'date'])['aum'].sum()))\n",
    "    # df_types['date'] = df_types['date'].dt.to_timestamp()\n",
    "    \n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_types, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        hue='typecode',\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'mean_{get_readable_param(param)}')\n",
    "        g.legend.set_title('Institution Type')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'{param}.png'))\n",
    "\n",
    "\n",
    "def graph_std_latent_demand(df_results: pd.DataFrame, figure_path: str):\n",
    "    df_ld = (df_results\n",
    "         .groupby(['inv_id', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'std',\n",
    "            'aum': 'last',\n",
    "            'typecode': 'last'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] * x['aum'])\n",
    "         .groupby(['typecode', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'mean',\n",
    "            'aum': 'sum'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] / x['aum']))\n",
    "    # df_ld['date'] = df_ld['date'].dt.to_timestamp()\n",
    "    \n",
    "    g = sns.relplot(data=df_ld, \n",
    "                    x='date',\n",
    "                    y='latent_demand',\n",
    "                    hue='typecode',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Standard Deviation of Latent Demand')\n",
    "    g.legend.set_title('Institution Type')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'std_latent_demand.png'))\n",
    "\n",
    "\n",
    "def create_tables(df_instrument: pd.DataFrame, df_inv_uni: pd.DataFrame):\n",
    "    df_pctile = df_inv_uni.assign(pctile=0)\n",
    "    arr_dates = df_pctile.index.get_level_values('date').unique()\n",
    "    \n",
    "    for date in arr_dates:\n",
    "        data = df_pctile.loc[date, 'aum']\n",
    "        df_pctile.loc[date, 'pctile'] = pd.qcut(data, q=100)\n",
    "        \n",
    "    df_grouped_pctile = (df_pctile['uni_persistence']\n",
    "                         .groupby(['pctile'])\n",
    "                         .median())\n",
    "    print(df_grouped_pctile.head(10))\n",
    "\n",
    "    \n",
    "def get_param_cols(cols: list) -> list:\n",
    "    return ['beta_' + col for col in cols]\n",
    "\n",
    "\n",
    "def get_readable_param(name: str) -> str:\n",
    "    return name.replace('_', ' ').title()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.361710700Z",
     "start_time": "2024-03-22T13:05:05.306228Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_import_s12(df_s12: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12['rdate']\n",
    "    print('Imported s12')\n",
    "    print('Number of holdings:  ', len(df_s12))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_import_s12type5(df_s12type5: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5['fdate']\n",
    "    print()\n",
    "    print('Imported s12type5')\n",
    "    print('Number of holdings:  ', len(df_s12type5))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_import_s34(df_s34: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34['rdate']\n",
    "    print()\n",
    "    print('Imported s34')\n",
    "    print('Number of holdings:  ', len(df_s34))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_import_beta(df_beta: pd.DataFrame):\n",
    "    dateindex_ffm = df_beta['DATE']\n",
    "    print()\n",
    "    print('Imported betas')\n",
    "    print('Number of dates:  ', len(df_beta))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_import_security(df_security: pd.DataFrame):\n",
    "    dateindex_security = df_security['datadate']\n",
    "    print()\n",
    "    print('Imported security')\n",
    "    print('Number of holdings:  ', len(df_security))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_clean_s12(df_s12_clean: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12_clean.index.get_level_values('date')\n",
    "    print('Cleaned s12')\n",
    "    print('Number of holdings:  ', len(df_s12_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_clean_s12type5(df_s12type5_clean: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s12type5')\n",
    "    print('Number of firm/dates:  ', len(df_s12type5_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_clean_s34(df_s34_clean: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s34')\n",
    "    print('Number of holdings:  ', len(df_s34_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_clean_beta(df_beta_clean: pd.DataFrame):\n",
    "    dateindex_beta = df_beta_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned beta')\n",
    "    print('Number of dates:  ', len(df_beta_clean))\n",
    "    print('Earliest date:  ', min(dateindex_beta))\n",
    "    print('Latest date:  ', max(dateindex_beta))\n",
    "\n",
    "\n",
    "def log_clean_security(df_security_clean: pd.DataFrame):\n",
    "    dateindex_security = df_security_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned security')\n",
    "    print('Number of asset/dates:  ', len(df_security_clean))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_holding_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged s12 and s34')\n",
    "    print('Number of holdings:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_asset_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged assets and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_holding: pd.DataFrame):\n",
    "    print('Created outside asset')\n",
    "    print('Number of holdings:  ', len(df_holding))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    print('Calculated investor AUM')\n",
    "    print(df_inv_aum.describe())\n",
    "\n",
    "\n",
    "def log_bins(df_inside_binned: pd.DataFrame, df_aum_binned: pd.DataFrame):\n",
    "    print('Binned investors')\n",
    "    print('Number of investors by holding:  ', len(df_inside_binned.index.unique('inv_id')))\n",
    "    print('Number of investors by aum:  ', len(df_aum_binned.index.unique('inv_id')))\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    print('Created investment universe')\n",
    "    print(df_inv_uni.describe())\n",
    "    \n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print('Created market equity instrument')\n",
    "    print(df_instrument.describe())\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print('Calculated holding weights')\n",
    "    print(df_model.describe())\n",
    "\n",
    "\n",
    "def log_results(result, params):\n",
    "    print(result.summary(yname='Latent demand', xname=params))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_params(df_params: pd.DataFrame):\n",
    "    print('Estimated parameters')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def log_latent_demand(df_results: pd.DataFrame):\n",
    "    print('Calculated latent demand')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.365000200Z",
     "start_time": "2024-03-22T13:05:05.334614400Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "sns.set_theme(style='ticks', palette=None)\n",
    "\n",
    "input_path = 'data/'\n",
    "output_path = 'output/'\n",
    "figure_path = 'figures/'\n",
    "\n",
    "start_date = pd.Timestamp('2012-01')\n",
    "end_date = pd.Timestamp('2017-12')\n",
    "\n",
    "characteristics = [\n",
    "    'b_mkt',\n",
    "    'b_smb',\n",
    "    'b_hml'\n",
    "] + ['const']\n",
    "params = ['beta_ln_me'] + get_param_cols(characteristics)\n",
    "dict_typecode = {\n",
    "     0: 'Households',\n",
    "     1: 'Banks',\n",
    "     2: 'Insurance companies',\n",
    "     3: 'Investment advisors',\n",
    "     4: 'Mutual funds',\n",
    "     5: 'Pension funds',\n",
    "}\n",
    "\n",
    "min_n_holding = 1000\n",
    "n_quarters = 11"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:05:05.376997Z",
     "start_time": "2024-03-22T13:05:05.342319100Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Imports---------------------------\n",
      "\n",
      "Imported s12\n",
      "Number of holdings:   30987\n",
      "Earliest date:   2014-06-30\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported s12type5\n",
      "Number of holdings:   572518\n",
      "Earliest date:   12/31/1994\n",
      "Latest date:   9/30/2022\n",
      "\n",
      "Imported s34\n",
      "Number of holdings:   22707709\n",
      "Earliest date:   2012-03-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported betas\n",
      "Number of dates:   432458\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-29\n",
      "\n",
      "Imported security\n",
      "Number of holdings:   402452\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "---------------Starting Cleaning---------------------------\n",
      "Cleaned s12\n",
      "Number of holdings:   27327\n",
      "Earliest date:   2014-05-31 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n",
      "\n",
      "Cleaned s12type5\n",
      "Number of firm/dates:   49372\n",
      "Earliest date:   2012-02-29 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n",
      "\n",
      "Cleaned s34\n",
      "Number of holdings:   22267070\n",
      "Earliest date:   2012-02-29 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n",
      "\n",
      "Cleaned beta\n",
      "Number of dates:   432414\n",
      "Earliest date:   2012-07-31 00:00:00\n",
      "Latest date:   2018-05-30 00:00:00\n",
      "\n",
      "Cleaned security\n",
      "Number of asset/dates:   126399\n",
      "Earliest date:   2012-01-31 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "print('\\n---------------Starting Imports---------------------------\\n')\n",
    "dfs = load_wrds(input_path, start_date, end_date)\n",
    "\n",
    "print('\\n---------------Starting Cleaning---------------------------\\n')\n",
    "df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean = clean_imports(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")\n",
    "\n",
    "df_s12_clean.to_csv(os.path.join(output_path, 'df_s12_clean.csv'))\n",
    "df_s12type5_clean.to_csv(os.path.join(output_path, 'df_s12type5_clean.csv'))\n",
    "df_s34_clean.to_csv(os.path.join(output_path, 'df_s34_clean.csv'))\n",
    "df_beta_clean.to_csv(os.path.join(output_path, 'df_beta_clean.csv'))\n",
    "df_security_clean.to_csv(os.path.join(output_path, 'df_security_clean.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:17:29.395669900Z",
     "start_time": "2024-03-22T13:05:05.354046900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Assets/Factors---------------------------\n",
      "\n",
      "Merged assets and factors\n",
      "Number of assets/dates:   50643\n",
      "\n",
      "---------------Merging s12/s34 Holdings---------------------------\n",
      "Merged s12 and s34\n",
      "Number of holdings:   22267070\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Assets/Factors---------------------------\\n')\n",
    "df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "df_asset.to_csv(os.path.join(output_path, 'df_asset.csv'))\n",
    "\n",
    "print('\\n---------------Merging s12/s34 Holdings---------------------------\\n')\n",
    "df_fund_manager = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "df_fund_manager.to_csv(os.path.join(output_path, 'df_fund_manager.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:19:34.029737600Z",
     "start_time": "2024-03-22T13:17:29.398231200Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Holdings/Factors---------------------------\n",
      "Merged holdings and factors\n",
      "Number of assets/dates:   22168166\n",
      "\n",
      "---------------Creating Household Sector---------------------------\n",
      "Created household sector\n",
      "Number of holdings:   217114\n",
      "\n",
      "---------------Partitioning Outside Asset---------------------------\n",
      "Created outside asset\n",
      "Number of holdings:   92001\n",
      "\n",
      "---------------Calculating Investor AUM---------------------------\n",
      "Calculated investor AUM\n",
      "             in_aum     n_holding       out_aum      typecode           aum\n",
      "count  9.283900e+04  92839.000000  9.283900e+04  92839.000000  9.283900e+04\n",
      "mean   3.222592e+03     84.299615  9.029653e+03      4.520945  1.225224e+04\n",
      "std    1.244887e+05    245.252978  4.016086e+05      0.768091  4.850028e+05\n",
      "min    0.000000e+00      0.000000  0.000000e+00      0.000000  1.552000e-05\n",
      "25%    4.203946e+00      2.000000  8.642027e+01      4.000000  1.543316e+02\n",
      "50%    5.913368e+01     14.000000  2.190730e+02      5.000000  3.598470e+02\n",
      "75%    3.042007e+02     62.000000  7.299716e+02      5.000000  1.242410e+03\n",
      "max    1.572778e+07   4050.000000  3.389591e+07      5.000000  4.784345e+07\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Holdings/Factors---------------------------\\n')\n",
    "df_holding_factor = merge_holding_factor(df_fund_manager, df_asset)\n",
    "df_holding_factor.to_csv(os.path.join(output_path, 'df_holding_factor.csv'))\n",
    "\n",
    "print('\\n---------------Creating Household Sector---------------------------\\n')\n",
    "df_household = create_household_sector(df_holding_factor)\n",
    "df_household.to_csv(os.path.join(output_path, 'df_household.csv'))\n",
    "\n",
    "print('\\n---------------Partitioning Outside Asset---------------------------\\n')\n",
    "df_inside, df_outside = create_outside_asset(df_household)\n",
    "df_inside.to_csv(os.path.join(output_path, 'df_inside.csv'))\n",
    "df_outside.to_csv(os.path.join(output_path, 'df_outside.csv'))\n",
    "\n",
    "print('\\n---------------Calculating Investor AUM---------------------------\\n')\n",
    "df_inv_aum = calc_inv_aum(df_inside, df_outside)\n",
    "df_inv_aum.to_csv(os.path.join(output_path, 'df_inv_aum.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:27:26.834639400Z",
     "start_time": "2024-03-22T13:19:34.030742500Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Pooling Investors By Type/Size---------------------------\n",
      "Binned investors\n",
      "Number of investors by holding:   5821\n",
      "Number of investors by aum:   237\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Pooling Investors By Type/Size---------------------------\\n')\n",
    "df_holding_binned, df_aum_binned = bin_concentrated_inv(df_inside, df_inv_aum, min_n_holding)\n",
    "df_holding_binned.to_csv(os.path.join(output_path, 'df_holding_binned.csv'))\n",
    "df_aum_binned.to_csv(os.path.join(output_path, 'df_aum_binned.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:28:55.627946300Z",
     "start_time": "2024-03-22T13:27:26.844787400Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Tracking Investment Universe---------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'n_holding'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3790\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3791\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3792\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'n_holding'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Tracking Investment Universe---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m df_inv_universe \u001B[38;5;241m=\u001B[39m calc_inv_universe(df_holding_binned, n_quarters)\n\u001B[0;32m      3\u001B[0m df_inv_universe\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf_inv_universe.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Calculating Instrument---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[33], line 123\u001B[0m, in \u001B[0;36mcalc_inv_universe\u001B[1;34m(df_holding, n_quarters)\u001B[0m\n\u001B[0;32m    120\u001B[0m     inv_uni \u001B[38;5;241m=\u001B[39m calc_past_quarters(inv_id, date)\n\u001B[0;32m    121\u001B[0m     df_inv_universe\u001B[38;5;241m.\u001B[39mloc[(inv_id, date), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_universe\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m inv_uni\n\u001B[1;32m--> 123\u001B[0m df_inv_universe \u001B[38;5;241m=\u001B[39m df_inv_universe\u001B[38;5;241m.\u001B[39massign(uni_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_universe\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28mlen\u001B[39m),\n\u001B[0;32m    124\u001B[0m                                          iweight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_holding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muni_size\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    125\u001B[0m                                          cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: (x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124miweight\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.95\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m))\n\u001B[0;32m    126\u001B[0m log_inv_universe(df_inv_universe)\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df_inv_universe\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5012\u001B[0m, in \u001B[0;36mDataFrame.assign\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m   5009\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m   5011\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m-> 5012\u001B[0m     data[k] \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(v, data)\n\u001B[0;32m   5013\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:379\u001B[0m, in \u001B[0;36mapply_if_callable\u001B[1;34m(maybe_callable, obj, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;124;03mEvaluate possibly callable input using obj and kwargs if it is callable,\u001B[39;00m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;124;03motherwise return as it is.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m \u001B[38;5;124;03m**kwargs\u001B[39;00m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(maybe_callable):\n\u001B[1;32m--> 379\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m maybe_callable(obj, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_callable\n",
      "Cell \u001B[1;32mIn[33], line 124\u001B[0m, in \u001B[0;36mcalc_inv_universe.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    120\u001B[0m     inv_uni \u001B[38;5;241m=\u001B[39m calc_past_quarters(inv_id, date)\n\u001B[0;32m    121\u001B[0m     df_inv_universe\u001B[38;5;241m.\u001B[39mloc[(inv_id, date), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_universe\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m inv_uni\n\u001B[0;32m    123\u001B[0m df_inv_universe \u001B[38;5;241m=\u001B[39m df_inv_universe\u001B[38;5;241m.\u001B[39massign(uni_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_universe\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28mlen\u001B[39m),\n\u001B[1;32m--> 124\u001B[0m                                          iweight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_holding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muni_size\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    125\u001B[0m                                          cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: (x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124miweight\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.95\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m))\n\u001B[0;32m    126\u001B[0m log_inv_universe(df_inv_universe)\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df_inv_universe\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3795\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3796\u001B[0m     ):\n\u001B[0;32m   3797\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3798\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3799\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3803\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'n_holding'"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Tracking Investment Universe---------------------------\\n')\n",
    "df_inv_universe = calc_inv_universe(df_holding_binned, n_quarters)\n",
    "df_inv_universe.to_csv(os.path.join(output_path, 'df_inv_universe.csv'))\n",
    "\n",
    "print('\\n---------------Calculating Instrument---------------------------\\n')\n",
    "df_instrument = create_instrument(df_inv_universe, df_aum_binned)\n",
    "df_instrument.to_csv(os.path.join(output_path, 'df_instrument.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:44:37.348877300Z",
     "start_time": "2024-03-22T13:43:29.029463800Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Holding Weights---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated holding weights\n",
      "           uni_size        in_aum     n_holding       out_aum      typecode  \\\n",
      "count  2.378424e+06  2.378424e+06  2.378424e+06  2.378424e+06  2.378424e+06   \n",
      "mean   2.482237e+03  2.436918e+05  1.888463e+03  3.156428e+05  4.088446e+00   \n",
      "std    7.582149e+02  1.374924e+06  5.795760e+02  2.444683e+06  1.407806e+00   \n",
      "min    1.030000e+03  3.466868e+01  1.000000e+03  1.489027e+01  0.000000e+00   \n",
      "25%    1.903000e+03  7.456938e+03  1.414000e+03  2.762783e+03  4.000000e+00   \n",
      "50%    2.415000e+03  2.571037e+04  1.842000e+03  8.863570e+03  5.000000e+00   \n",
      "75%    3.002000e+03  7.113223e+04  2.299000e+03  2.416874e+04  5.000000e+00   \n",
      "max    4.967000e+03  1.572778e+07  4.050000e+03  3.211567e+07  5.000000e+00   \n",
      "\n",
      "                aum    out_weight     in_weight    allocation         iv_me  \\\n",
      "count  2.378424e+06  2.378424e+06  2.378424e+06  2.378424e+06  2.378424e+06   \n",
      "mean   5.593346e+05  2.940100e-01  7.059900e-01  1.468428e+02  1.084030e+05   \n",
      "std    3.809998e+06  1.693222e-01  1.693222e-01  9.086609e+02  2.508248e+04   \n",
      "min    6.781394e+01  4.120320e-02  5.044396e-02  4.857732e-02  1.615504e+01   \n",
      "25%    1.105182e+04  1.832759e-01  6.190424e-01  5.741995e+00  1.071139e+05   \n",
      "50%    3.629757e+04  2.308793e-01  7.691207e-01  1.484086e+01  1.199048e+05   \n",
      "75%    9.959496e+04  3.809576e-01  8.167241e-01  3.682513e+01  1.226962e+05   \n",
      "max    4.784345e+07  9.495560e-01  9.587968e-01  9.630324e+03  1.236260e+05   \n",
      "\n",
      "       ...         b_smb         b_hml         ln_me      ln_iv_me  \\\n",
      "count  ...  2.378424e+06  2.378424e+06  2.378424e+06  2.378424e+06   \n",
      "mean   ...  5.936942e-01  1.426449e-01  7.847787e+00  1.153772e+01   \n",
      "std    ...  9.312055e-01  9.157387e-01  1.794630e+00  4.157670e-01   \n",
      "min    ... -2.139320e+01 -7.644300e+00 -7.336896e-01  2.782232e+00   \n",
      "25%    ...  1.020000e-02 -3.422000e-01  6.633576e+00  1.158165e+01   \n",
      "50%    ...  4.857000e-01  8.380000e-02  7.804665e+00  1.169445e+01   \n",
      "75%    ...  1.073200e+00  5.996000e-01  9.034209e+00  1.171747e+01   \n",
      "max    ...  9.794700e+00  1.235910e+01  1.366465e+01  1.172502e+01   \n",
      "\n",
      "             weight     ln_weight       rweight    ln_rweight  \\\n",
      "count  2.378424e+06  2.378424e+06  2.378424e+06  2.378424e+06   \n",
      "mean   8.193445e-04          -inf  2.686097e-03          -inf   \n",
      "std    1.717746e-02           NaN  2.827224e-02           NaN   \n",
      "min    0.000000e+00          -inf  0.000000e+00          -inf   \n",
      "25%    9.198898e-06 -1.159643e+01  3.347769e-05 -1.030463e+01   \n",
      "50%    5.358376e-05 -9.834264e+00  2.203630e-04 -8.420234e+00   \n",
      "75%    2.673298e-04 -8.227027e+00  1.179133e-03 -6.742976e+00   \n",
      "max    3.981640e+00  1.381694e+00  5.931541e+00  1.780284e+00   \n",
      "\n",
      "       mean_ln_rweight      const  \n",
      "count     2.378424e+06  2378424.0  \n",
      "mean              -inf        1.0  \n",
      "std                NaN        0.0  \n",
      "min               -inf        1.0  \n",
      "25%      -9.743304e+00        1.0  \n",
      "50%      -8.486490e+00        1.0  \n",
      "75%      -7.666138e+00        1.0  \n",
      "max      -5.139131e+00        1.0  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Holding Weights---------------------------\\n')\n",
    "df_weights = calc_holding_weights(df_instrument, df_holding_binned, min_n_holding)\n",
    "df_weights.to_csv(os.path.join(output_path, 'df_weights.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:31:54.330665700Z",
     "start_time": "2024-03-22T13:31:16.230240300Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\n---------------Estimating Demand System---------------------------\\n')\n",
    "df_model = estimate_model(df_weights, characteristics, params)\n",
    "df_model.to_csv(os.path.join(output_path, 'df_model.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:31:54.335937500Z",
     "start_time": "2024-03-22T13:31:54.332271500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Latent Demand---------------------------\\n')\n",
    "df_results = calc_latent_demand(df_model, characteristics, params)\n",
    "df_results.to_csv(os.path.join(output_path, 'df_results.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:31:54.354585700Z",
     "start_time": "2024-03-22T13:31:54.338378Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Moment Condition---------------------------\\n')\n",
    "df_mom = check_moment_condition(df_results[df_results['uni_size'] > df_results['n_holding']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T13:31:54.342048900Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Instrument Validity---------------------------\\n')\n",
    "critical_value_test(df_results, characteristics, figure_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T13:31:54.350268800Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Hypothetical Index Fund---------------------------\\n')\n",
    "test_index_fund(df_results, characteristics, params, figure_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T13:31:54.353520900Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Creating Figures---------------------------\\n')\n",
    "graph_type_params(df_results, params, figure_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:31:54.361314300Z",
     "start_time": "2024-03-22T13:31:54.356963300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "graph_std_latent_demand(df_results, figure_path)\n",
    "print('\\n---------------Finished---------------------------\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T13:31:54.360247700Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T13:31:54.469210700Z",
     "start_time": "2024-03-22T13:31:54.363715800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
