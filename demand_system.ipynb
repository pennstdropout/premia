{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.156598700Z",
     "start_time": "2024-04-01T05:46:14.921891700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import wrds\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.246427900Z",
     "start_time": "2024-04-01T05:46:17.181146200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def request_wrds(path: str,\n",
    "                 start_date: pd.Timestamp,\n",
    "                 end_date: pd.Timestamp) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    with wrds.Connection() as db:\n",
    "        df_s12 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s12\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s12.to_csv(f'{path}s12.csv')\n",
    "\n",
    "        df_s12type5 = db.raw_sql(f'''\n",
    "                    SELECT *\n",
    "                    FROM tfn.s12type5\n",
    "                    WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "                ''', date_cols=['fdate'])\n",
    "        df_s12type5.to_csv(f'{path}df_s12type5.csv')\n",
    "    \n",
    "        df_s34 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s34\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s34.to_csv(f'{path}s34.csv')\n",
    "\n",
    "        df_beta = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM beta.ff3\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_beta.to_csv(f'{path}security.csv')\n",
    "\n",
    "        df_security = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM crspm.crspm_stock\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_security.to_csv(f'{path}security.csv')\n",
    "\n",
    "        return df_s12, df_s34, df_security\n",
    "\n",
    "\n",
    "def dask_read(path: str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    try:\n",
    "        df_s12 = dd.read_csv(f'{path}s12.csv', encoding='ISO-8859-1')\n",
    "        log_import_s12(df_s12)\n",
    "\n",
    "        df_s12type5 = dd.read_csv(f'{path}s12type5.csv', encoding='ISO-8859-1')\n",
    "        log_import_s12type5(df_s12type5)\n",
    "\n",
    "        df_s34 = dd.read_csv(f'{path}s34.csv', encoding='ISO-8859-1')\n",
    "        log_import_s34(df_s34)\n",
    "\n",
    "        df_beta = dd.read_csv(f'{path}beta.csv', encoding='ISO-8859-1')\n",
    "        log_import_beta(df_beta)\n",
    "\n",
    "        df_security = dd.read_csv(f'{path}security.csv', encoding='ISO-8859-1')\n",
    "        log_import_security(df_security)\n",
    "\n",
    "        return df_s12, df_s12type5, df_s34, df_beta, df_security\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return request_wrds(path, start_date, end_date)\n",
    "\n",
    "\n",
    "def pandas_read(path: str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    try:\n",
    "        df_s12 = pd.read_csv(f'{path}s12.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12(df_s12)\n",
    "\n",
    "        df_s12type5 = pd.read_csv(f'{path}s12type5.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12type5(df_s12type5)\n",
    "\n",
    "        df_s34 = pd.read_csv(f'{path}s34.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s34(df_s34)\n",
    "\n",
    "        df_beta = pd.read_csv(f'{path}beta.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_beta(df_beta)\n",
    "\n",
    "        df_security = pd.read_csv(f'{path}security.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_security(df_security)\n",
    "\n",
    "        return df_s12, df_s12type5, df_s34, df_beta, df_security\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return request_wrds(path, start_date, end_date)\n",
    "\n",
    "\n",
    "def clean_imports(df_s12,\n",
    "                  df_s12type5,\n",
    "                  df_s34,\n",
    "                  df_beta,\n",
    "                  df_security,\n",
    "                  start_date,\n",
    "                  end_date\n",
    "                  ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    df_s12_clean = clean_s12(df_s12, start_date, end_date)\n",
    "    log_clean_s12(df_s12_clean)\n",
    "\n",
    "    df_s12type5_clean = clean_s12type5(df_s12type5, start_date, end_date)\n",
    "    log_clean_s12type5(df_s12type5_clean)\n",
    "\n",
    "    df_s34_clean = clean_s34(df_s34, start_date, end_date)\n",
    "    log_clean_s34(df_s34_clean)\n",
    "\n",
    "    df_beta_clean = clean_beta(df_beta, start_date, end_date)\n",
    "    log_clean_beta(df_beta_clean)\n",
    "\n",
    "    df_security_clean = clean_security(df_security, start_date, end_date)\n",
    "    log_clean_security(df_security_clean)\n",
    "\n",
    "    return df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean\n",
    "\n",
    "\n",
    "def clean_s12(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'fundno',\n",
    "        'rdate',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['fundno', 'shares', 'cusip'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'fundno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(date=lambda x: fix_date(x['date']))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last'))\n",
    "\n",
    "\n",
    "def clean_s12type5(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    return (df.rename(columns={'fdate': 'date', 'fundno': 'inv_id'})\n",
    "            .assign(date=lambda x: fix_date(x['date']))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .dropna(how='any', subset=['inv_id', 'date']))\n",
    "\n",
    "\n",
    "def clean_s34(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'mgrno',\n",
    "        'rdate',\n",
    "        'typecode',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['mgrno', 'rdate', 'cusip', 'shares'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'mgrno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(date=lambda x: fix_date(x['date']))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last'))\n",
    "\n",
    "\n",
    "def clean_beta(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns=[\n",
    "        'PERMNO',\n",
    "        'DATE',\n",
    "        'b_mkt',\n",
    "        'b_smb',\n",
    "        'b_hml'\n",
    "    ]\n",
    "    offset = 6\n",
    "    return (df[columns]\n",
    "            .dropna()\n",
    "            .rename(columns={\n",
    "                'DATE': 'date',\n",
    "                'PERMNO':'permno'})\n",
    "            .assign(date=lambda x: fix_date(x['date']) + offset)\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last'))\n",
    "\n",
    "\n",
    "def clean_security(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'LPERMNO',\n",
    "        'cusip',\n",
    "        'datadate',\n",
    "        'prccm',\n",
    "        'trt1m',\n",
    "        'cshoq'\n",
    "    ]\n",
    "    offset = 6\n",
    "    return (df[columns]\n",
    "            .rename(columns={\n",
    "                'LPERMNO': 'permno',\n",
    "                'cusip': 'asset_id',\n",
    "                'prccm': 'prc',\n",
    "                'cshoq': 'shrout',\n",
    "                'datadate': 'date'})\n",
    "            .assign(shrout=lambda x: x.groupby('asset_id')['shrout'].ffill() * 1000000,\n",
    "                    asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]),\n",
    "                    date=lambda x: fix_date(x['date']) + offset)\n",
    "            .dropna(how='any', subset=['asset_id', 'prc', 'shrout'])\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last'))\n",
    "\n",
    "\n",
    "# def fix_date(dates: pd.Series) -> pd.Series:\n",
    "#     return pd.to_datetime(dates) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "\n",
    "def fix_date(dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(dates).dt.to_period(freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.286098300Z",
     "start_time": "2024-04-01T05:46:17.209891200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stocks Monthly\n",
    "\n",
    "def merge_assets_factors(df_assets: pd.DataFrame, df_factors: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_assets,\n",
    "        right=df_factors,\n",
    "        how='left',\n",
    "        on=['date', 'permno'])\n",
    "\n",
    "    df_merged_indexed = (df_merged\n",
    "                         .assign(shrout=lambda x: x['shrout'].astype('int64'),\n",
    "                                 date=lambda x: x['date'].dt.asfreq('Q'))\n",
    "                         .drop_duplicates(subset=['date', 'asset_id'], keep='last')\n",
    "                         .drop(columns=['permno']))\n",
    "\n",
    "    log_asset_merge(df_merged_indexed)\n",
    "    return df_merged_indexed\n",
    "\n",
    "\n",
    "# Manager / Holdings\n",
    "\n",
    "def match_fund_manager(df_fund: pd.DataFrame, df_manager: pd.DataFrame, df_key: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fund_manager = (df_manager\n",
    "                       .assign(shares=lambda x: x['shares'].astype('int64'),\n",
    "                               date=lambda x: x['date'].dt.asfreq(freq='Q'))\n",
    "                       .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last'))\n",
    "    \n",
    "    log_holding_merge(df_fund_manager)\n",
    "    return df_fund_manager\n",
    "\n",
    "\n",
    "def construct_zero_holdings(df_fund_manager: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    \n",
    "    def calc_inv_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.groupby('inv_id')['date'].transform('max') - df['date']).apply(lambda x: x.n)\n",
    "        min_diff = np.minimum(date_diff, n_quarters) + 1\n",
    "        return min_diff\n",
    "    \n",
    "    def calc_asset_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.sort_values('date')\n",
    "                     .groupby(['inv_id', 'asset_id'])\n",
    "                     ['date']\n",
    "                     .diff(periods=1)\n",
    "                     .fillna(pd.DateOffset(n=1))\n",
    "                     .apply(lambda x: x.n))\n",
    "        min_diff = np.minimum(date_diff, df['inv_obs'])\n",
    "        return min_diff.apply(lambda x: list(range(x)))\n",
    "    \n",
    "    df_holding = (df_fund_manager\n",
    "                  .assign(inv_obs=lambda x: calc_inv_obs(x),\n",
    "                          asset_obs=lambda x: calc_asset_obs(x))\n",
    "                  .explode('asset_obs')\n",
    "                  .assign(asset_obs=lambda x: x['asset_obs'].astype('int8'),\n",
    "                          mask=lambda x: x['asset_obs'] == 0,\n",
    "                          shares=lambda x: x['shares'] * x['mask'],\n",
    "                          date=lambda x: x['date'] + x['asset_obs'])\n",
    "                  .drop(columns=['inv_obs', 'asset_obs', 'mask']))\n",
    "        \n",
    "    log_zero_holdings(df_holding)\n",
    "    return df_holding\n",
    "\n",
    "def merge_holding_factor(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_holding,\n",
    "        right=df_asset,\n",
    "        how='left',\n",
    "        on=['date', 'asset_id'])\n",
    "\n",
    "    df_holding_factor = (df_merged\n",
    "                         .assign(shares=lambda x: np.minimum(x['shares'], x['shrout']),\n",
    "                                 holding=lambda x: x['prc'] * x['shares'] / 1000000,\n",
    "                                 me=lambda x: x['prc'] * x['shrout'] / 1000000,\n",
    "                                 typecode=lambda x: x['typecode'].fillna(0).astype('int8'))\n",
    "                         .dropna(subset='holding'))\n",
    "    \n",
    "    log_holding_factor_merge(df_holding_factor)\n",
    "    return df_holding_factor\n",
    "\n",
    "\n",
    "def drop_unsused_inv_asset(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_dropped = (df_holding_factor\n",
    "                  .assign(Tinv_shares=lambda x: x.groupby(['inv_id', 'date'])['shares'].transform('sum'),\n",
    "                          Tasset_shares=lambda x: x.groupby(['date', 'asset_id'])['shares'].transform('sum'),\n",
    "                          mask=lambda x: (x['Tinv_shares'] == 0) | (x['Tasset_shares'] == 0))\n",
    "                  .loc[lambda x: ~x['mask']]\n",
    "                  .drop(columns=['Tinv_shares', 'Tasset_shares', 'mask']))\n",
    "    \n",
    "    log_drop_unused(df_dropped)\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def create_household_sector(df_dropped: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_dropped\n",
    "                      .groupby(['date', 'asset_id'], as_index=False)\n",
    "                      .agg({\n",
    "                        'shares': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'shrout': 'last',\n",
    "                        'b_mkt': 'last',\n",
    "                        'b_smb': 'last',\n",
    "                        'b_hml': 'last',\n",
    "                        'holding': 'sum',\n",
    "                        'me': 'last'})\n",
    "                      .assign(shares=lambda x: np.maximum(x['shrout'] - x['shares'], 0),\n",
    "                              holding=lambda x: np.maximum(x['me'] - x['holding'], 0),\n",
    "                              inv_id=0,\n",
    "                              typecode=0))\n",
    "    \n",
    "    log_household_sector(df_household)\n",
    "    df_concat = pd.concat([df_dropped, df_household])\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.287034700Z",
     "start_time": "2024-04-01T05:46:17.220183900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def partition_outside_asset(df_household: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_outside = (df_household\n",
    "                  .assign(out_mask=lambda x: x.isna().any(axis=1),\n",
    "                          out_holding=lambda x: x['holding'] * x['out_mask']))\n",
    "    \n",
    "    log_outside_asset(df_outside)\n",
    "    return df_outside\n",
    "\n",
    "\n",
    "def calc_inv_aum(df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_grouped = df_outside.groupby(['inv_id', 'date'])\n",
    "    \n",
    "    df_inv_aum = (df_outside\n",
    "                  .assign(aum=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                          out_aum=lambda x: df_grouped['out_holding'].transform('sum'),\n",
    "                          out_weight=lambda x: x['out_aum'] / x['aum']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def agg_small_inv(df_inv_aum: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_agg = (df_inv_aum\n",
    "              .assign(small_mask=lambda x: (x['aum'] < 10) | (x['out_weight'] == 0) | (x['out_weight'] == 1),\n",
    "                      inv_id=lambda x: ~x['small_mask'] * x['inv_id'],\n",
    "                      hh_mask=lambda x: x['inv_id'] == 0))\n",
    "\n",
    "    df_grouped = df_agg.groupby(['inv_id', 'date', 'asset_id'])\n",
    "    \n",
    "    df_agg = (df_agg\n",
    "              .assign(Tshares=lambda x: df_grouped['shares'].transform('sum'),\n",
    "                      Tholding=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                      shares=lambda x: np.where(x['hh_mask'], x['Tshares'], x['shares']),\n",
    "                      holding=lambda x: np.where(x['hh_mask'], x['Tholding'], x['holding']))\n",
    "              .drop(columns=['Tshares', 'Tholding']))\n",
    "    \n",
    "    df_grouped = df_agg.groupby(['inv_id', 'date'])\n",
    "    \n",
    "    df_agg = (df_agg\n",
    "              .assign(Taum=lambda x: df_grouped['holding'].transform('sum'),\n",
    "                      Tout_aum=lambda x: df_grouped['out_holding'].transform('sum'),\n",
    "                      aum=lambda x: np.where(x['hh_mask'], x['Taum'], x['aum']),\n",
    "                      out_aum=lambda x: np.where(x['hh_mask'], x['Tout_aum'], x['out_aum']),\n",
    "                      x_holding=lambda x: df_grouped['holding'].transform('count'),\n",
    "                      n_holding=lambda x: x['x_holding'] - df_grouped['out_mask'].transform('sum'),\n",
    "                      equal_alloc=lambda x: ~x['hh_mask'] * x['aum'] / (1 + x['x_holding']),\n",
    "                      drop_mask=lambda x: x['small_mask'] | x['out_mask'])\n",
    "              .loc[lambda x: ~x['drop_mask']]\n",
    "              .drop(columns=['Taum', 'Tout_aum', 'out_mask', 'out_holding', 'out_weight', 'small_mask', 'drop_mask']))\n",
    "    \n",
    "    log_agg_small_inv(df_agg)\n",
    "    return df_agg\n",
    "    \n",
    "\n",
    "def bin_inv(df_inv_aum: pd.DataFrame, min_n_holding: int) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    def calc_bin(df_type_date: pd.DataFrame) -> pd.Series:\n",
    "        typecode = df_type_date['typecode'].iloc[0]\n",
    "        n_holding = (df_type_date['holding'] > 0).sum()\n",
    "        n_bins = np.floor(n_holding / (2 * min_n_holding)).astype(int)\n",
    "        idx = df_type_date.index\n",
    "        \n",
    "        if (n_bins <= 1) | (n_holding >= min_n_holding) | (typecode == 0):\n",
    "            return pd.Series(0, index=idx)\n",
    "        else:\n",
    "            return pd.Series(pd.qcut(x=df_type_date['aum'], q=n_bins, labels=False), index=idx)\n",
    "    \n",
    "    \n",
    "    df_binned = (df_inv_aum\n",
    "                 .reset_index(drop=True)\n",
    "                 .assign(bin=lambda x: x.groupby(['typecode', 'date']).apply(calc_bin).reset_index(drop=True))\n",
    "                 .assign(bin=lambda x: x.groupby(['date', 'typecode', 'bin']).ngroup()))\n",
    "    \n",
    "    log_bins(df_binned)\n",
    "    return df_binned\n",
    "\n",
    "\n",
    "def calc_instrument(df_binned: pd.DataFrame) -> pd.DataFrame:    \n",
    "    df_instrument = (df_binned\n",
    "                     .assign(total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum'),\n",
    "                             iv_me=lambda x: x['total_alloc'] - x['equal_alloc'])\n",
    "                     .drop(columns=['total_alloc', 'equal_alloc']))\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def dask_calc_instrument(df_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_binned = dd.from_pandas(df_binned.set_index('date'), npartitions=32)\n",
    "    \n",
    "    df_instrument = (df_binned\n",
    "                     .assign(total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum', meta=pd.Series(dtype='float64')),\n",
    "                             iv_me=lambda x: x['total_alloc'] - x['equal_alloc'])\n",
    "                     .drop(columns=['total_alloc', 'equal_alloc'])\n",
    "                     .compute()\n",
    "                     .reset_index())\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = (df_instrument['iv_me'] > 0)\n",
    "\n",
    "    df_weights = (df_instrument.loc[mask]\n",
    "                 .assign(ln_me=lambda x: np.log(x['me']),\n",
    "                         ln_iv_me=lambda x: np.log(x['iv_me']),\n",
    "                         rweight=lambda x: x['holding'] / x['out_aum'],\n",
    "                         ln_rweight=lambda x: np.log(x['rweight'].mask(x['rweight'] == 0, np.NaN)),\n",
    "                         mean_ln_rweight=lambda x: x.groupby(['inv_id', 'date'])['ln_rweight'].transform('mean'),\n",
    "                         const=1,\n",
    "                         pct_uni_held=lambda x: np.log(x['n_holding']) - np.log(x['x_holding']))\n",
    "                .drop(columns=['me', 'iv_me']))\n",
    "    \n",
    "    log_holding_weights(df_weights)\n",
    "    return df_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T06:04:41.722329600Z",
     "start_time": "2024-04-01T06:04:41.299274400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def bound_float(arr: np.array) -> float:\n",
    "    ln_bound = 709.7827\n",
    "    return np.minimum(np.maximum(arr, -ln_bound), ln_bound)\n",
    "\n",
    "\n",
    "def momcond_L(params: np.array, exog: np.array) -> np.array:\n",
    "    upper_bound = 0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    mean_ln_rweight = exog[1]\n",
    "    arr_characteristics = exog[2:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = ln_me_term + characteristics_term + mean_ln_rweight\n",
    "    \n",
    "    return pred_weight\n",
    "\n",
    "\n",
    "def fit_inv_date_L(df_inv_date: pd.DataFrame, characteristics: list, params: list,) -> gmm.GMMResults:\n",
    "    df_inv_date = df_inv_date.dropna(subset='ln_rweight')\n",
    "    \n",
    "    exog = np.asarray(df_inv_date[['ln_me', 'mean_ln_rweight'] + characteristics])\n",
    "    instrument = np.asarray(df_inv_date[['ln_iv_me', 'mean_ln_rweight'] + characteristics])\n",
    "    n = exog.shape[0]\n",
    "    endog = np.asarray(df_inv_date['ln_rweight'] - df_inv_date['pct_uni_held'])\n",
    "    start_params = np.zeros(len(params))\n",
    "    w0inv = np.dot(instrument.T, instrument) / n\n",
    "    \n",
    "    try:\n",
    "        model = gmm.NonlinearIVGMM(\n",
    "            endog=endog,\n",
    "            exog=exog,\n",
    "            instrument=instrument, \n",
    "            func=momcond_L)\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=0,\n",
    "            inv_weights=w0inv)\n",
    "        # log_results(result, params)\n",
    "        return result\n",
    "    \n",
    "    except np.linalg.LinAlgError:\n",
    "        print('Linear Algebra Error')\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def estimate_model_L(df_weights: pd.DataFrame, characteristics: list, params: list, min_n_holding: int) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_weights['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_institutions = (df_weights\n",
    "                       .loc[mask]\n",
    "                       .set_index(['inv_id', 'date'])\n",
    "                       .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                       .reset_index())\n",
    "    \n",
    "    df_bins = (df_weights\n",
    "               .loc[~mask]\n",
    "               .set_index(['bin', 'date'])\n",
    "               .assign(gmm_result=lambda x: x.groupby(['bin', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "               .reset_index())\n",
    "    \n",
    "    df_model = pd.concat([df_institutions, df_bins])\n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def momcond_NL(params: np.array, exog: np.array) -> np.array:\n",
    "    upper_bound = 0.9999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = bound_float(params[0])\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    rweight = exog[1]\n",
    "    mean_ln_rweight = exog[2]\n",
    "    arr_characteristics = exog[3:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_ln_rweight = bound_float(ln_me_term + characteristics_term + mean_ln_rweight)\n",
    "    pred_rweight = np.exp(-1 * pred_ln_rweight)\n",
    "    \n",
    "    return rweight * pred_rweight\n",
    "\n",
    "\n",
    "def fit_inv_date_NL(df_inv_date: pd.DataFrame, characteristics: list, params: list) -> gmm.GMMResults:\n",
    "    exog = np.asarray(df_inv_date[['ln_me', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "    instrument = np.asarray(df_inv_date[['ln_iv_me', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "    n = exog.shape[0]\n",
    "    endog = np.ones(n)\n",
    "    \n",
    "    try:\n",
    "        model = gmm.NonlinearIVGMM(\n",
    "            endog=endog,\n",
    "            exog=exog,\n",
    "            instrument=instrument, \n",
    "            func=momcond_NL)\n",
    "        w0inv = np.dot(instrument.T, instrument) / n\n",
    "        start_params = np.zeros(len(params))\n",
    "        result = model.fit(\n",
    "            start_params=start_params,\n",
    "            maxiter=100,\n",
    "            inv_weights=w0inv)\n",
    "        # log_results(result, params)\n",
    "        return result\n",
    "    \n",
    "    except np.linalg.LinAlgError:\n",
    "        print('Linear Algebra Error')\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def estimate_model_NL(df_weights: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_weights['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_institutions = (df_weights\n",
    "                       .loc[mask]\n",
    "                       .set_index(['inv_id', 'date'])\n",
    "                       .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                       .reset_index())\n",
    "    \n",
    "    df_bins = (df_weights\n",
    "               .loc[~mask]\n",
    "               .set_index(['bin', 'date'])\n",
    "               .assign(gmm_result=lambda x: x.groupby(['bin', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "               .reset_index())\n",
    "    \n",
    "    df_model = pd.concat([df_institutions, df_bins])\n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def calc_latent_demand_L(df_model: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    def unpack_result(result: gmm.GMMResults) -> list:\n",
    "        return result.params\n",
    "\n",
    "    def unpack_params(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[params] = pd.DataFrame(df['lst_params'].tolist(), index=df.index)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    upper_bound = 0.9999\n",
    "    \n",
    "    df_results = (df_model\n",
    "                  .dropna()\n",
    "                  .assign(lst_params=lambda x: x['gmm_result'].apply(unpack_result))\n",
    "                  .pipe(unpack_params)\n",
    "                  .drop(columns='lst_params')\n",
    "                  .assign(beta_ln_me=lambda x: upper_bound - np.exp(-1 * x['beta_ln_me']),\n",
    "                          beta_const=lambda x: x['beta_const'] + x['mean_ln_rweight'],\n",
    "                          pred_ln_rweight=lambda x: np.einsum('ij,ij->i', x[['ln_me'] + characteristics], x[params]),\n",
    "                          latent_demand=lambda x: x['ln_rweight'] - x['pred_ln_rweight']))\n",
    "    \n",
    "    log_latent_demand(df_results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Liquidity\n",
    "\n",
    "def calc_coliquidity_matrix(df_date: pd.DataFrame) -> pd.Series:\n",
    "    df_asset_grouped = df_date.groupby('asset_id')\n",
    "    df_inv_grouped = df_date.groupby('inv_id')\n",
    "    \n",
    "    types = df_date['typecode'].unique()\n",
    "\n",
    "    df_liquid = (df_date\n",
    "                 .assign(first=lambda x: df_asset_grouped.cumcount() == 0,\n",
    "                         total_holding=lambda x: df_asset_grouped['holding'].transform('sum'),\n",
    "                         share_of_holding=lambda x: x['holding'] / x['total_holding'],\n",
    "                         max_weight=lambda x: df_inv_grouped['weight'].transform('max'),\n",
    "                         rcweight=lambda x: 1 - x['max_weight'],\n",
    "                         Zcol=lambda x: x['beta_ln_me'] * x['rcweight'] * x['share_of_holding'],\n",
    "                         Acol=lambda x: x['rcweight'] * x['share_of_holding']))\n",
    "    \n",
    "    for i, t in enumerate(types):\n",
    "        Bcol_name = 'Bcol_' + str(t)        \n",
    "        mask = df_liquid['typecode'] == t\n",
    "        df_liquid[Bcol_name] = mask * df_liquid['Acol']\n",
    "    \n",
    "    df_asset_grouped = df_date.groupby('asset_id')\n",
    "    \n",
    "    df_matrix = (df_asset_grouped\n",
    "                 .agg({'Zcol': 'sum', 'Acol': 'sum'}))\n",
    "    \n",
    "    for t in types:\n",
    "        Bcol_name = 'Bcol_' + str(t)\n",
    "        \n",
    "        n_type = df_asset_grouped.apply(lambda x: (x['typecode'] == t).sum())\n",
    "        df_matrix[Bcol_name] = df_asset_grouped[Bcol_name].sum() / np.maximum(n_type, 1)\n",
    "    \n",
    "\n",
    "    print(df_date['asset_id'].nunique())\n",
    "    print(len(types))\n",
    "    print(df_matrix.shape)\n",
    "    \n",
    "    return df_matrix\n",
    "\n",
    "\n",
    "def calc_price_elasticity(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    mask = df_results['holding'] > 0\n",
    "    \n",
    "    df_liquidity = (df_results\n",
    "                    .loc[mask]\n",
    "                    .groupby('date').apply(calc_coliquidity_matrix))\n",
    "    \n",
    "    log_liquidity(df_liquidity)\n",
    "    return df_liquidity\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.402507300Z",
     "start_time": "2024-04-01T05:46:17.251297200Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.404721600Z",
     "start_time": "2024-04-01T05:46:17.270990300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figures\n",
    "\n",
    "def clean_figures(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_results\n",
    "            .assign(date=lambda x: x['date'].dt.to_timestamp(),\n",
    "                    typecode=lambda x: x['typecode'].apply(get_readable_typecode)))\n",
    "\n",
    "\n",
    "def typecode_share_counts(df_figure: pd.DataFrame, figure_path: str):\n",
    "    df_type_share = (df_figure\n",
    "                     .groupby(['typecode', 'date'])\n",
    "                     .agg({'aum': 'sum', 'n_holding': 'count'})\n",
    "                     .assign(share=lambda x: x['aum'] / x['aum'].groupby('date').transform('sum')))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, x='date', y='n_holding', hue='typecode', kind='line')\n",
    "    g.set_axis_labels('Date', 'Number of Investors')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_count.png'))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, x='date', y='share', hue='typecode', kind='line')\n",
    "    g.set_axis_labels('Date', 'Share of AUM')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_share.png'))\n",
    "    \n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def check_moment_condition(df_figures: pd.DataFrame, min_n_holding: int, moment: int, ):\n",
    "    mask = df_figures['n_holding'] >= min_n_holding\n",
    "    df_mom = (df_figures\n",
    "              .loc[mask]\n",
    "              .groupby(['inv_id', 'date'])\n",
    "              .agg({'latent_demand': 'mean'}))\n",
    "    \n",
    "    epsilon = 0.01\n",
    "    mask = (df_mom['latent_demand'] > moment - epsilon) & (df_mom['latent_demand'] < moment + epsilon)\n",
    "    valid_rate = len(df_mom[mask]) / len(df_mom)\n",
    "    print(f'Percentage of valid moments:  {100*valid_rate:.4f}%')\n",
    "    \n",
    "    g = sns.displot(\n",
    "        data=df_mom,\n",
    "        x='latent_demand'\n",
    "    )\n",
    "    g.set_axis_labels('Log Latent Demand', 'Frequency')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'moment_condition.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def critical_value_test(df_figures: pd.DataFrame, characteristics: list, min_n_holding: int, figure_path: str):\n",
    "    \n",
    "    def iv_reg(df_inv_date: pd.DataFrame):\n",
    "        y = df_inv_date['ln_me']\n",
    "        X = df_inv_date[['ln_iv_me'] + characteristics]\n",
    "        model = OLS(y, X)\n",
    "        result = model.fit()\n",
    "        t_stat = result.tvalues.iloc[0]\n",
    "        \n",
    "        return t_stat\n",
    "    \n",
    "    \n",
    "    mask = df_figures['n_holding'] >= min_n_holding\n",
    "    \n",
    "    df_iv_a = (df_figures\n",
    "               .loc[mask]\n",
    "               .groupby(['inv_id', 'date'])\n",
    "               .apply(iv_reg))\n",
    "    \n",
    "    df_iv_b = (df_figures\n",
    "               .loc[~mask]\n",
    "               .groupby(['bin', 'date'])\n",
    "               .apply(iv_reg))\n",
    "    \n",
    "    df_iv = (pd.concat([df_iv_a, df_iv_b])\n",
    "             .to_frame('t_stat')\n",
    "             .groupby('date')\n",
    "             .min())\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data=df_iv,\n",
    "        x='date',\n",
    "        y='t_stat',\n",
    "        kind='line')\n",
    "    g.refline(\n",
    "        y=4.05,\n",
    "        linestyle='--')\n",
    "    g.set_axis_labels('Date', 'First stage t-statistic')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'instrument_validity.png'))\n",
    "    \n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def test_index_fund(df_figures: pd.DataFrame, characteristics: list, params: list, indexfund_id: int, figure_path: str):\n",
    "    mask = df_figures['inv_id'] == indexfund_id\n",
    "    df_index_fund = (df_figures\n",
    "                     .assign(ln_rweight=lambda x: x['ln_me'] + x['mean_ln_rweight'],\n",
    "                             pct_uni_held=1)\n",
    "                     .loc[mask])\n",
    "    \n",
    "    df_index_fund_model = (df_index_fund\n",
    "                           .set_index(['inv_id', 'date'])\n",
    "                           .assign(gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(lambda y: fit_inv_date_L(y, characteristics, params)))\n",
    "                           .reset_index())\n",
    "    df_index_fund_result = calc_latent_demand_L(df_index_fund_model, characteristics, params)\n",
    "    cols = params + ['latent_demand']\n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_index_fund_result,\n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.despine()\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.savefig(os.path.join(figure_path, f'index_fund_{param}.png'))\n",
    "\n",
    "    return df_figures\n",
    "    \n",
    "\n",
    "def graph_type_params(df_results: pd.DataFrame, params: list, figure_path: str):\n",
    "    df_types = df_results.copy()\n",
    "    df_types[params] = df_types[params].apply(lambda x: x * df_results['aum'])\n",
    "    df_types = (df_types\n",
    "                .groupby(['typecode', 'date'])\n",
    "                [params]\n",
    "                .sum()\n",
    "                .apply(lambda x: x / df_results.groupby(['typecode', 'date'])['aum'].sum()))\n",
    "    \n",
    "    for param in params:\n",
    "        g = sns.relplot(data=df_types, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        hue='typecode',\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.legend.set_title('Institution Type')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'{param}.png'))\n",
    "        \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def graph_std_latent_demand(df_figures: pd.DataFrame, figure_path: str):\n",
    "    df_ld = (df_figures\n",
    "         .groupby(['inv_id', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'std',\n",
    "            'aum': 'last',\n",
    "            'typecode': 'last'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] * x['aum'])\n",
    "         .groupby(['typecode', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'mean',\n",
    "            'aum': 'sum'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] / x['aum']))\n",
    "    \n",
    "    g = sns.relplot(data=df_ld, \n",
    "                    x='date',\n",
    "                    y='latent_demand',\n",
    "                    hue='typecode',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Standard Deviation of Latent Demand')\n",
    "    g.legend.set_title('Institution Type')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'std_latent_demand.png'))\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "def summary_tables(df_figures: pd.DataFrame):\n",
    "    return df_figures\n",
    "\n",
    "\n",
    "def get_param_cols(cols: list) -> list:\n",
    "    return ['beta_' + col for col in cols]\n",
    "\n",
    "\n",
    "def get_readable_param(name: str) -> str:\n",
    "    return name.replace('_', ' ').title()\n",
    "\n",
    "\n",
    "def get_readable_typecode(typecode: int):\n",
    "    return dict_typecode[typecode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.446281500Z",
     "start_time": "2024-04-01T05:46:17.287034700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_import_s12(df_s12: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12['rdate']\n",
    "    print('Imported s12')\n",
    "    print('Number of holdings:  ', len(df_s12))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_import_s12type5(df_s12type5: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5['fdate']\n",
    "    print()\n",
    "    print('Imported s12type5')\n",
    "    print('Number of holdings:  ', len(df_s12type5))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_import_s34(df_s34: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34['rdate']\n",
    "    print()\n",
    "    print('Imported s34')\n",
    "    print('Number of holdings:  ', len(df_s34))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_import_beta(df_beta: pd.DataFrame):\n",
    "    dateindex_ffm = df_beta['DATE']\n",
    "    print()\n",
    "    print('Imported betas')\n",
    "    print('Number of dates:  ', len(df_beta))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_import_security(df_security: pd.DataFrame):\n",
    "    dateindex_security = df_security['datadate']\n",
    "    print()\n",
    "    print('Imported security')\n",
    "    print('Number of holdings:  ', len(df_security))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_clean_s12(df_s12_clean: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12_clean['date']\n",
    "    print()\n",
    "    print('Cleaned s12')\n",
    "    print('Number of holdings:  ', len(df_s12_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_clean_s12type5(df_s12type5_clean: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5_clean['date']\n",
    "    print()\n",
    "    print('Cleaned s12type5')\n",
    "    print('Number of firm/dates:  ', len(df_s12type5_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_clean_s34(df_s34_clean: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34_clean['date']\n",
    "    print()\n",
    "    print('Cleaned s34')\n",
    "    print('Number of holdings:  ', len(df_s34_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_clean_beta(df_beta_clean: pd.DataFrame):\n",
    "    dateindex_beta = df_beta_clean['date']\n",
    "    print()\n",
    "    print('Cleaned beta')\n",
    "    print('Number of dates:  ', len(df_beta_clean))\n",
    "    print('Earliest date:  ', min(dateindex_beta))\n",
    "    print('Latest date:  ', max(dateindex_beta))\n",
    "\n",
    "\n",
    "def log_clean_security(df_security_clean: pd.DataFrame):\n",
    "    dateindex_security = df_security_clean['date']\n",
    "    print()\n",
    "    print('Cleaned security')\n",
    "    print('Number of asset/dates:  ', len(df_security_clean))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_holding_merge(df_merged: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged s12 and s34')\n",
    "    print('Number of holdings:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_drop_unused(df_dropped: pd.DataFrame):\n",
    "    print()\n",
    "    print('Dropped unused investors/assets')\n",
    "    print('Number of holdings:  ', len(df_dropped))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_asset_merge(df_merged: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged assets and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print()\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of investor/date/asset:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_outside: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created outside asset')\n",
    "    print('Number of outside holdings:  ', df_outside['out_mask'].sum())\n",
    "\n",
    "\n",
    "def log_zero_holdings(df_holding: pd.DataFrame):\n",
    "    print()\n",
    "    print('Constructed zero holdings')\n",
    "    print('Number of non-zero holdings:  ', sum(df_holding['shares'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_holding['shares'] == 0))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated investor AUM')\n",
    "    print('Number of investors:  ', df_inv_aum['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_agg_small_inv(df_agg: pd.DataFrame):\n",
    "    print()\n",
    "    print('Aggregated small investors')\n",
    "    print('Number of investors:  ', df_agg['inv_id'].nunique())\n",
    "\n",
    "\n",
    "def log_bins(df_binned: pd.DataFrame):\n",
    "    print()\n",
    "    print('Binned investors')\n",
    "    print('Number of investors:  ', df_binned['inv_id'].nunique())\n",
    "    print('Number of bins:  ', df_binned['bin'].nunique())\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created investment universe')\n",
    "    print('Number of investors:  ', df_inv_uni['inv_id'].nunique())\n",
    "    print('Average investment universe size:  ', df_inv_uni['uni_size'].mean())\n",
    "    \n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print()\n",
    "    print('Created market equity instrument')\n",
    "    print('Number of valid instruments:  ', sum(df_instrument['iv_me'] > 0))\n",
    "    print('Number of invalid instruments:  ', sum(df_instrument['iv_me'] == 0))\n",
    "    \n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated holding weights')\n",
    "    print('Number of non-zero holdings:  ', sum(df_model['holding'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_model['holding'] == 0))\n",
    "\n",
    "\n",
    "def log_results(result, params):\n",
    "    print()\n",
    "    print(result.summary(yname='Latent demand', xname=params))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_params(df_params: pd.DataFrame):\n",
    "    print()\n",
    "    print('Estimated parameters')\n",
    "    print('Number of converged estimations:  ', sum(df_params['gmm_result'].notna()))\n",
    "    print('Number of unconverged estimations:  ', sum(df_params['gmm_result'].isna()))\n",
    "    \n",
    "    \n",
    "def log_latent_demand(df_results: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated latent demand')\n",
    "    print('Number of investors:  ', df_results['inv_id'].nunique())\n",
    "    \n",
    "    \n",
    "def log_liquidity(df_liquidity: pd.DataFrame):\n",
    "    print()\n",
    "    print('Calculated price elasticity')\n",
    "    print('Number of investors:  ', df_liquidity['inv_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    dfs = pandas_read(input_path)\n",
    "    df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean = clean_imports(\n",
    "        *dfs,\n",
    "        start_date,\n",
    "        end_date\n",
    "    )\n",
    "    \n",
    "    df_fund_manager = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "    \n",
    "    df_holding = (df_fund_manager\n",
    "                  .pipe(construct_zero_holdings, n_quarters))\n",
    "    \n",
    "    df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "    df_holding_factor = merge_holding_factor(df_holding, df_asset)\n",
    "    \n",
    "    df_results = (df_holding_factor\n",
    "                 .pipe(drop_unsused_inv_asset)\n",
    "                 .pipe(create_household_sector)\n",
    "                 .pipe(partition_outside_asset)\n",
    "                 .pipe(calc_inv_aum)\n",
    "                 .pipe(agg_small_inv)\n",
    "                 .pipe(bin_inv, min_n_holding)\n",
    "                 .pipe(calc_instrument)\n",
    "                 .pipe(calc_holding_weights, min_n_holding)\n",
    "                 .pipe(estimate_model_L, characteristics, params, min_n_holding)\n",
    "                 .pipe(calc_latent_demand_L, characteristics, params))\n",
    "    df_results.to_csv(os.path.join(output_path, 'df_results.csv'))\n",
    "    \n",
    "    df_figures = (df_results\n",
    "                  .pipe(clean_figures)\n",
    "                  .pipe(typecode_share_counts, figure_path)\n",
    "                  .pipe(check_moment_condition, min_n_holding, 0)\n",
    "                  .pipe(critical_value_test, characteristics, figure_path)\n",
    "                  .pipe(test_index_fund, characteristics, params, figure_path)\n",
    "                  .pipe(graph_type_params, params, figure_path)\n",
    "                  .pipe(graph_std_latent_demand, figure_path))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.448761300Z",
     "start_time": "2024-04-01T05:46:17.298057100Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "sns.set_theme(style='ticks', palette=sns.color_palette('hls', 6), context='paper')\n",
    "\n",
    "input_path = 'data/'\n",
    "output_path = 'output/'\n",
    "figure_path = 'figures/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(figure_path, exist_ok=True)\n",
    "\n",
    "start_date = pd.Period('2012-01')\n",
    "end_date = pd.Period('2017-12')\n",
    "\n",
    "characteristics = [\n",
    "    'b_mkt',\n",
    "    'b_smb',\n",
    "    'b_hml'\n",
    "] + ['const']\n",
    "params = ['beta_ln_me'] + get_param_cols(characteristics)\n",
    "dict_typecode = {\n",
    "     0: 'Households',\n",
    "     1: 'Banks',\n",
    "     2: 'Insurance companies',\n",
    "     3: 'Investment advisors',\n",
    "     4: 'Mutual funds',\n",
    "     5: 'Pension funds',\n",
    "}\n",
    "\n",
    "min_n_holding = 1000\n",
    "n_quarters = 11\n",
    "indexfund_id = 90457"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:46:17.450998700Z",
     "start_time": "2024-04-01T05:46:17.308832700Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:48:29.878453500Z",
     "start_time": "2024-04-01T05:46:17.319109300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Imports---------------------------\n",
      "\n",
      "Imported s12\n",
      "Number of holdings:   30987\n",
      "Earliest date:   2014-06-30\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported s12type5\n",
      "Number of holdings:   572518\n",
      "Earliest date:   12/31/1994\n",
      "Latest date:   9/30/2022\n",
      "\n",
      "Imported s34\n",
      "Number of holdings:   22707709\n",
      "Earliest date:   2012-03-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported betas\n",
      "Number of dates:   432458\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-29\n",
      "\n",
      "Imported security\n",
      "Number of holdings:   402452\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-31\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Starting Imports---------------------------\\n')\n",
    "dfs = pandas_read(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Cleaning---------------------------\n",
      "\n",
      "Cleaned s12\n",
      "Number of holdings:   27327\n",
      "Earliest date:   2014-06\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned s12type5\n",
      "Number of firm/dates:   49372\n",
      "Earliest date:   2012-03\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned s34\n",
      "Number of holdings:   22267070\n",
      "Earliest date:   2012-03\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned beta\n",
      "Number of dates:   394535\n",
      "Earliest date:   2012-07\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned security\n",
      "Number of asset/dates:   348924\n",
      "Earliest date:   2012-07\n",
      "Latest date:   2017-12\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Starting Cleaning---------------------------\\n')\n",
    "df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean = clean_imports(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")\n",
    "\n",
    "# df_s12_clean.to_csv(os.path.join(output_path, 'df_s12_clean.csv'))\n",
    "# df_s12type5_clean.to_csv(os.path.join(output_path, 'df_s12type5_clean.csv'))\n",
    "# df_s34_clean.to_csv(os.path.join(output_path, 'df_s34_clean.csv'))\n",
    "# df_beta_clean.to_csv(os.path.join(output_path, 'df_beta_clean.csv'))\n",
    "# df_security_clean.to_csv(os.path.join(output_path, 'df_security_clean.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:50:18.113634Z",
     "start_time": "2024-04-01T05:48:29.895551Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Assets/Factors---------------------------\n",
      "\n",
      "Merged assets and factors\n",
      "Number of assets/dates:   118413\n",
      "\n",
      "\n",
      "---------------Merging s12/s34 Holdings---------------------------\n",
      "\n",
      "Merged s12 and s34\n",
      "Number of holdings:   22267070\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Assets/Factors---------------------------\\n')\n",
    "df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "# df_asset.to_csv(os.path.join(output_path, 'df_asset.csv'))\n",
    "\n",
    "print('\\n---------------Merging s12/s34 Holdings---------------------------\\n')\n",
    "df_fund_manager = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "# df_fund_manager.to_csv(os.path.join(output_path, 'df_fund_manager.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:51:05.833983200Z",
     "start_time": "2024-04-01T05:50:18.115857Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:54:39.691623900Z",
     "start_time": "2024-04-01T05:51:05.836166800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Constructing Zero Holdings---------------------------\n",
      "\n",
      "Constructed zero holdings\n",
      "Number of non-zero holdings:   22267070\n",
      "Number of zero holdings:   2478516\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Constructing Zero Holdings---------------------------\\n')\n",
    "df_holding = construct_zero_holdings(df_fund_manager, n_quarters)\n",
    "# df_holding.to_csv(os.path.join(output_path, 'df_holding.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Holdings/Factors---------------------------\n",
      "\n",
      "Merged holdings and factors\n",
      "Number of investor/date/asset:   18074588\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Holdings/Factors---------------------------\\n')\n",
    "df_holding_factor = merge_holding_factor(df_holding, df_asset)\n",
    "# df_holding_factor.to_csv(os.path.join(output_path, 'df_holding_factor.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:54:48.695783700Z",
     "start_time": "2024-04-01T05:54:39.700992800Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Dropping Mangers/Assets Without Holdings---------------------------\n",
      "\n",
      "Dropped unused investors/assets\n",
      "Number of holdings:   18029864\n",
      "\n",
      "\n",
      "---------------Creating Household Sector---------------------------\n",
      "\n",
      "Created household sector\n",
      "Number of holdings:   87361\n",
      "\n",
      "---------------Partitioning Outside Asset---------------------------\n",
      "\n",
      "Created outside asset\n",
      "Number of outside holdings:   1096998\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Dropping Mangers/Assets Without Holdings---------------------------\\n')\n",
    "df_dropped = drop_unsused_inv_asset(df_holding_factor)\n",
    "# df_holding.to_csv(os.path.join(output_path, 'df_holding.csv'))\n",
    "\n",
    "print('\\n---------------Creating Household Sector---------------------------\\n')\n",
    "df_household = create_household_sector(df_dropped)\n",
    "# df_household.to_csv(os.path.join(output_path, 'df_household.csv'))\n",
    "\n",
    "print('\\n---------------Partitioning Outside Asset---------------------------\\n')\n",
    "df_outside = partition_outside_asset(df_household)\n",
    "# df_outside.to_csv(os.path.join(output_path, 'df_outside.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:55:01.498429300Z",
     "start_time": "2024-04-01T05:54:48.702135600Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Investor AUM---------------------------\n",
      "\n",
      "Calculated investor AUM\n",
      "Number of investors:   5972\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Investor AUM---------------------------\\n')\n",
    "df_inv_aum = calc_inv_aum(df_outside)\n",
    "# df_inv_aum.to_csv(os.path.join(output_path, 'df_inv_aum.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:55:08.450684300Z",
     "start_time": "2024-04-01T05:55:01.498429300Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Aggregating Small Investors---------------------------\n",
      "\n",
      "Aggregated small investors\n",
      "Number of investors:   5163\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Aggregating Small Investors---------------------------\\n')\n",
    "df_agg = agg_small_inv(df_inv_aum)\n",
    "# df_agg.to_csv(os.path.join(output_path, 'df_agg.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:55:48.297298Z",
     "start_time": "2024-04-01T05:55:08.462254500Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Pooling Investors By Type/Size---------------------------\n",
      "\n",
      "Binned investors\n",
      "Number of investors:   5163\n",
      "Number of bins:   111\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Pooling Investors By Type/Size---------------------------\\n')\n",
    "df_binned = bin_inv(df_agg, min_n_holding)\n",
    "# df_binned.to_csv(os.path.join(output_path, 'df_binned.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:56:04.098408600Z",
     "start_time": "2024-04-01T05:55:48.300577400Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:56:09.574434500Z",
     "start_time": "2024-04-01T05:56:04.050943600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Instrument---------------------------\n",
      "\n",
      "Created market equity instrument\n",
      "Number of valid instruments:   16362962\n",
      "Number of invalid instruments:   378\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Instrument---------------------------\\n')\n",
    "df_instrument = calc_instrument(df_binned)\n",
    "# df_instrument.to_csv(os.path.join(output_path, 'df_instrument.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:56:37.824586400Z",
     "start_time": "2024-04-01T05:56:09.573353100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Holding Weights---------------------------\n",
      "\n",
      "Calculated holding weights\n",
      "Number of non-zero holdings:   14612714\n",
      "Number of zero holdings:   1750248\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Holding Weights---------------------------\\n')\n",
    "df_weights = calc_holding_weights(df_instrument)\n",
    "# df_weights.to_csv(os.path.join(output_path, 'df_weights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T06:04:07.813443600Z",
     "start_time": "2024-04-01T06:03:56.367601800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Estimating Demand System---------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit_inv_date_L() got an unexpected keyword argument 'args'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1770\u001B[0m, in \u001B[0;36mGroupBy.apply\u001B[1;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1769\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1770\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_python_apply_general(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_selected_obj)\n\u001B[0;32m   1771\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   1772\u001B[0m     \u001B[38;5;66;03m# gh-20949\u001B[39;00m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;66;03m# try again, with .apply acting as a filtering\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1777\u001B[0m     \u001B[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001B[39;00m\n\u001B[0;32m   1778\u001B[0m     \u001B[38;5;66;03m# on a string grouper column\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1819\u001B[0m, in \u001B[0;36mGroupBy._python_apply_general\u001B[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001B[0m\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;124;03mApply function f in python space\u001B[39;00m\n\u001B[0;32m   1795\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1817\u001B[0m \u001B[38;5;124;03m    data after applying f\u001B[39;00m\n\u001B[0;32m   1818\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1819\u001B[0m values, mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrouper\u001B[38;5;241m.\u001B[39mapply_groupwise(f, data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis)\n\u001B[0;32m   1820\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_indexed_same \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:911\u001B[0m, in \u001B[0;36mBaseGrouper.apply_groupwise\u001B[1;34m(self, f, data, axis)\u001B[0m\n\u001B[0;32m    910\u001B[0m group_axes \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39maxes\n\u001B[1;32m--> 911\u001B[0m res \u001B[38;5;241m=\u001B[39m f(group)\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mutated \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1758\u001B[0m, in \u001B[0;36mGroupBy.apply.<locals>.f\u001B[1;34m(g)\u001B[0m\n\u001B[0;32m   1756\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf\u001B[39m(g):\n\u001B[1;32m-> 1758\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(g, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mTypeError\u001B[0m: fit_inv_date_L() got an unexpected keyword argument 'args'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Estimating Demand System---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m df_model \u001B[38;5;241m=\u001B[39m estimate_model_L(df_weights, characteristics, params, min_n_holding)\n",
      "Cell \u001B[1;32mIn[23], line 61\u001B[0m, in \u001B[0;36mestimate_model_L\u001B[1;34m(df_weights, characteristics, params, min_n_holding)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mestimate_model_L\u001B[39m(df_weights: pd\u001B[38;5;241m.\u001B[39mDataFrame, characteristics: \u001B[38;5;28mlist\u001B[39m, params: \u001B[38;5;28mlist\u001B[39m, min_n_holding: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[0;32m     56\u001B[0m     mask \u001B[38;5;241m=\u001B[39m df_weights[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_holding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m min_n_holding\n\u001B[0;32m     58\u001B[0m     df_institutions \u001B[38;5;241m=\u001B[39m (df_weights\n\u001B[0;32m     59\u001B[0m                        \u001B[38;5;241m.\u001B[39mloc[mask]\n\u001B[0;32m     60\u001B[0m                        \u001B[38;5;241m.\u001B[39mset_index([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 61\u001B[0m                        \u001B[38;5;241m.\u001B[39massign(gmm_result\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mapply(fit_inv_date_L, args\u001B[38;5;241m=\u001B[39m(characteristics, params)))\n\u001B[0;32m     62\u001B[0m                        \u001B[38;5;241m.\u001B[39mreset_index())\n\u001B[0;32m     64\u001B[0m     df_bins \u001B[38;5;241m=\u001B[39m (df_weights\n\u001B[0;32m     65\u001B[0m                \u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[0;32m     66\u001B[0m                \u001B[38;5;241m.\u001B[39mset_index([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     67\u001B[0m                \u001B[38;5;241m.\u001B[39massign(gmm_result\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mapply(fit_inv_date_L, args\u001B[38;5;241m=\u001B[39m(characteristics, params)))\n\u001B[0;32m     68\u001B[0m                \u001B[38;5;241m.\u001B[39mreset_index())\n\u001B[0;32m     70\u001B[0m     df_model \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df_institutions, df_bins])\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5012\u001B[0m, in \u001B[0;36mDataFrame.assign\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m   5009\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m   5011\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m-> 5012\u001B[0m     data[k] \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(v, data)\n\u001B[0;32m   5013\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:379\u001B[0m, in \u001B[0;36mapply_if_callable\u001B[1;34m(maybe_callable, obj, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;124;03mEvaluate possibly callable input using obj and kwargs if it is callable,\u001B[39;00m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;124;03motherwise return as it is.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m \u001B[38;5;124;03m**kwargs\u001B[39;00m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(maybe_callable):\n\u001B[1;32m--> 379\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m maybe_callable(obj, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_callable\n",
      "Cell \u001B[1;32mIn[23], line 61\u001B[0m, in \u001B[0;36mestimate_model_L.<locals>.<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mestimate_model_L\u001B[39m(df_weights: pd\u001B[38;5;241m.\u001B[39mDataFrame, characteristics: \u001B[38;5;28mlist\u001B[39m, params: \u001B[38;5;28mlist\u001B[39m, min_n_holding: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[0;32m     56\u001B[0m     mask \u001B[38;5;241m=\u001B[39m df_weights[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_holding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m min_n_holding\n\u001B[0;32m     58\u001B[0m     df_institutions \u001B[38;5;241m=\u001B[39m (df_weights\n\u001B[0;32m     59\u001B[0m                        \u001B[38;5;241m.\u001B[39mloc[mask]\n\u001B[0;32m     60\u001B[0m                        \u001B[38;5;241m.\u001B[39mset_index([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 61\u001B[0m                        \u001B[38;5;241m.\u001B[39massign(gmm_result\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mapply(fit_inv_date_L, args\u001B[38;5;241m=\u001B[39m(characteristics, params)))\n\u001B[0;32m     62\u001B[0m                        \u001B[38;5;241m.\u001B[39mreset_index())\n\u001B[0;32m     64\u001B[0m     df_bins \u001B[38;5;241m=\u001B[39m (df_weights\n\u001B[0;32m     65\u001B[0m                \u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[0;32m     66\u001B[0m                \u001B[38;5;241m.\u001B[39mset_index([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     67\u001B[0m                \u001B[38;5;241m.\u001B[39massign(gmm_result\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mapply(fit_inv_date_L, args\u001B[38;5;241m=\u001B[39m(characteristics, params)))\n\u001B[0;32m     68\u001B[0m                \u001B[38;5;241m.\u001B[39mreset_index())\n\u001B[0;32m     70\u001B[0m     df_model \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df_institutions, df_bins])\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1780\u001B[0m, in \u001B[0;36mGroupBy.apply\u001B[1;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1770\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_python_apply_general(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_selected_obj)\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   1772\u001B[0m         \u001B[38;5;66;03m# gh-20949\u001B[39;00m\n\u001B[0;32m   1773\u001B[0m         \u001B[38;5;66;03m# try again, with .apply acting as a filtering\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1777\u001B[0m         \u001B[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001B[39;00m\n\u001B[0;32m   1778\u001B[0m         \u001B[38;5;66;03m# on a string grouper column\u001B[39;00m\n\u001B[1;32m-> 1780\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_python_apply_general(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj_with_exclusions)\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1819\u001B[0m, in \u001B[0;36mGroupBy._python_apply_general\u001B[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001B[0m\n\u001B[0;32m   1784\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   1785\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_python_apply_general\u001B[39m(\n\u001B[0;32m   1786\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1791\u001B[0m     is_agg: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1792\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDFrameT:\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;124;03m    Apply function f in python space\u001B[39;00m\n\u001B[0;32m   1795\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1817\u001B[0m \u001B[38;5;124;03m        data after applying f\u001B[39;00m\n\u001B[0;32m   1818\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1819\u001B[0m     values, mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrouper\u001B[38;5;241m.\u001B[39mapply_groupwise(f, data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis)\n\u001B[0;32m   1820\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m not_indexed_same \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1821\u001B[0m         not_indexed_same \u001B[38;5;241m=\u001B[39m mutated\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:911\u001B[0m, in \u001B[0;36mBaseGrouper.apply_groupwise\u001B[1;34m(self, f, data, axis)\u001B[0m\n\u001B[0;32m    909\u001B[0m \u001B[38;5;66;03m# group might be modified\u001B[39;00m\n\u001B[0;32m    910\u001B[0m group_axes \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39maxes\n\u001B[1;32m--> 911\u001B[0m res \u001B[38;5;241m=\u001B[39m f(group)\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mutated \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_indexed_like(res, group_axes, axis):\n\u001B[0;32m    913\u001B[0m     mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1758\u001B[0m, in \u001B[0;36mGroupBy.apply.<locals>.f\u001B[1;34m(g)\u001B[0m\n\u001B[0;32m   1756\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf\u001B[39m(g):\n\u001B[1;32m-> 1758\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(g, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mTypeError\u001B[0m: fit_inv_date_L() got an unexpected keyword argument 'args'"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Estimating Demand System---------------------------\\n')\n",
    "df_model = estimate_model_L(df_weights, characteristics, params, min_n_holding)\n",
    "# df_model.to_csv(os.path.join(output_path, 'df_model.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.739190300Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Latent Demand---------------------------\\n')\n",
    "df_results = calc_latent_demand_L(df_model, characteristics, params)\n",
    "# df_results.to_csv(os.path.join(output_path, 'df_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.741263200Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Moment Condition---------------------------\\n')\n",
    "# df_figures = clean_figures(df_results)\n",
    "# _ = check_moment_condition(df_figures, 0, min_n_holding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.752915100Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Investors by Typecode---------------------------\\n')\n",
    "# _ = typecode_share_counts(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T05:56:52.798013400Z",
     "start_time": "2024-04-01T05:56:52.753994500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Instrument Validity---------------------------\\n')\n",
    "# _ = critical_value_test(df_figures, characteristics, min_n_holding, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.753994500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Hypothetical Index Fund---------------------------\\n')\n",
    "# _ = test_index_fund(df_figures, characteristics, params, indexfund_id, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.753994500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Parameters By Typecode---------------------------\\n')\n",
    "# _ = graph_type_params(df_figures, params, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.754996900Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Standard Deviation of Latent Demand By Typecode---------------------------\\n')\n",
    "# _ = graph_std_latent_demand(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Price Elasticity---------------------------\\n')\n",
    "df_liquidity = calc_price_elasticity(df_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.754996900Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_liquidity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.756047100Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Predictability---------------------------\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.759207900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Simulating Monetary Policy Shock---------------------------\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.760395600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-01T05:56:52.762483Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Finished---------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
