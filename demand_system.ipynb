{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import post as requests_post\n",
    "import wrds\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T18:54:22.629162800Z",
     "start_time": "2024-02-24T18:54:19.959758300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def request_wrds(path: str,\n",
    "                 start_date: pd.Period,\n",
    "                 end_date: pd.Period) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    with wrds.Connection() as db:\n",
    "        df_s12 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s12\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s12.to_csv(f'{path}s12.csv')\n",
    "\n",
    "        df_s34 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s34\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s34.to_csv(f'{path}s34.csv')\n",
    "\n",
    "        df_ffm = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM ff.factors_monthly\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_ffm.to_csv(f'{path}ffm.csv')\n",
    "\n",
    "        df_security = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM ff.factors_monthly\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_security.to_csv(f'{path}security.csv')\n",
    "\n",
    "        df_fundamental = db.raw_sql(f'''TODO''', date_cols=['date'])\n",
    "        df_fundamental.to_csv(f'{path}fundamental.csv')\n",
    "\n",
    "        return df_s12, df_s34, df_ffm, df_security, df_fundamental\n",
    "\n",
    "\n",
    "# def wrds_beta(api_key: str, begin_date: str, end_date: str):\n",
    "#     url = 'https://wrds-api.wharton.upenn.edu/v0.1/research/betas/'\n",
    "#     headers = {\n",
    "#         'Authorization': f'Token {api_key}',\n",
    "#     }\n",
    "#     data = {\n",
    "#         'company_identifier': 'PERMNO',\n",
    "#         'begin_date': begin_date,\n",
    "#         'end_date': end_date,\n",
    "#         'freq': 'm',\n",
    "#         'estwindow': 5 * 12,\n",
    "#         'minwindow': 1 * 12,\n",
    "#         'risk_model': 'ff3',\n",
    "#         'return_type': 'r'\n",
    "#     }\n",
    "#     result = requests_post(\n",
    "#         url,\n",
    "#         headers=headers,\n",
    "#         json=data,\n",
    "#     )\n",
    "#     df = pd.DataFrame(result.json()['betas'], columns=result.json()['betas_cols'])\n",
    "#     return df\n",
    "\n",
    "\n",
    "def load_wrds(path: str,\n",
    "              start_date: pd.Period,\n",
    "              end_date: pd.Period\n",
    "              ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    try:\n",
    "        df_s12 = pd.read_csv(f'{path}s12.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12(df_s12)\n",
    "\n",
    "        df_s12type5 = pd.read_csv(f'{path}s12type5.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12type5(df_s12type5)\n",
    "\n",
    "        df_s34 = pd.read_csv(f'{path}s34.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s34(df_s34)\n",
    "\n",
    "        df_ffm = pd.read_csv(f'{path}ffm.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_ffm(df_ffm)\n",
    "\n",
    "        df_beta = pd.read_csv(f'{path}beta.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_beta(df_beta)\n",
    "\n",
    "        df_security = pd.read_csv(f'{path}security.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_security(df_security)\n",
    "\n",
    "        df_fundamental = pd.read_csv(f'{path}fundamental.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_fundamental(df_fundamental)\n",
    "\n",
    "        return df_s12, df_s12type5, df_s34, df_ffm, df_beta, df_security, df_fundamental\n",
    "    except FileNotFoundError:\n",
    "        return request_wrds(path, start_date, end_date)\n",
    "\n",
    "\n",
    "def clean_imports(df_s12,\n",
    "                  df_s12type5,\n",
    "                  df_s34,\n",
    "                  df_ffm,\n",
    "                  df_beta,\n",
    "                  df_security,\n",
    "                  df_fundamental,\n",
    "                  start_date,\n",
    "                  end_date\n",
    "                  ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    df_s12_clean = clean_s12(df_s12, start_date, end_date)\n",
    "    log_clean_s12(df_s12_clean)\n",
    "\n",
    "    df_s12type5_clean = clean_s12type5(df_s12type5, start_date, end_date)\n",
    "    log_clean_s12type5(df_s12type5_clean)\n",
    "\n",
    "    df_s34_clean = clean_s34(df_s34, start_date, end_date)\n",
    "    log_clean_s34(df_s34_clean)\n",
    "\n",
    "    df_ffm_clean = clean_ffm(df_ffm, start_date, end_date)\n",
    "    log_clean_ffm(df_ffm_clean)\n",
    "\n",
    "    df_beta_clean = clean_beta(df_beta, start_date, end_date)\n",
    "    log_clean_beta(df_beta_clean)\n",
    "\n",
    "    df_security_clean = clean_security(df_security, start_date, end_date)\n",
    "    log_clean_security(df_security_clean)\n",
    "\n",
    "    # TODO: df_fundamental_clean = clean_fundamental(df_fundamental, start_date, end_date)\n",
    "    df_fundamental_clean = df_fundamental\n",
    "    # log_clean_fundamental(df_fundamental_clean)\n",
    "\n",
    "    return df_s12_clean, df_s12type5_clean, df_s34_clean, df_ffm_clean, df_beta_clean, df_security_clean, df_fundamental_clean\n",
    "\n",
    "\n",
    "def clean_s12(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'fundno',\n",
    "        'rdate',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    df['rdate'] = df['rdate'].apply(fix_date, args=(start_date, end_date))\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['fundno', 'shares', 'cusip'])\n",
    "            .drop_duplicates(subset=['fundno', 'rdate', 'cusip'], keep='last')\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'fundno': 'inv_id',\n",
    "                'cusip': 'asset_id'\n",
    "            })\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_s12type5(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    return (df\n",
    "            .assign(fdate=df['fdate'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .dropna(how='any', subset=['fundno', 'fdate'])\n",
    "            .rename(columns={'fdate': 'date', 'fundno': 'inv_id'})\n",
    "            .set_index(['inv_id', 'date']))\n",
    "\n",
    "\n",
    "def clean_s34(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'mgrno',\n",
    "        'rdate',\n",
    "        'typecode',\n",
    "        'cusip',\n",
    "        'shares',\n",
    "        'backup_holding',\n",
    "        'backup_me'\n",
    "    ]\n",
    "    df['rdate'] = df['rdate'].apply(fix_date, args=(start_date, end_date))\n",
    "    df['backup_holding'] = df['shares'] * df['prc']\n",
    "    df['backup_me'] = df['shrout2'] * df['prc'] * 1000\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['cusip', 'shares'])\n",
    "            .drop_duplicates(subset=['mgrno', 'rdate', 'cusip'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'mgrno': 'inv_id',\n",
    "                'cusip': 'asset_id'\n",
    "            })\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_ffm(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    df['dateff'] = df['dateff'].apply(fix_date, args=(start_date, end_date))\n",
    "    return (df\n",
    "            .dropna()\n",
    "            .rename(columns={'dateff': 'date'})\n",
    "            .set_index(['date']))\n",
    "\n",
    "\n",
    "def clean_beta(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    columns=[\n",
    "        'PERMNO',\n",
    "        'DATE',\n",
    "        'b_mkt',\n",
    "        'b_smb',\n",
    "        'b_hml'\n",
    "    ]\n",
    "    df['DATE'] = df['DATE'].apply(fix_date, args=(start_date, end_date)) + 6\n",
    "    return (df[columns]\n",
    "            .rename(columns={\n",
    "                'DATE': 'date',\n",
    "                'PERMNO':'permno'})\n",
    "            .dropna()\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def clean_security(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'LPERMNO',\n",
    "        'cusip',\n",
    "        'datadate',\n",
    "        'prccm',\n",
    "        'cshoq'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .rename(columns={\n",
    "                'LPERMNO': 'permno',\n",
    "                'cusip': 'asset_id',\n",
    "                'prccm': 'prc',\n",
    "                'cshoq': 'shrout',\n",
    "                'datadate': 'date'})\n",
    "            .assign(\n",
    "                shrout=df['cshoq'] * 1000000,\n",
    "                asset_id=df['cusip'].apply(lambda  x: x[:-1]),\n",
    "                date=df['datadate'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .dropna(how='any', subset=['asset_id', 'prc', 'shrout'])\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def clean_fundamental(df: pd.DataFrame, start_date: pd.Period, end_date: pd.Period) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'LPERMNO'\n",
    "        'cusip',\n",
    "        'datadate',\n",
    "        'fyearq',\n",
    "        'fqtr',\n",
    "        'fyr'\n",
    "        'act',\n",
    "        'at',\n",
    "        'ceq',\n",
    "        'che',\n",
    "        'cogs',\n",
    "        'csho',\n",
    "        'dlc',\n",
    "        'lct',\n",
    "        'lt',\n",
    "        'pstk',\n",
    "        'pstkl',\n",
    "        'pstkrv',\n",
    "        'revt',\n",
    "        'seq',\n",
    "        'txditc',\n",
    "        'xint',\n",
    "        'xsga',\n",
    "        'sich'\n",
    "    ]\n",
    "    df.columns = [col[:-1] if col[-1] == 'q' else col for col in df.columns]\n",
    "    return (df[columns]\n",
    "            .rename(columns={\n",
    "                'LPERMNO': 'permno',\n",
    "                'cusip': 'asset_id',\n",
    "                'csho': 'shrout'})\n",
    "            .assign(\n",
    "                shrout=df['shrout'] * df['ajex'] * 1000,\n",
    "                date=df['date'].apply(fix_date, args=(start_date, end_date)),\n",
    "                na_flag=df['cogs'].isnan() & df['cogs'].isnan() & df['cogs'].isnan(),\n",
    "                cogs=lambda x: 0 if x['na_flag'] else x['cogs'],\n",
    "                xint=lambda x: 0 if x['na_flag'] else x['xint'],\n",
    "                xsga=lambda x: 0 if x['na_flag'] else x['xsga'],\n",
    "                txditc=df['txditc'].fillna(0),\n",
    "                seq=lambda x: x['ceq'] + x['pstk'] if np.isnan(x['seq']) else x['seq'])\n",
    "            .assign(\n",
    "                seq=lambda x: x['seq'].fillna(x['at'] - x['lt']),\n",
    "                at=lambda x: x['seq'].fillna(x['lt'] + x['seq']),\n",
    "                lt=lambda x: x['seq'].fillna(x['at'] - x['seq']),\n",
    "                lag_date=lambda x: x['date'] + 6,\n",
    "                preferred=lambda x: x['pstkrv'].fillna(x['pstkl'].fillna(x['pstk'])),\n",
    "                be=lambda x: np.nan if x['seq'] + x['txditc'] - x['preferred'] <= 0 else x['seq'] + x['txditc'] - x['preferred'],\n",
    "                operating=lambda x: x['act'] - x['che'] - x['lct'] + x['dlc'],\n",
    "                at_be=lambda x: x['at'] / x['be'],\n",
    "                profit=lambda x: (x['revt'] - x['cogs'] - x['xsga'] - x['xint']) / x['be'],\n",
    "                lag_fyear=df['fyear'].shift(1),\n",
    "                accruals='df')\n",
    "            .drop(columns=['ajex', 'lag_fyear'])\n",
    "            .dropna(how='any', subset=['asset_id', 'prc', 'shrout'])\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def fix_date(date: str, start_date: pd.Period, end_date: pd.Period) -> pd.Period:\n",
    "    date_converted = pd.Period(date, freq='M')\n",
    "\n",
    "    if date_converted < start_date or date_converted > end_date:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return date_converted\n",
    "\n",
    "\n",
    "# Stocks Monthly\n",
    "\n",
    "def merge_assets_factors(df_assets: pd.DataFrame, df_factors: pd.DataFrame) -> pd.DataFrame:\n",
    "    merged = pd.merge(left=df_assets,\n",
    "                      right=df_factors,\n",
    "                      how='inner',\n",
    "                      left_index=True,\n",
    "                      right_index=True)\n",
    "    merged_indexed = (merged\n",
    "                      .reset_index('permno', drop=True)\n",
    "                      .set_index('asset_id', append=True)\n",
    "                      .sort_index())\n",
    "    log_asset_merge(merged_indexed)\n",
    "    return merged_indexed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T18:54:22.693836200Z",
     "start_time": "2024-02-24T18:54:22.663034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Manager / Holdings\n",
    "\n",
    "def match_fund_manager(df_fund: pd.DataFrame, df_manager: pd.DataFrame, df_key: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO\n",
    "    # df_fund_wkey = df_fund.assign(mgrno=lambda x: df_key.loc[x.index.get_level_values(0), 'mgrcocd'])\n",
    "    # df_merged = df_manager.merge(df_fund_wkey, how='outer', on='mgrno')\n",
    "    log_holding_merge(df_manager)\n",
    "    return df_manager\n",
    "\n",
    "\n",
    "def merge_holding_factor(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_holding,\n",
    "        right=df_asset,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        indicator=True\n",
    "    ).reset_index()\n",
    "\n",
    "    df_merged = (df_merged\n",
    "                .assign(\n",
    "                    ccm_holding=lambda x: x['prc'] * x['shares'],\n",
    "                    ccm_me=lambda x: x['prc'] * x['shrout'],\n",
    "                    holding=lambda x: x['backup_holding'] if x['ccm_holding'].isna().any() else x['ccm_holding'],\n",
    "                    me=lambda x: x['backup_me'] if x['ccm_me'].isna().any() else x['ccm_me'],\n",
    "                    date=df_merged['date'].apply(lambda x: x.asfreq('Q')))\n",
    "                 .drop(columns=['ccm_holding', 'backup_holding', 'ccm_me', 'backup_me'])\n",
    "                .set_index(['inv_id', 'date', 'asset_id'])\n",
    "                .sort_index())\n",
    "    log_holding_factor_merge(df_merged)\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def create_outside_asset(df_holding_factor: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    df_inside = df_holding_factor[df_holding_factor['_merge'] == 'both'].drop(columns='_merge')\n",
    "    df_outside = df_holding_factor[df_holding_factor['_merge'] == 'left_only'].groupby(['inv_id', 'date']).agg({\n",
    "        'typecode': 'first',\n",
    "        'holding': 'sum'\n",
    "    }).assign(asset_id='-1').set_index('asset_id', append=True)\n",
    "    log_outside_asset(df_outside)\n",
    "    return df_inside, df_outside\n",
    "\n",
    "\n",
    "def assetid_all(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.Series:\n",
    "    unique_asset_id = df_holding.index.unique('asset_id')\n",
    "    universe_asset_id = df_asset.index.unique('asset_id')\n",
    "    intersection = np.intersect1d(unique_asset_id, universe_asset_id, assume_unique=True)\n",
    "    return pd.Series(intersection)\n",
    "\n",
    "\n",
    "def assetid_byinv(df_holding: pd.DataFrame) -> pd.Series:\n",
    "    return df_holding.reset_index('asset_id')['asset_id']\n",
    "\n",
    "\n",
    "def date_byassetid(df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    idx_holding = (df_outside\n",
    "                   .reset_index('date')\n",
    "                   ['date']\n",
    "                   .groupby('asset_id', group_keys=True)\n",
    "                   .apply(np.unique))\n",
    "    df_date_byassetid = idx_holding.copy()\n",
    "    for asset_id in idx_holding.index:\n",
    "        holding_dates = idx_holding.loc[asset_id]\n",
    "        df_date_byassetid.loc[asset_id] = holding_dates\n",
    "\n",
    "    return (df_date_byassetid\n",
    "            .drop('-1')\n",
    "            .explode()\n",
    "            .reset_index())\n",
    "\n",
    "\n",
    "def create_household_sector(df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_outside\n",
    "                      .groupby(['date', 'asset_id'])\n",
    "                      .agg({\n",
    "                        'shares': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'shrout': 'last',\n",
    "                        'b_mkt': 'last',\n",
    "                        'b_smb': 'last',\n",
    "                        'b_hml': 'last',\n",
    "                        'holding': 'sum',\n",
    "                        'me': 'last'})\n",
    "                      .assign(\n",
    "                        shares=lambda x: x['shrout'] - x['shares'],\n",
    "                        holding=lambda x: x['me'] - x['holding'],\n",
    "                        inv_id=-1,\n",
    "                        typecode=-1)\n",
    "                    .set_index('inv_id', append=True)\n",
    "                    .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    log_household_sector(df_household)\n",
    "    return df_household"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T18:54:22.720524Z",
     "start_time": "2024-02-24T18:54:22.688576800Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def calc_inv_aum(df_inside: pd.DataFrame, df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_in_aum = (df_inside\n",
    "                 .groupby(['inv_id', 'date'])\n",
    "                 .agg({\n",
    "                   'holding': 'sum',\n",
    "                   'shares': 'count',\n",
    "                   'typecode': 'last'})\n",
    "                 .rename(columns={\n",
    "                   'holding': 'in_aum',\n",
    "                   'shares': 'n_holding'}))\n",
    "\n",
    "    df_out_aum = (df_outside\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({'holding': 'sum'})\n",
    "                  .rename(columns={'holding': 'out_aum'}))\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        left=df_in_aum,\n",
    "        right=df_out_aum,\n",
    "        how='outer',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    df_inv_aum = (df_merged\n",
    "                  .assign(\n",
    "                    out_aum=lambda x: x['out_aum'].fillna(0),\n",
    "                    in_aum=lambda x: x['in_aum'].fillna(0),\n",
    "                    aum=lambda x: x['out_aum'] + x['in_aum']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def bin_concentrated_inv(df_inside: pd.DataFrame, df_inv_aum: pd.DataFrame) -> pd.DataFrame:\n",
    "    cutoff = 50\n",
    "    diversified_mask = df_inv_aum['n_holding'] >= cutoff\n",
    "    household_mask = (df_inv_aum['out_aum'] == 0) & (df_inv_aum['in_aum'] == 0)\n",
    "    \n",
    "    df_aum_diversified = df_inv_aum[diversified_mask & ~household_mask]\n",
    "    df_aum_concentrated = df_inv_aum[~diversified_mask & ~household_mask].assign(bin=np.NaN)\n",
    "    \n",
    "    inv_type_codes = df_inv_aum['typecode'].unique()\n",
    "    for type in inv_type_codes:\n",
    "        type_mask = df_aum_concentrated['typecode'] == type\n",
    "        df_type = df_aum_concentrated[type_mask]\n",
    "        n_bins = np.ceil(df_type['n_holding'].sum() / (2 * cutoff)).astype(int)\n",
    "        \n",
    "        if n_bins <= 1:\n",
    "            df_aum_concentrated.loc[type_mask, 'bin'] = 10 * type\n",
    "        else:\n",
    "            df_aum_concentrated.loc[type_mask, 'bin'] = pd.qcut(df_type['aum'], q=n_bins, labels=False) + (10 * type)\n",
    "\n",
    "\n",
    "    df_concentrated_binned = (df_aum_concentrated\n",
    "                              .reset_index()\n",
    "                              .groupby(['bin', 'date'])\n",
    "                              .agg({\n",
    "                                'in_aum': 'sum',\n",
    "                                'out_aum': 'sum',\n",
    "                                'aum': 'sum',\n",
    "                                'n_holding': 'sum',\n",
    "                                'typecode': 'last'})\n",
    "                              .rename_axis(index={'bin': 'inv_id'}))\n",
    "    df_aum_binned = (pd.concat([df_aum_diversified, df_concentrated_binned])\n",
    "                     .sort_index()\n",
    "                     .assign(\n",
    "                       out_weight=lambda x: x['out_aum'] / x['aum'],\n",
    "                       in_weight=lambda x: x['in_aum'] / x['aum']))\n",
    "    \n",
    "    df_inside_merged = pd.merge(\n",
    "        left=df_inside,\n",
    "        right=df_aum_concentrated['bin'],\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "                        \n",
    "    df_inside_binned = (df_inside_merged\n",
    "                        .reset_index()\n",
    "                        .assign(bin=lambda x: x['bin'].fillna(x['inv_id']).astype(int))\n",
    "                        .groupby(['bin', 'date', 'asset_id'])\n",
    "                        .agg({\n",
    "                          'holding': 'sum',\n",
    "                          'prc': 'last',\n",
    "                          'shrout': 'last',\n",
    "                          'b_mkt': 'last',\n",
    "                          'b_smb': 'last',\n",
    "                          'b_hml': 'last'})\n",
    "                        .rename_axis(index={'bin': 'inv_id'}))\n",
    "    \n",
    "    log_bins(df_inside_binned, df_aum_binned)\n",
    "    return df_inside_binned, df_aum_binned\n",
    "\n",
    "\n",
    "def calc_inv_universe(df_holding: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_quarters = 11\n",
    "    df_assetid_byinv = assetid_byinv(df_holding).sort_index()\n",
    "    idx_inv_universe = df_assetid_byinv.index.unique()\n",
    "    df_inv_universe = pd.DataFrame(index=idx_inv_universe, columns=['inv_universe'])\n",
    "\n",
    "    def calc_past_quarters(i: int, d: pd.Period) -> np.array:\n",
    "        prev_date = d - num_quarters\n",
    "        asset_id_within_range = df_assetid_byinv.loc[i].loc[prev_date:d]\n",
    "        inv_uni = asset_id_within_range.unique().tolist()\n",
    "        return inv_uni\n",
    "\n",
    "    for (inv_id, date) in idx_inv_universe.to_flat_index():\n",
    "        inv_uni = calc_past_quarters(inv_id, date)\n",
    "        df_inv_universe.loc[(inv_id, date), 'inv_universe'] = inv_uni\n",
    "\n",
    "    df_inv_universe = df_inv_universe.assign(uni_size=lambda x: x['inv_universe'].apply(len))\n",
    "    log_inv_universe(df_inv_universe)\n",
    "    return df_inv_universe\n",
    "\n",
    "\n",
    "def create_equal_allocation(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_alloc = pd.merge(\n",
    "        left=df_inv_universe,\n",
    "        right=df_aum_binned,\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    return (df_equal_alloc\n",
    "            .assign(allocation=lambda x: x['aum'] / x['uni_size'])\n",
    "            .explode('inv_universe')\n",
    "            .rename(columns={'inv_universe':'asset_id'})\n",
    "            .set_index('asset_id', append=True))\n",
    "\n",
    "\n",
    "def create_total_allocation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df['allocation']\n",
    "            .groupby('asset_id')\n",
    "            .sum())\n",
    "\n",
    "\n",
    "def create_instrument(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_allocation = create_equal_allocation(df_inv_universe, df_aum_binned)\n",
    "    total_allocation = create_total_allocation(df_equal_allocation)\n",
    "    df_instrument = df_equal_allocation.assign(\n",
    "        iv_me=lambda x: total_allocation - x['allocation'],\n",
    "        ln_iv_me=lambda x: np.log(x['iv_me'])).dropna()\n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame, df_inside_binned: pd.DataFrame, df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_instrument,\n",
    "        right=df_inside_binned,\n",
    "        how='outer',\n",
    "        left_index=True,\n",
    "        right_index=True).combine_first(df_holding_factor)\n",
    "    \n",
    "    # df_merged = pd.merge(\n",
    "    #     left=df_merged,\n",
    "    #     right=df_asset,\n",
    "    #     how='left',\n",
    "    #     on=['date', 'asset_id', 'prc', 'shrout', 'b_mkt', 'b_smb', 'b_hml'])\n",
    "\n",
    "    df_model = (df_merged\n",
    "                .assign(\n",
    "                    weight=lambda x: np.log(x['holding'] / x['aum']),\n",
    "                    ln_weight=lambda x: np.log(x['weight']),\n",
    "                    ln_out_weight=lambda x: np.log(x['out_weight']),\n",
    "                    ln_rweight=lambda x: x['ln_weight'] - x['ln_out_weight'],\n",
    "                    rweight=lambda x: np.exp(x['ln_rweight']),\n",
    "                    cons=lambda x: x['ln_rweight'].groupby(['inv_id', 'date']).transform('mean')))\n",
    "    \n",
    "    log_holding_weights(df_model)\n",
    "    return df_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:14:52.382960200Z",
     "start_time": "2024-02-26T04:14:52.357971400Z"
    }
   },
   "execution_count": 247
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def momcond(params, exog):\n",
    "    bound = 0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    characteristics = exog[1:-1]\n",
    "    out_weight = exog[-1]\n",
    "    \n",
    "    ln_me_term = (bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, characteristics)\n",
    "    pred_weight = np.exp(-1 * (ln_me_term + characteristics_term))\n",
    "    \n",
    "    return pred_weight * out_weight\n",
    "\n",
    "\n",
    "def estimate_model(df_model: pd.DataFrame):\n",
    "    idx = df_model.index\n",
    "    cols = ['beta_ln_me', 'beta_cons', 'beta_b_mkt', 'beta_b_smb', 'beta_b_hml']\n",
    "    df_params = pd.DataFrame(\n",
    "        index=idx, \n",
    "        columns=cols)\n",
    "\n",
    "    for (inv_id, date) in idx.to_flat_index():\n",
    "        data = df_model.loc[(inv_id, date)]\n",
    "        exog = data[['ln_me', 'b_mkt', 'b_smb', 'b_hml', 'out_weight']]\n",
    "        instrument = data[['iv_me', 'b_mkt', 'b_smb', 'b_hml', 'out_weight']]\n",
    "        endog = np.ones(exog.shape[0])\n",
    "\n",
    "        model = gmm.NonlinearIVGMM(endog=endog, exog=exog, instrument=instrument, func=momcond, k_moms=1)\n",
    "        w0inv = np.dot(instrument.T, instrument) / len(endog)\n",
    "        start_params = np.ones(exog.shape[1])\n",
    "        result = model.fit(start_params=start_params, maxiter=1000, inv_weights=w0inv, optim_method='hac')\n",
    "        print(result.summary(yname='Demand System for Asset Pricing', xname=cols))\n",
    "        df_params.loc[(inv_id, date)] = result.params\n",
    "        # TODO finish momcond and sending data\n",
    "        # TODO fix param recording/printing\n",
    "        # TODO graphs/tables\n",
    "        \n",
    "    return df_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T02:17:04.490397100Z",
     "start_time": "2024-02-25T02:17:04.473883500Z"
    }
   },
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_import_s12(df_s12: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12['rdate']\n",
    "    print('Imported s12')\n",
    "    print('Number of holdings:  ', len(df_s12))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_import_s12type5(df_s12type5: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5['fdate']\n",
    "    print()\n",
    "    print('Imported s12type5')\n",
    "    print('Number of holdings:  ', len(df_s12type5))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_import_s34(df_s34: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34['rdate']\n",
    "    print()\n",
    "    print('Imported s34')\n",
    "    print('Number of holdings:  ', len(df_s34))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_import_ffm(df_ffm: pd.DataFrame):\n",
    "    dateindex_ffm = df_ffm['dateff']\n",
    "    print()\n",
    "    print('Imported ffm')\n",
    "    print('Number of dates:  ', len(df_ffm))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_import_beta(df_beta: pd.DataFrame):\n",
    "    dateindex_ffm = df_beta['DATE']\n",
    "    print()\n",
    "    print('Imported ffm')\n",
    "    print('Number of dates:  ', len(df_beta))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_import_security(df_security: pd.DataFrame):\n",
    "    dateindex_security = df_security['datadate']\n",
    "    print()\n",
    "    print('Imported security')\n",
    "    print('Number of holdings:  ', len(df_security))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_import_fundamental(df_fundamental: pd.DataFrame):\n",
    "    dateindex_fundamental = df_fundamental['datadate']\n",
    "    print()\n",
    "    print('Imported fundamental')\n",
    "    print('Number of holdings:  ', len(df_fundamental))\n",
    "    print('Earliest date:  ', min(dateindex_fundamental))\n",
    "    print('Latest date:  ', max(dateindex_fundamental))\n",
    "\n",
    "\n",
    "def log_clean_s12(df_s12_clean: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12_clean.index.get_level_values('date')\n",
    "    print('Cleaned s12')\n",
    "    print('Number of holdings:  ', len(df_s12_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_clean_s12type5(df_s12type5_clean: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s12type5')\n",
    "    print('Number of firm/dates:  ', len(df_s12type5_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_clean_s34(df_s34_clean: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s34')\n",
    "    print('Number of holdings:  ', len(df_s34_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_clean_ffm(df_ffm_clean: pd.DataFrame):\n",
    "    dateindex_ffm = df_ffm_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned ffm')\n",
    "    print('Number of dates:  ', len(df_ffm_clean))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_clean_beta(df_beta_clean: pd.DataFrame):\n",
    "    dateindex_beta = df_beta_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned beta')\n",
    "    print('Number of dates:  ', len(df_beta_clean))\n",
    "    print('Earliest date:  ', min(dateindex_beta))\n",
    "    print('Latest date:  ', max(dateindex_beta))\n",
    "\n",
    "\n",
    "def log_clean_security(df_security_clean: pd.DataFrame):\n",
    "    dateindex_security = df_security_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned security')\n",
    "    print('Number of asset/dates:  ', len(df_security_clean))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_clean_fundamental(df_fundamental_clean: pd.DataFrame):\n",
    "    dateindex_fundamental = df_fundamental_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned fundamental')\n",
    "    print('Number of asset/dates:  ', len(df_fundamental_clean))\n",
    "    print('Earliest date:  ', min(dateindex_fundamental))\n",
    "    print('Latest date:  ', max(dateindex_fundamental))\n",
    "\n",
    "\n",
    "def log_holding_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged s12 and s34')\n",
    "    print('Number of holdings:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_asset_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged assets and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_holding: pd.DataFrame):\n",
    "    print('Created outside asset')\n",
    "    print('Number of holdings:  ', len(df_holding))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    print('Calculated investor AUM')\n",
    "    print(df_inv_aum.describe())\n",
    "\n",
    "\n",
    "def log_bins(df_inside_binned: pd.DataFrame, df_aum_binned: pd.DataFrame):\n",
    "    print('Binned investors')\n",
    "    print('Number of investors:  ', len(df_inside_binned.index.get_level_values(0).unique()))\n",
    "    print('Number of investors:  ', len(df_aum_binned.index.get_level_values(0).unique()))\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    print('Created investment universe')\n",
    "    print(df_inv_uni.describe())\n",
    "\n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print('Created market equity instrument')\n",
    "    print(df_instrument.describe())\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print('Calculated holding weights')\n",
    "    print(df_model.describe())\n",
    "\n",
    "\n",
    "def log_inv_groups(df_group: pd.DataFrame):\n",
    "    print('Partitioned investors')\n",
    "    print('Number of groups:  ', len(df_group.index.get_level_values(0)))\n",
    "\n",
    "\n",
    "def log_results(result):\n",
    "    print(result.summary(\n",
    "        yname='log portfolio weight',\n",
    "        xname=['log market equity', 'market return', 'high minus low', 'small minus big']))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T21:22:59.111925200Z",
     "start_time": "2024-02-25T21:22:58.932891100Z"
    }
   },
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Imports---------------------------\n",
      "\n",
      "Imported s12\n",
      "Number of holdings:   30987\n",
      "Earliest date:   2014-06-30\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported s12type5\n",
      "Number of holdings:   572518\n",
      "Earliest date:   12/31/1994\n",
      "Latest date:   9/30/2022\n",
      "\n",
      "Imported s34\n",
      "Number of holdings:   4841\n",
      "Earliest date:   2012-03-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported ffm\n",
      "Number of dates:   72\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-29\n",
      "\n",
      "Imported ffm\n",
      "Number of dates:   432458\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-29\n",
      "\n",
      "Imported security\n",
      "Number of holdings:   402452\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported fundamental\n",
      "Number of holdings:   130615\n",
      "Earliest date:   2012-06-30\n",
      "Latest date:   2018-05-31\n",
      "\n",
      "---------------Starting Cleaning---------------------------\n",
      "Cleaned s12\n",
      "Number of holdings:   27327\n",
      "Earliest date:   2014-06\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned s12type5\n",
      "Number of firm/dates:   49372\n",
      "Earliest date:   2012-03\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned s34\n",
      "Number of holdings:   4809\n",
      "Earliest date:   2012-03\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned ffm\n",
      "Number of dates:   72\n",
      "Earliest date:   2012-01\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned beta\n",
      "Number of dates:   432414\n",
      "Earliest date:   2012-07\n",
      "Latest date:   2018-06\n",
      "\n",
      "Cleaned security\n",
      "Number of asset/dates:   126399\n",
      "Earliest date:   2012-01\n",
      "Latest date:   2017-12\n",
      "\n",
      "---------------Merging Assets/Factors---------------------------\n",
      "\n",
      "Merged assets and factors\n",
      "Number of assets/dates:   98923\n",
      "\n",
      "---------------Merging s12/s34 Holdings---------------------------\n",
      "\n",
      "Merged s12 and s34\n",
      "Number of holdings:   4809\n"
     ]
    }
   ],
   "source": [
    "path = 'data/'\n",
    "output = 'output/'\n",
    "start_date = pd.Period('2012-01', freq='M')\n",
    "end_date = pd.Period('2017-12', freq='M')\n",
    "\n",
    "\n",
    "print('\\n---------------Starting Imports---------------------------\\n')\n",
    "dfs = load_wrds(path, start_date, end_date)\n",
    "\n",
    "print('\\n---------------Starting Cleaning---------------------------\\n')\n",
    "df_s12_clean, df_s12type5_clean, df_s34_clean, df_ffm_clean, df_beta_clean, df_security_clean, df_fundamental_clean = clean_imports(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")\n",
    "\n",
    "df_s12_clean.to_csv(os.path.join(output, 'df_s12_clean.csv'))\n",
    "df_s12type5_clean.to_csv(os.path.join(output, 'df_s12type5_clean.csv'))\n",
    "df_s34_clean.to_csv(os.path.join(output, 'df_s34_clean.csv'))\n",
    "df_ffm_clean.to_csv(os.path.join(output, 'df_ffm_clean.csv'))\n",
    "df_beta_clean.to_csv(os.path.join(output, 'df_beta_clean.csv'))\n",
    "df_security_clean.to_csv(os.path.join(output, 'df_security_clean.csv'))\n",
    "df_fundamental_clean.to_csv(os.path.join(output, 'df_fundamental_clean.csv'))\n",
    "\n",
    "print('\\n---------------Merging Assets/Factors---------------------------\\n')\n",
    "df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "df_asset.to_csv(os.path.join(output, 'df_asset.csv'))\n",
    "\n",
    "print('\\n---------------Merging s12/s34 Holdings---------------------------\\n')\n",
    "df_merged = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "df_merged.to_csv(os.path.join(output, 'df_merged.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T18:55:45.830530600Z",
     "start_time": "2024-02-24T18:54:22.768297700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Holdings/Factors---------------------------\n",
      "Merged holdings and factors\n",
      "Number of assets/dates:   4809\n",
      "\n",
      "\n",
      "---------------Partitioning Outside Asset---------------------------\n",
      "\n",
      "Created outside asset\n",
      "Number of holdings:   52\n",
      "\n",
      "---------------Creating Household Sector---------------------------\n",
      "\n",
      "Created household sector\n",
      "Number of holdings:   2500\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Holdings/Factors---------------------------\\n')\n",
    "df_holding_factor = merge_holding_factor(df_merged, df_asset)\n",
    "df_holding_factor.to_csv(os.path.join(output, 'df_holding_factor.csv'))\n",
    "\n",
    "print('\\n---------------Partitioning Outside Asset---------------------------\\n')\n",
    "df_inside, df_outside = create_outside_asset(df_holding_factor)\n",
    "df_inside.to_csv(os.path.join(output, 'df_inside.csv'))\n",
    "df_outside.to_csv(os.path.join(output, 'df_outside.csv'))\n",
    "\n",
    "print('\\n---------------Creating Household Sector---------------------------\\n')\n",
    "df_household = create_household_sector(df_inside)\n",
    "df_household.to_csv(os.path.join(output, 'df_household.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T01:35:56.605930300Z",
     "start_time": "2024-02-25T01:35:54.443135900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Investor AUM---------------------------\n",
      "\n",
      "Calculated investor AUM\n",
      "             in_aum   n_holding   typecode       out_aum           aum\n",
      "count  5.300000e+01   49.000000  49.000000  5.300000e+01  5.300000e+01\n",
      "mean   5.978051e+08   58.734694   4.204082  2.687144e+08  8.665195e+08\n",
      "std    6.988388e+08   40.906792   0.407206  2.948406e+08  9.017440e+08\n",
      "min    0.000000e+00    1.000000   4.000000  0.000000e+00  1.230994e+07\n",
      "25%    4.529729e+07   38.000000   4.000000  4.904507e+07  9.782133e+07\n",
      "50%    6.837648e+07   55.000000   4.000000  6.282391e+07  1.280629e+08\n",
      "75%    1.185759e+09   75.000000   4.000000  4.815766e+08  1.662714e+09\n",
      "max    1.882495e+09  159.000000   5.000000  1.363582e+09  2.329958e+09\n",
      "\n",
      "---------------Pooling Investors By Type/Size---------------------------\n",
      "\n",
      "Binned investors\n",
      "Number of investors:   7\n",
      "Number of investors:   7\n",
      "\n",
      "---------------Tracking Investment Universe---------------------------\n",
      "\n",
      "Created investment universe\n",
      "         uni_size\n",
      "count   49.000000\n",
      "mean    83.367347\n",
      "std     68.377535\n",
      "min      1.000000\n",
      "25%     47.000000\n",
      "50%     74.000000\n",
      "75%    101.000000\n",
      "max    274.000000\n",
      "\n",
      "---------------Calculating Instrument---------------------------\n",
      "\n",
      "Created market equity instrument\n",
      "          uni_size        in_aum    n_holding     typecode       out_aum  \\\n",
      "count  4085.000000  4.085000e+03  4085.000000  4085.000000  4.085000e+03   \n",
      "mean    138.305753  1.124058e+09    90.904774     4.006365  3.633869e+08   \n",
      "std      74.063657  6.800474e+08    39.500834     0.079535  2.039060e+08   \n",
      "min       1.000000  7.071725e+05     1.000000     4.000000  0.000000e+00   \n",
      "25%      75.000000  7.432199e+07    57.000000     4.000000  5.958799e+07   \n",
      "50%     124.000000  1.434733e+09    77.000000     4.000000  4.474624e+08   \n",
      "75%     189.000000  1.644277e+09   109.000000     4.000000  5.003079e+08   \n",
      "max     274.000000  1.882495e+09   159.000000     5.000000  6.513856e+08   \n",
      "\n",
      "                aum   out_weight    in_weight    allocation         iv_me  \\\n",
      "count  4.085000e+03  4085.000000  4085.000000  4.085000e+03  4.085000e+03   \n",
      "mean   1.487445e+09     0.298926     0.701074  1.060272e+07  2.053710e+08   \n",
      "std    8.677811e+08     0.110601     0.110601  6.326001e+06  1.373370e+08   \n",
      "min    4.057274e+07     0.000000     0.012310  1.460038e+06  0.000000e+00   \n",
      "25%    1.289215e+08     0.223135     0.603466  2.574245e+06  6.364246e+07   \n",
      "50%    2.044520e+09     0.260283     0.739717  1.027838e+07  2.144614e+08   \n",
      "75%    2.143966e+09     0.396534     0.776865  1.679680e+07  3.282753e+08   \n",
      "max    2.329958e+09     0.987690     1.000000  4.909220e+07  5.513709e+08   \n",
      "\n",
      "          ln_iv_me  \n",
      "count  4085.000000  \n",
      "mean          -inf  \n",
      "std            NaN  \n",
      "min           -inf  \n",
      "25%      17.968791  \n",
      "50%      19.183640  \n",
      "75%      19.609363  \n",
      "max      20.127918  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Investor AUM---------------------------\\n')\n",
    "df_inv_aum = calc_inv_aum(df_inside, df_outside)\n",
    "df_inv_aum.to_csv(os.path.join(output, 'df_inv_aum.csv'))\n",
    "\n",
    "print('\\n---------------Pooling Investors By Type/Size---------------------------\\n')\n",
    "df_inside_binned, df_aum_binned = bin_concentrated_inv(df_inside, df_inv_aum)\n",
    "df_inside_binned.to_csv(os.path.join(output, 'df_inside_binned.csv'))\n",
    "df_aum_binned.to_csv(os.path.join(output, 'df_aum_binned.csv'))\n",
    "\n",
    "print('\\n---------------Tracking Investment Universe---------------------------\\n')\n",
    "df_inv_universe = calc_inv_universe(df_inside_binned)\n",
    "df_inv_universe.to_csv(os.path.join(output, 'df_inv_universe.csv'))\n",
    "\n",
    "print('\\n---------------Calculating Instrument---------------------------\\n')\n",
    "df_instrument = create_instrument(df_inv_universe, df_aum_binned)\n",
    "df_instrument.to_csv(os.path.join(output, 'df_instrument.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:05:09.658619100Z",
     "start_time": "2024-02-26T04:05:09.406572200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Holding Weights---------------------------\n",
      "\n",
      "Calculated holding weights\n",
      "         allocation           aum        b_hml        b_mkt        b_smb  \\\n",
      "count  4.085000e+03  4.085000e+03  3246.000000  3246.000000  3246.000000   \n",
      "mean   1.060272e+07  1.487445e+09    -0.004963     1.026580     0.033087   \n",
      "std    6.326001e+06  8.677811e+08     0.643913     0.396848     0.599474   \n",
      "min    1.460038e+06  4.057274e+07    -3.231900    -0.288800    -3.833400   \n",
      "25%    2.574245e+06  1.289215e+08    -0.351300     0.757850    -0.385150   \n",
      "50%    1.027838e+07  2.044520e+09    -0.061350     1.000500    -0.012800   \n",
      "75%    1.679680e+07  2.143966e+09     0.286775     1.255175     0.361850   \n",
      "max    4.909220e+07  2.329958e+09     7.200200     3.101200     2.568100   \n",
      "\n",
      "            holding        in_aum    in_weight         iv_me     ln_iv_me  \\\n",
      "count  5.168000e+03  4.085000e+03  4085.000000  4.085000e+03  4085.000000   \n",
      "mean   8.987703e+06  1.124058e+09     0.701074  2.053710e+08         -inf   \n",
      "std    1.800639e+07  6.800474e+08     0.110601  1.373370e+08          NaN   \n",
      "min    1.905085e+04  7.071725e+05     0.012310  0.000000e+00         -inf   \n",
      "25%    7.677390e+05  7.432199e+07     0.603466  6.364246e+07    17.968791   \n",
      "50%    1.369700e+06  1.434733e+09     0.739717  2.144614e+08    19.183640   \n",
      "75%    2.728816e+06  1.644277e+09     0.776865  3.282753e+08    19.609363   \n",
      "max    1.823118e+08  1.882495e+09     1.000000  5.513709e+08    20.127918   \n",
      "\n",
      "       ...        shares        shrout     typecode     uni_size       weight  \\\n",
      "count  ...  4.809000e+03  3.246000e+03  6374.000000  4085.000000  2888.000000   \n",
      "mean   ...  1.411685e+05  1.260496e+09     4.049733   138.305753    -5.887978   \n",
      "std    ...  5.190805e+05  1.680455e+09     0.217410    74.063657     1.906731   \n",
      "min    ...  4.000000e+00  8.056000e+06     4.000000     1.000000   -11.714249   \n",
      "25%    ...  1.003600e+04  2.070690e+08     4.000000    75.000000    -7.672184   \n",
      "50%    ...  2.127500e+04  6.750000e+08     4.000000   124.000000    -4.900442   \n",
      "75%    ...  6.703100e+04  1.500000e+09     4.000000   189.000000    -4.299703   \n",
      "max    ...  2.564571e+07  1.045747e+10     5.000000   274.000000     0.000000   \n",
      "\n",
      "       ln_weight  ln_out_weight  ln_rweight  rweight  cons  \n",
      "count        1.0    4085.000000         0.0      0.0   0.0  \n",
      "mean        -inf           -inf         NaN      NaN   NaN  \n",
      "std          NaN            NaN         NaN      NaN   NaN  \n",
      "min         -inf           -inf         NaN      NaN   NaN  \n",
      "25%          NaN      -1.499980         NaN      NaN   NaN  \n",
      "50%          NaN      -1.345985         NaN      NaN   NaN  \n",
      "75%          NaN      -0.924995         NaN      NaN   NaN  \n",
      "max         -inf      -0.012387         NaN      NaN   NaN  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "\n",
      "---------------Estimating Demand System---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\wyatt\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:4573: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[248], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m df_model\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf_model.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Estimating Demand System---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m df_results \u001B[38;5;241m=\u001B[39m estimate_model(df_model)\n\u001B[0;32m      7\u001B[0m df_results\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf_results.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Finished---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[77], line 27\u001B[0m, in \u001B[0;36mestimate_model\u001B[1;34m(df_model)\u001B[0m\n\u001B[0;32m     22\u001B[0m idx \u001B[38;5;241m=\u001B[39m df_model\u001B[38;5;241m.\u001B[39mindex\n\u001B[0;32m     23\u001B[0m df_params \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(\n\u001B[0;32m     24\u001B[0m     index\u001B[38;5;241m=\u001B[39midx, \n\u001B[0;32m     25\u001B[0m     columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeta_ln_me\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeta_cons\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeta_b_mkt\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeta_b_smb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeta_b_hml\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (inv_id, date) \u001B[38;5;129;01min\u001B[39;00m idx\u001B[38;5;241m.\u001B[39mto_flat_index():\n\u001B[0;32m     28\u001B[0m     data \u001B[38;5;241m=\u001B[39m df_model\u001B[38;5;241m.\u001B[39mloc[(inv_id, date)]\n\u001B[0;32m     29\u001B[0m     exog \u001B[38;5;241m=\u001B[39m data[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mln_me\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb_mkt\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb_smb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb_hml\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout_weight\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Holding Weights---------------------------\\n')\n",
    "df_model = calc_holding_weights(df_instrument, df_inside_binned, df_holding_factor)\n",
    "df_model.to_csv(os.path.join(output, 'df_model.csv'))\n",
    "\n",
    "print('\\n---------------Estimating Demand System---------------------------\\n')\n",
    "df_results = estimate_model(df_model)\n",
    "df_results.to_csv(os.path.join(output, 'df_results.csv'))\n",
    "\n",
    "print('\\n---------------Finished---------------------------\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:14:57.760948600Z",
     "start_time": "2024-02-26T04:14:57.561507100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                     prc       shrout   b_mkt   b_smb   b_hml\ndate    asset_id                                             \n2012-07 00095710  18.600   54201000.0  0.8011  1.0778  0.5065\n        00103110  46.980    5524000.0  1.0750 -0.4055  1.0752\n        00160010   6.720    3808000.0  0.8258  0.5093  0.5514\n        00215Q10  19.720   81267000.0  0.8311  2.1696 -0.3116\n        00289620  33.800   82567000.0  1.7054 -0.6199 -0.0366\n...                  ...          ...     ...     ...     ...\n2017-12 Y8564M10  20.150   79627000.0  1.1677  0.2662  1.2556\n        Y8564W10   9.320   89127000.0  1.5039  1.3498  1.8666\n        Y8565J10   2.360  410045000.0  2.6957  0.0852  1.3155\n        Y8565N30   1.400  268202000.0  2.1212  0.7123 -0.5588\n        Y8897Y23   0.251   89236000.0  1.2306  1.5504  2.2723\n\n[98923 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>prc</th>\n      <th>shrout</th>\n      <th>b_mkt</th>\n      <th>b_smb</th>\n      <th>b_hml</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th>asset_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2012-07</th>\n      <th>00095710</th>\n      <td>18.600</td>\n      <td>54201000.0</td>\n      <td>0.8011</td>\n      <td>1.0778</td>\n      <td>0.5065</td>\n    </tr>\n    <tr>\n      <th>00103110</th>\n      <td>46.980</td>\n      <td>5524000.0</td>\n      <td>1.0750</td>\n      <td>-0.4055</td>\n      <td>1.0752</td>\n    </tr>\n    <tr>\n      <th>00160010</th>\n      <td>6.720</td>\n      <td>3808000.0</td>\n      <td>0.8258</td>\n      <td>0.5093</td>\n      <td>0.5514</td>\n    </tr>\n    <tr>\n      <th>00215Q10</th>\n      <td>19.720</td>\n      <td>81267000.0</td>\n      <td>0.8311</td>\n      <td>2.1696</td>\n      <td>-0.3116</td>\n    </tr>\n    <tr>\n      <th>00289620</th>\n      <td>33.800</td>\n      <td>82567000.0</td>\n      <td>1.7054</td>\n      <td>-0.6199</td>\n      <td>-0.0366</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2017-12</th>\n      <th>Y8564M10</th>\n      <td>20.150</td>\n      <td>79627000.0</td>\n      <td>1.1677</td>\n      <td>0.2662</td>\n      <td>1.2556</td>\n    </tr>\n    <tr>\n      <th>Y8564W10</th>\n      <td>9.320</td>\n      <td>89127000.0</td>\n      <td>1.5039</td>\n      <td>1.3498</td>\n      <td>1.8666</td>\n    </tr>\n    <tr>\n      <th>Y8565J10</th>\n      <td>2.360</td>\n      <td>410045000.0</td>\n      <td>2.6957</td>\n      <td>0.0852</td>\n      <td>1.3155</td>\n    </tr>\n    <tr>\n      <th>Y8565N30</th>\n      <td>1.400</td>\n      <td>268202000.0</td>\n      <td>2.1212</td>\n      <td>0.7123</td>\n      <td>-0.5588</td>\n    </tr>\n    <tr>\n      <th>Y8897Y23</th>\n      <td>0.251</td>\n      <td>89236000.0</td>\n      <td>1.2306</td>\n      <td>1.5504</td>\n      <td>2.2723</td>\n    </tr>\n  </tbody>\n</table>\n<p>98923 rows  5 columns</p>\n</div>"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_asset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T04:15:15.367184500Z",
     "start_time": "2024-02-26T04:15:15.313546200Z"
    }
   },
   "execution_count": 249
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
