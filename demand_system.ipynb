{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import wrds\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.api import OLS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:13:27.422851300Z",
     "start_time": "2024-03-17T06:13:26.141653800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def request_wrds(path: str,\n",
    "                 start_date: pd.Timestamp,\n",
    "                 end_date: pd.Timestamp) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    with wrds.Connection() as db:\n",
    "        df_s12 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s12\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s12.to_csv(f'{path}s12.csv')\n",
    "\n",
    "        df_s34 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s34\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s34.to_csv(f'{path}s34.csv')\n",
    "\n",
    "        df_security = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM ff.factors_monthly\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_security.to_csv(f'{path}security.csv')\n",
    "\n",
    "        return df_s12, df_s34, df_security\n",
    "\n",
    "\n",
    "def load_wrds(path: str,\n",
    "              start_date: pd.Timestamp,\n",
    "              end_date: pd.Timestamp\n",
    "              ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    try:\n",
    "        df_s12 = pd.read_csv(f'{path}s12.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12(df_s12)\n",
    "\n",
    "        df_s12type5 = pd.read_csv(f'{path}s12type5.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12type5(df_s12type5)\n",
    "\n",
    "        df_s34 = pd.read_csv(f'{path}s34.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s34(df_s34)\n",
    "\n",
    "        df_beta = pd.read_csv(f'{path}beta.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_beta(df_beta)\n",
    "\n",
    "        df_security = pd.read_csv(f'{path}security.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_security(df_security)\n",
    "\n",
    "        return df_s12, df_s12type5, df_s34, df_beta, df_security\n",
    "    except FileNotFoundError:\n",
    "        return request_wrds(path, start_date, end_date)\n",
    "\n",
    "\n",
    "def clean_imports(df_s12,\n",
    "                  df_s12type5,\n",
    "                  df_s34,\n",
    "                  df_beta,\n",
    "                  df_security,\n",
    "                  start_date,\n",
    "                  end_date\n",
    "                  ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    df_s12_clean = clean_s12(df_s12, start_date, end_date)\n",
    "    log_clean_s12(df_s12_clean)\n",
    "\n",
    "    df_s12type5_clean = clean_s12type5(df_s12type5, start_date, end_date)\n",
    "    log_clean_s12type5(df_s12type5_clean)\n",
    "\n",
    "    df_s34_clean = clean_s34(df_s34, start_date, end_date)\n",
    "    log_clean_s34(df_s34_clean)\n",
    "\n",
    "    df_beta_clean = clean_beta(df_beta, start_date, end_date)\n",
    "    log_clean_beta(df_beta_clean)\n",
    "\n",
    "    df_security_clean = clean_security(df_security, start_date, end_date)\n",
    "    log_clean_security(df_security_clean)\n",
    "\n",
    "    return df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean\n",
    "\n",
    "\n",
    "def clean_s12(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'fundno',\n",
    "        'rdate',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['fundno', 'shares', 'cusip'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'fundno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_s12type5(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    return (df\n",
    "            .rename(columns={'fdate': 'date', 'fundno': 'inv_id'})\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .dropna(how='any', subset=['inv_id', 'date'])\n",
    "            .set_index(['inv_id', 'date']))\n",
    "\n",
    "\n",
    "def clean_s34(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'mgrno',\n",
    "        'rdate',\n",
    "        'typecode',\n",
    "        'cusip',\n",
    "        'prc',\n",
    "        'shrout2',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['cusip', 'shares'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'mgrno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)),\n",
    "                backup_holding=lambda x: x['shares'] * x['prc'],\n",
    "                backup_me=lambda x: x['shrout2'] * x['prc'] * 1000,\n",
    "                typecode=lambda x: x['typecode'].fillna(0).astype(int))\n",
    "            .astype({'inv_id': 'str'})\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .drop(columns=['prc', 'shrout2'])\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_beta(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns=[\n",
    "        'PERMNO',\n",
    "        'DATE',\n",
    "        'b_mkt',\n",
    "        'b_smb',\n",
    "        'b_hml'\n",
    "    ]\n",
    "    offset = pd.DateOffset(months=6)\n",
    "    return (df[columns]\n",
    "            .dropna()\n",
    "            .rename(columns={\n",
    "                'DATE': 'date',\n",
    "                'PERMNO':'permno'})\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)) + offset)\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def clean_security(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'LPERMNO',\n",
    "        'cusip',\n",
    "        'datadate',\n",
    "        'prccm',\n",
    "        'cshoq'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['cusip', 'prccm', 'cshoq'])\n",
    "            .rename(columns={\n",
    "                'LPERMNO': 'permno',\n",
    "                'cusip': 'asset_id',\n",
    "                'prccm': 'prc',\n",
    "                'cshoq': 'shrout',\n",
    "                'datadate': 'date'})\n",
    "            .assign(\n",
    "                shrout=lambda x: x['shrout'] * 1000000,\n",
    "                asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]),\n",
    "                date=lambda x: x['date'].apply(fix_date, args=(start_date, end_date)))\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def fix_date(date: str, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.Timestamp:\n",
    "    date_converted = pd.Timestamp(date) - pd.offsets.MonthEnd()\n",
    "    date_filtered = np.NaN if (date_converted < start_date) or (date_converted > end_date) else date_converted\n",
    "    return date_filtered"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:13:27.465614800Z",
     "start_time": "2024-03-17T06:13:27.446694200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Stocks Monthly\n",
    "\n",
    "def merge_assets_factors(df_assets: pd.DataFrame, df_factors: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(left=df_assets,\n",
    "                      right=df_factors,\n",
    "                      how='inner',\n",
    "                      left_index=True,\n",
    "                      right_index=True)\n",
    "    df_merged_indexed = (df_merged\n",
    "                         .reset_index()\n",
    "                         .assign(date=lambda x: x['date'] - pd.offsets.BQuarterEnd())\n",
    "                         .drop_duplicates(subset=['date', 'asset_id'], keep='last')\n",
    "                         .set_index(['date', 'asset_id']))\n",
    "    log_asset_merge(df_merged_indexed)\n",
    "    return df_merged_indexed\n",
    "\n",
    "\n",
    "# Manager / Holdings\n",
    "\n",
    "def match_fund_manager(df_fund: pd.DataFrame, df_manager: pd.DataFrame, df_key: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO\n",
    "    # df_fund_wkey = df_fund.assign(mgrno=lambda x: df_key.loc[x.index.get_level_values(0), 'mgrcocd'])\n",
    "    # df_merged = df_manager.merge(df_fund_wkey, how='outer', on='mgrno')\n",
    "    \n",
    "    df_fund_manager = (df_manager\n",
    "                       .reset_index()\n",
    "                       .assign(date=lambda x: x['date'] - pd.offsets.BQuarterEnd())\n",
    "                       .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "                       .set_index(['inv_id', 'date', 'asset_id']))\n",
    "    log_holding_merge(df_fund_manager)\n",
    "    return df_fund_manager\n",
    "\n",
    "\n",
    "def merge_holding_factor(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_holding,\n",
    "        right=df_asset,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    df_holding_factor = (df_merged\n",
    "                         .assign(\n",
    "                           shares=lambda x: np.minimum(x['shares'], x['shrout']),\n",
    "                           ccm_holding=lambda x: x['prc'] * x['shares'],\n",
    "                           ccm_me=lambda x: x['prc'] * x['shrout'],\n",
    "                           holding=lambda x: x['ccm_holding'].fillna(x['backup_holding']) / 1000000,\n",
    "                           me=lambda x: x['ccm_me'].fillna(x['backup_me']) / 1000000,\n",
    "                           type_code=lambda x: x['typecode'].fillna(0))\n",
    "                         .drop(columns=['ccm_holding', 'backup_holding', 'ccm_me', 'backup_me'])\n",
    "                         .dropna(subset=['holding', 'me'])\n",
    "                         .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    log_holding_factor_merge(df_holding_factor)\n",
    "    return df_holding_factor\n",
    "\n",
    "\n",
    "def create_household_sector(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_holding_factor\n",
    "                      .groupby(['date', 'asset_id'])\n",
    "                      .agg({\n",
    "                        'shares': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'shrout': 'last',\n",
    "                        'b_mkt': 'last',\n",
    "                        'b_smb': 'last',\n",
    "                        'b_hml': 'last',\n",
    "                        'holding': 'sum',\n",
    "                        'me': 'last',\n",
    "                        '_merge': 'last'})\n",
    "                      .assign(\n",
    "                        shares=lambda x: np.maximum(x['shrout'] - x['shares'], 0),\n",
    "                        holding=lambda x: np.maximum(x['me'] - x['holding'], 0),\n",
    "                        inv_id='0',\n",
    "                        typecode=0)\n",
    "                    .set_index('inv_id', append=True)\n",
    "                    .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    log_household_sector(df_household)\n",
    "    df_concat = pd.concat([df_holding_factor, df_household])\n",
    "    return df_concat\n",
    "\n",
    "def create_outside_asset(df_household: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    mask = df_household['_merge'] == 'both'\n",
    "    df_inside = df_household[mask].drop(columns='_merge')\n",
    "    df_outside = (df_household[~mask]\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({\n",
    "                    'typecode': 'last',\n",
    "                    'holding': 'sum'})\n",
    "                  .assign(asset_id='-1')\n",
    "                  .set_index('asset_id', append=True))\n",
    "    log_outside_asset(df_outside)\n",
    "    return df_inside, df_outside"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T07:14:39.904007400Z",
     "start_time": "2024-03-17T07:14:39.776176500Z"
    }
   },
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def calc_inv_aum(df_inside: pd.DataFrame, df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_in_aum = (df_inside\n",
    "                 .groupby(['inv_id', 'date'])\n",
    "                 .agg({\n",
    "                   'holding': 'sum',\n",
    "                   'shares': 'count',\n",
    "                   'typecode': 'last'})\n",
    "                 .rename(columns={\n",
    "                   'holding': 'in_aum',\n",
    "                   'shares': 'n_holding'}))\n",
    "\n",
    "    df_out_aum = (df_outside\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({\n",
    "                    'holding': 'sum',\n",
    "                    'typecode': 'last'})\n",
    "                  .rename(columns={'holding': 'out_aum'}))\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        left=df_in_aum,\n",
    "        right=df_out_aum,\n",
    "        how='outer',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    df_inv_aum = (df_merged\n",
    "                  .assign(\n",
    "                    typecode=lambda x: x['typecode_x'].fillna(x['typecode_y']),\n",
    "                    n_holding=lambda x: x['n_holding'].fillna(0),\n",
    "                    out_aum=lambda x: x['out_aum'].fillna(0),\n",
    "                    in_aum=lambda x: x['in_aum'].fillna(0),\n",
    "                    aum=lambda x: x['out_aum'] + x['in_aum'])\n",
    "                  .drop(columns=['typecode_x', 'typecode_y']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def bin_concentrated_inv(df_inside: pd.DataFrame, df_inv_aum: pd.DataFrame, min_n_holding: int) -> (pd.DataFrame, pd.DataFrame):\n",
    "    household_mask = (df_inv_aum['out_aum'] == 0) | (df_inv_aum['in_aum'] == 0) | (df_inv_aum['aum'] < 10)\n",
    "    df_valid = (df_inv_aum\n",
    "                .reset_index()\n",
    "                .assign(\n",
    "                    inv_id=lambda x: x['inv_id'].mask(household_mask.values, '0'),\n",
    "                    typecode=lambda x: x['typecode'].mask(household_mask.values, '0')))\n",
    "    \n",
    "    diversified_mask = (df_valid['n_holding'] >= min_n_holding) & (df_valid['inv_id'] != '0')\n",
    "    df_aum_diversified = df_valid[diversified_mask]\n",
    "    \n",
    "    \n",
    "    def calc_bin(df_date_type: pd.DataFrame):\n",
    "        typecode = df_date_type['typecode'].iloc[0]\n",
    "        n_bins = np.ceil(df_date_type['n_holding'].sum() / (2 * min_n_holding)).astype(int)\n",
    "        if (n_bins <= 1) or (typecode == 0):\n",
    "            return '0'\n",
    "        else:\n",
    "            return pd.qcut(x=df_date_type['aum'], q=n_bins, labels=False).apply(fix_qcut_bin, args=(str(typecode),))\n",
    "\n",
    "\n",
    "    def fix_qcut_bin(bin: int, typecode: str):\n",
    "        return typecode + ':' + str(bin)\n",
    "    \n",
    "\n",
    "    df_aum_concentrated = df_valid[~diversified_mask].assign(\n",
    "        bin=lambda x: x.groupby(['date', 'typecode']).apply(calc_bin).reset_index(drop=True))\n",
    "    df_concentrated_binned = (df_aum_concentrated\n",
    "                              .reset_index()\n",
    "                              .groupby(['bin', 'date'])\n",
    "                              .agg({\n",
    "                                'in_aum': 'sum',\n",
    "                                'out_aum': 'sum',\n",
    "                                'aum': 'sum',\n",
    "                                'n_holding': 'sum', # incorrect because of double counting\n",
    "                                'typecode': 'last'})\n",
    "                              .rename_axis(index={'bin': 'inv_id'}))\n",
    "    df_aum_binned = (pd.concat([df_aum_diversified, df_concentrated_binned])\n",
    "                     .sort_index()\n",
    "                     .assign(\n",
    "                       out_weight=lambda x: x['out_aum'] / x['aum'],\n",
    "                       in_weight=lambda x: x['in_aum'] / x['aum']))\n",
    "    \n",
    "    df_inside_merged = pd.merge(\n",
    "        left=df_inside,\n",
    "        right=df_aum_concentrated['bin'],\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    df_inside_binned = (df_inside_merged\n",
    "                        .reset_index()\n",
    "                        .assign(bin=lambda x: x['bin'].fillna(x['inv_id']))\n",
    "                        .groupby(['bin', 'date', 'asset_id'])\n",
    "                        .agg({\n",
    "                          'holding': 'sum',\n",
    "                          'prc': 'last',\n",
    "                          'shrout': 'last',\n",
    "                          'me': 'last',\n",
    "                          'b_mkt': 'last',\n",
    "                          'b_smb': 'last',\n",
    "                          'b_hml': 'last'})\n",
    "                        .rename_axis(index={'bin': 'inv_id'}))\n",
    "    \n",
    "    log_bins(df_inside_binned, df_aum_binned)\n",
    "    return df_inside_binned, df_aum_binned\n",
    "\n",
    "\n",
    "def assetid_byinv(df_holding: pd.DataFrame) -> pd.Series:\n",
    "    return df_holding.reset_index('asset_id')['asset_id']\n",
    "\n",
    "\n",
    "def calc_inv_universe(df_holding: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    df_assetid_byinv = assetid_byinv(df_holding).sort_index()\n",
    "    idx_inv_universe = df_assetid_byinv.index.unique()\n",
    "    df_inv_universe = pd.DataFrame(index=idx_inv_universe, columns=['inv_universe'])\n",
    "    offset = pd.DateOffset(months=3 * n_quarters)\n",
    "\n",
    "    def calc_past_quarters(i: int, d: pd.Timestamp) -> np.array:\n",
    "        prev_date = d - offset\n",
    "        asset_id_within_range = df_assetid_byinv.loc[i].loc[prev_date:d]\n",
    "        inv_uni = asset_id_within_range.unique().tolist()\n",
    "        return inv_uni\n",
    "\n",
    "    for (inv_id, date) in idx_inv_universe.to_flat_index():\n",
    "        inv_uni = calc_past_quarters(inv_id, date)\n",
    "        df_inv_universe.loc[(inv_id, date), 'inv_universe'] = inv_uni\n",
    "\n",
    "    df_inv_universe = df_inv_universe.assign(uni_size=lambda x: x['inv_universe'].apply(len))\n",
    "    log_inv_universe(df_inv_universe)\n",
    "    return df_inv_universe\n",
    "\n",
    "\n",
    "def create_equal_allocation(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_alloc = pd.merge(\n",
    "        left=df_inv_universe,\n",
    "        right=df_aum_binned,\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    return (df_equal_alloc\n",
    "            .assign(allocation=lambda x: x['aum'] / (x['uni_size'] + 1))\n",
    "            .explode('inv_universe')\n",
    "            .rename(columns={'inv_universe':'asset_id'})\n",
    "            .set_index('asset_id', append=True))\n",
    "\n",
    "\n",
    "def create_total_allocation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df['allocation']\n",
    "            .groupby('asset_id')\n",
    "            .sum())\n",
    "\n",
    "\n",
    "def create_instrument(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_allocation = create_equal_allocation(df_inv_universe, df_aum_binned)\n",
    "    total_allocation = create_total_allocation(df_equal_allocation)\n",
    "    df_instrument = df_equal_allocation.assign(iv_me=lambda x: total_allocation - x['allocation'])\n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame, df_inside_binned: pd.DataFrame, min_holding: int) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_instrument,\n",
    "        right=df_inside_binned,\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    mask = (df_merged['n_holding'] >= min_holding) & (df_merged['out_weight'] > 0) & (df_merged['in_weight'] > 0) & (df_merged['me'] > 0) & (df_merged['iv_me'] > 0)\n",
    "\n",
    "    df_weights = (df_merged.loc[mask]\n",
    "                 .assign(\n",
    "                   ln_me=lambda x: np.log(x['me']),\n",
    "                   ln_iv_me=lambda x: np.log(x['iv_me']),\n",
    "                   weight=lambda x: x['holding'] / x['aum'],\n",
    "                   ln_weight=lambda x: np.log(x['weight']),\n",
    "                   rweight=lambda x: x['weight'] / x['out_weight'],\n",
    "                   ln_rweight=lambda x: np.log(x['rweight']),\n",
    "                   mean_ln_rweight=lambda x: x['ln_rweight'].groupby(['inv_id', 'date']).transform('mean'),\n",
    "                   const=1)\n",
    "                .reset_index('asset_id'))\n",
    "    \n",
    "    log_holding_weights(df_weights)\n",
    "    return df_weights\n",
    "\n",
    "\n",
    "def unpack_result(result: gmm.GMMResults) -> dict:\n",
    "    return result.params\n",
    "\n",
    "\n",
    "def momcond_1(params, exog):\n",
    "    upper_bound = 0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    mean_ln_rweight = exog[1]\n",
    "    arr_characteristics = exog[2:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = ln_me_term + characteristics_term + mean_ln_rweight\n",
    "    \n",
    "    return pred_weight\n",
    "\n",
    "\n",
    "def momcond_2(params, exog):\n",
    "    upper_bound = 0.9999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    rweight = exog[1]\n",
    "    mean_ln_rweight = exog[2]\n",
    "    arr_characteristics = exog[3:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = np.exp(-1 * (ln_me_term + characteristics_term + mean_ln_rweight))\n",
    "    \n",
    "    return rweight * pred_weight\n",
    "        \n",
    "\n",
    "def estimate_model(df_weights: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    def fit_inv_date_1(df_inv_date: pd.DataFrame) -> gmm.GMMResults:\n",
    "        exog = np.asarray(df_inv_date[['ln_me', 'mean_ln_rweight'] + characteristics])\n",
    "        instrument = np.asarray(df_inv_date[['ln_iv_me', 'mean_ln_rweight'] + characteristics])\n",
    "        n = exog.shape[0]\n",
    "        endog = np.asarray(df_inv_date['ln_rweight'])\n",
    "        start_params = np.zeros(len(params))\n",
    "        w0inv = np.dot(instrument.T, instrument) / n\n",
    "        \n",
    "        try:\n",
    "            model = gmm.NonlinearIVGMM(\n",
    "                endog=endog,\n",
    "                exog=exog,\n",
    "                instrument=instrument, \n",
    "                func=momcond_1)\n",
    "            result = model.fit(\n",
    "                start_params=start_params,\n",
    "                maxiter=0,\n",
    "                inv_weights=w0inv)\n",
    "            log_results(result, params)\n",
    "            return result\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Linear Algebra Error')\n",
    "            print(f'Investor Id:  {df_inv_date.index.get_level_values(0).unique()}')\n",
    "            print(f'Date:  {df_inv_date.index.get_level_values(1).unique()}')\n",
    "            return np.NaN\n",
    "    \n",
    "    \n",
    "    def fit_inv_date_2(df_inv_date: pd.DataFrame) -> gmm.GMMResults:\n",
    "        exog = np.asarray(df_inv_date[['ln_me', 'rweight', 'mean_ln_rweight'] + characteristics], dtype=np.longdouble)\n",
    "        instrument = np.asarray(df_inv_date[['ln_iv_me', 'rweight', 'mean_ln_rweight'] + characteristics], dtype=np.longdouble)\n",
    "        n = exog.shape[0]\n",
    "        endog = np.ones(n, dtype=np.longdouble)\n",
    "        \n",
    "        try:\n",
    "            model = gmm.NonlinearIVGMM(\n",
    "                endog=endog,\n",
    "                exog=exog,\n",
    "                instrument=instrument, \n",
    "                func=momcond_1)\n",
    "            w0inv = np.dot(instrument.T, instrument) / n\n",
    "            start_params = np.zeros(len(params))\n",
    "            result = model.fit(\n",
    "                start_params=start_params,\n",
    "                maxiter=0,\n",
    "                inv_weights=w0inv)\n",
    "            log_results(result, params)\n",
    "            return result\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular Matrix Error')\n",
    "            print(f'Investor Id:  {df_inv_date.index.get_level_values(0).unique()}')\n",
    "            print(f'Date:  {df_inv_date.index.get_level_values(1).unique()}')\n",
    "            return np.NaN\n",
    "    \n",
    "    \n",
    "    df_model = df_weights.assign(\n",
    "        gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(fit_inv_date_1),\n",
    "        lst_params=lambda x: x['gmm_result'].apply(unpack_result))\n",
    "    df_model[params] = pd.DataFrame(df_model['lst_params'].tolist(), index=df_model.index)\n",
    "    df_model = df_model.drop(columns='lst_params')\n",
    "    \n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def calc_latent_demand(df_model: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    upper_bound = 0.9999\n",
    "    \n",
    "    df_results = df_model.assign(\n",
    "        beta_ln_me=lambda x: upper_bound - np.exp(-1 * x['beta_ln_me']),\n",
    "        char_demand=lambda x: np.einsum('ij,ij->i', x[['ln_me'] + characteristics], x[params]),\n",
    "        pred_rweight=lambda x: np.exp(-1 * (x['char_demand'] + x['mean_ln_rweight'])),\n",
    "        latent_demand=lambda x: x['rweight'] * x['pred_rweight'])\n",
    "    log_latent_demand(df_results)\n",
    "    return df_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:13:27.531426300Z",
     "start_time": "2024-03-17T06:13:27.504944400Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Figures\n",
    "\n",
    "def check_moment_condition(df_results: pd.DataFrame):\n",
    "    df_mom = (df_results\n",
    "              .groupby(['inv_id', 'date'])\n",
    "              .agg({\n",
    "                'latent_demand': 'mean',\n",
    "                'n_holding': 'last',\n",
    "                'uni_size': 'last'})\n",
    "              .assign(\n",
    "                iweight=lambda x: x['n_holding'] / x['uni_size'],\n",
    "                latent_demand=lambda x: x['latent_demand'] * x['iweight']))\n",
    "    \n",
    "    epsilon = .0001\n",
    "    mask = (df_mom['latent_demand'] > 1 - epsilon) & (df_mom['latent_demand'] < 1 + epsilon)\n",
    "    valid_rate = len(df_mom[mask]) / len(df_mom)\n",
    "    print(f'Percentage of valid portfolios:  {100*valid_rate:.4f}')\n",
    "    return df_mom\n",
    "\n",
    "\n",
    "def critical_value_test(df_results: pd.DataFrame, characteristics: list, figure_path: str):\n",
    "    \n",
    "    def iv_reg(df_inv_date: pd.DataFrame):\n",
    "        y = df_inv_date['ln_me']\n",
    "        X = df_inv_date[['ln_iv_me'] + characteristics]\n",
    "        model = OLS(y, X)\n",
    "        result = model.fit()\n",
    "        t_stat = result.tvalues.iloc[0]\n",
    "        return t_stat\n",
    "    \n",
    "    df_iv = (df_results\n",
    "             .groupby(['inv_id', 'date'])\n",
    "             .apply(iv_reg)\n",
    "             .to_frame('t_stat')\n",
    "             .groupby('date')\n",
    "             .min()\n",
    "             .reset_index())\n",
    "    # df_iv['date'] = df_iv['date'].dt.to_timestamp()\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data=df_iv, \n",
    "        x='date', \n",
    "        y='t_stat', \n",
    "        kind='line')\n",
    "    g.refline(\n",
    "        y=4.05,\n",
    "        linestyle='--')\n",
    "    g.set_axis_labels('Date', 'First stage t-statistic')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'instrument_validity.png'))\n",
    "\n",
    "\n",
    "def test_index_fund(df_results: pd.DataFrame, characteristics: list, params: list, figure_path: str):\n",
    "    index_id = '90457'\n",
    "    mask = df_results.index.get_level_values(0) == index_id\n",
    "    df_index_fund = (df_results\n",
    "                     .loc[mask]\n",
    "                     .assign(\n",
    "                        ln_rweight=lambda x: x['ln_me'] + x['mean_ln_rweight']))\n",
    "    \n",
    "    df_index_fund_model = estimate_model(df_index_fund, characteristics, params)\n",
    "    df_index_fund_result = calc_latent_demand(df_index_fund_model, characteristics, params)\n",
    "    # df_index_fund_result['date'] = df_index_fund_result['date'].dt.to_timestamp()\n",
    "    \n",
    "    cols = params + ['latent_demand']\n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_index_fund_result, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'index_fund_{param}.png'))\n",
    "    \n",
    "\n",
    "def graph_type_params(df_results: pd.DataFrame, params: list, figure_path: str):\n",
    "    cols = params + ['latent_demand']\n",
    "\n",
    "    df_types = df_results.copy()\n",
    "    df_types[cols] = df_types[cols].apply(lambda x: x * df_results['aum'])\n",
    "    df_types = (df_types\n",
    "                .groupby(['typecode', 'date'])\n",
    "                [cols]\n",
    "                .sum()\n",
    "                .apply(lambda x: x / df_results.groupby(['typecode', 'date'])['aum'].sum()))\n",
    "    # df_types['date'] = df_types['date'].dt.to_timestamp()\n",
    "    \n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_types, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        hue='typecode',\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'mean_{get_readable_param(param)}')\n",
    "        g.legend.set_title('Institution Type')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'{param}.png'))\n",
    "\n",
    "\n",
    "def graph_std_latent_demand(df_results: pd.DataFrame, figure_path: str):\n",
    "    df_ld = (df_results\n",
    "         .groupby(['inv_id', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'std',\n",
    "            'aum': 'last',\n",
    "            'typecode': 'last'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] * x['aum'])\n",
    "         .groupby(['typecode', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'mean',\n",
    "            'aum': 'sum'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] / x['aum']))\n",
    "    # df_ld['date'] = df_ld['date'].dt.to_timestamp()\n",
    "    \n",
    "    g = sns.relplot(data=df_ld, \n",
    "                    x='date',\n",
    "                    y='latent_demand',\n",
    "                    hue='typecode',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Standard Deviation of Latent Demand')\n",
    "    g.legend.set_title('Institution Type')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'std_latent_demand.png'))\n",
    "\n",
    "\n",
    "def create_tables(df_instrument: pd.DataFrame, df_inv_uni: pd.DataFrame):\n",
    "    df_pctile = df_inv_uni.assign(pctile=0)\n",
    "    arr_dates = df_pctile.index.get_level_values('date').unique()\n",
    "    \n",
    "    for date in arr_dates:\n",
    "        data = df_pctile.loc[date, 'aum']\n",
    "        df_pctile.loc[date, 'pctile'] = pd.qcut(data, q=100)\n",
    "        \n",
    "    df_grouped_pctile = (df_pctile['uni_persistence']\n",
    "                         .groupby(['pctile'])\n",
    "                         .median())\n",
    "    print(df_grouped_pctile.head(10))\n",
    "\n",
    "    \n",
    "def get_param_cols(cols: list) -> list:\n",
    "    return ['beta_' + col for col in cols]\n",
    "\n",
    "\n",
    "def get_readable_param(name: str) -> str:\n",
    "    return name.replace('_', ' ').title()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:13:27.552710300Z",
     "start_time": "2024-03-17T06:13:27.515010400Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_import_s12(df_s12: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12['rdate']\n",
    "    print('Imported s12')\n",
    "    print('Number of holdings:  ', len(df_s12))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_import_s12type5(df_s12type5: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5['fdate']\n",
    "    print()\n",
    "    print('Imported s12type5')\n",
    "    print('Number of holdings:  ', len(df_s12type5))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_import_s34(df_s34: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34['rdate']\n",
    "    print()\n",
    "    print('Imported s34')\n",
    "    print('Number of holdings:  ', len(df_s34))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_import_beta(df_beta: pd.DataFrame):\n",
    "    dateindex_ffm = df_beta['DATE']\n",
    "    print()\n",
    "    print('Imported betas')\n",
    "    print('Number of dates:  ', len(df_beta))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_import_security(df_security: pd.DataFrame):\n",
    "    dateindex_security = df_security['datadate']\n",
    "    print()\n",
    "    print('Imported security')\n",
    "    print('Number of holdings:  ', len(df_security))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_clean_s12(df_s12_clean: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12_clean.index.get_level_values('date')\n",
    "    print('Cleaned s12')\n",
    "    print('Number of holdings:  ', len(df_s12_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_clean_s12type5(df_s12type5_clean: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s12type5')\n",
    "    print('Number of firm/dates:  ', len(df_s12type5_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_clean_s34(df_s34_clean: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s34')\n",
    "    print('Number of holdings:  ', len(df_s34_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_clean_beta(df_beta_clean: pd.DataFrame):\n",
    "    dateindex_beta = df_beta_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned beta')\n",
    "    print('Number of dates:  ', len(df_beta_clean))\n",
    "    print('Earliest date:  ', min(dateindex_beta))\n",
    "    print('Latest date:  ', max(dateindex_beta))\n",
    "\n",
    "\n",
    "def log_clean_security(df_security_clean: pd.DataFrame):\n",
    "    dateindex_security = df_security_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned security')\n",
    "    print('Number of asset/dates:  ', len(df_security_clean))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_holding_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged s12 and s34')\n",
    "    print('Number of holdings:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_asset_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged assets and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_holding: pd.DataFrame):\n",
    "    print('Created outside asset')\n",
    "    print('Number of holdings:  ', len(df_holding))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    print('Calculated investor AUM')\n",
    "    print(df_inv_aum.describe())\n",
    "\n",
    "\n",
    "def log_bins(df_inside_binned: pd.DataFrame, df_aum_binned: pd.DataFrame):\n",
    "    print('Binned investors')\n",
    "    print('Number of investors by aum:  ', len(df_inside_binned.index.unique('inv_id')))\n",
    "    print('Number of investors by holding:  ', len(df_aum_binned.index.unique('inv_id')))\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    print('Created investment universe')\n",
    "    print(df_inv_uni.describe())\n",
    "    \n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print('Created market equity instrument')\n",
    "    print(df_instrument.describe())\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print('Calculated holding weights')\n",
    "    print(df_model.describe())\n",
    "\n",
    "\n",
    "def log_results(result, params):\n",
    "    print(result.summary(yname='Latent demand', xname=params))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_params(df_params: pd.DataFrame):\n",
    "    print('Estimated parameters')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def log_latent_demand(df_results: pd.DataFrame):\n",
    "    print('Calculated latent demand')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:13:27.620663800Z",
     "start_time": "2024-03-17T06:13:27.547080700Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "sns.set_theme(style='ticks', palette=None)\n",
    "\n",
    "input_path = 'data/'\n",
    "output_path = 'output/'\n",
    "figure_path = 'figures/'\n",
    "\n",
    "start_date = pd.Timestamp('2012-01')\n",
    "end_date = pd.Timestamp('2017-12')\n",
    "\n",
    "characteristics = [\n",
    "    'b_mkt',\n",
    "    'b_smb',\n",
    "    'b_hml'\n",
    "] + ['const']\n",
    "params = ['beta_ln_me'] + get_param_cols(characteristics)\n",
    "dict_typecode = {\n",
    "     0: 'Households',\n",
    "     1: 'Banks',\n",
    "     2: 'Insurance companies',\n",
    "     3: 'Investment advisors',\n",
    "     4: 'Mutual funds',\n",
    "     5: 'Pension funds',\n",
    "}\n",
    "\n",
    "min_n_holding = 1000\n",
    "n_quarters = 11"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:13:27.622816300Z",
     "start_time": "2024-03-17T06:13:27.559521300Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Imports---------------------------\n",
      "\n",
      "Imported s12\n",
      "Number of holdings:   30987\n",
      "Earliest date:   2014-06-30\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported s12type5\n",
      "Number of holdings:   572518\n",
      "Earliest date:   12/31/1994\n",
      "Latest date:   9/30/2022\n",
      "\n",
      "Imported s34\n",
      "Number of holdings:   22707709\n",
      "Earliest date:   2012-03-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported betas\n",
      "Number of dates:   432458\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-29\n",
      "\n",
      "Imported security\n",
      "Number of holdings:   402452\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "---------------Starting Cleaning---------------------------\n",
      "Cleaned s12\n",
      "Number of holdings:   27327\n",
      "Earliest date:   2014-05-31 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n",
      "\n",
      "Cleaned s12type5\n",
      "Number of firm/dates:   49372\n",
      "Earliest date:   2012-02-29 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n",
      "\n",
      "Cleaned s34\n",
      "Number of holdings:   22267070\n",
      "Earliest date:   2012-02-29 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n",
      "\n",
      "Cleaned beta\n",
      "Number of dates:   432414\n",
      "Earliest date:   2012-07-31 00:00:00\n",
      "Latest date:   2018-05-30 00:00:00\n",
      "\n",
      "Cleaned security\n",
      "Number of asset/dates:   126399\n",
      "Earliest date:   2012-01-31 00:00:00\n",
      "Latest date:   2017-11-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "print('\\n---------------Starting Imports---------------------------\\n')\n",
    "dfs = load_wrds(input_path, start_date, end_date)\n",
    "\n",
    "print('\\n---------------Starting Cleaning---------------------------\\n')\n",
    "df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean = clean_imports(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")\n",
    "\n",
    "df_s12_clean.to_csv(os.path.join(output_path, 'df_s12_clean.csv'))\n",
    "df_s12type5_clean.to_csv(os.path.join(output_path, 'df_s12type5_clean.csv'))\n",
    "df_s34_clean.to_csv(os.path.join(output_path, 'df_s34_clean.csv'))\n",
    "df_beta_clean.to_csv(os.path.join(output_path, 'df_beta_clean.csv'))\n",
    "df_security_clean.to_csv(os.path.join(output_path, 'df_security_clean.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:23:14.192141600Z",
     "start_time": "2024-03-17T06:13:27.570434400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Assets/Factors---------------------------\n",
      "\n",
      "Merged assets and factors\n",
      "Number of assets/dates:   50643\n",
      "\n",
      "---------------Merging s12/s34 Holdings---------------------------\n",
      "Merged s12 and s34\n",
      "Number of holdings:   22267070\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Assets/Factors---------------------------\\n')\n",
    "df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "df_asset.to_csv(os.path.join(output_path, 'df_asset.csv'))\n",
    "\n",
    "print('\\n---------------Merging s12/s34 Holdings---------------------------\\n')\n",
    "df_fund_manager = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "df_fund_manager.to_csv(os.path.join(output_path, 'df_fund_manager.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:24:53.669414200Z",
     "start_time": "2024-03-17T06:23:14.193267100Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Holdings/Factors---------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Merging Holdings/Factors---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m df_holding_factor \u001B[38;5;241m=\u001B[39m merge_holding_factor(df_fund_manager, df_asset)\n\u001B[0;32m      3\u001B[0m df_holding_factor\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf_holding_factor.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Creating Household Sector---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[55], line 35\u001B[0m, in \u001B[0;36mmerge_holding_factor\u001B[1;34m(df_holding, df_asset)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmerge_holding_factor\u001B[39m(df_holding: pd\u001B[38;5;241m.\u001B[39mDataFrame, df_asset: pd\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m---> 35\u001B[0m     df_merged \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[0;32m     36\u001B[0m         left\u001B[38;5;241m=\u001B[39mdf_holding,\n\u001B[0;32m     37\u001B[0m         right\u001B[38;5;241m=\u001B[39mdf_asset,\n\u001B[0;32m     38\u001B[0m         how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     39\u001B[0m         left_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     40\u001B[0m         right_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     41\u001B[0m         indicator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     42\u001B[0m     )\n\u001B[0;32m     44\u001B[0m     df_holding_factor \u001B[38;5;241m=\u001B[39m (df_merged\n\u001B[0;32m     45\u001B[0m                          \u001B[38;5;241m.\u001B[39massign(\n\u001B[0;32m     46\u001B[0m                            shares\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: np\u001B[38;5;241m.\u001B[39mminimum(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshares\u001B[39m\u001B[38;5;124m'\u001B[39m], x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshrout\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m                          \u001B[38;5;241m.\u001B[39mdropna(subset\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mholding\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mme\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     54\u001B[0m                          \u001B[38;5;241m.\u001B[39mreorder_levels([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124masset_id\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[0;32m     55\u001B[0m     log_holding_factor_merge(df_holding_factor)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001B[0m, in \u001B[0;36mmerge\u001B[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    169\u001B[0m     op \u001B[38;5;241m=\u001B[39m _MergeOperation(\n\u001B[0;32m    170\u001B[0m         left_df,\n\u001B[0;32m    171\u001B[0m         right_df,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    181\u001B[0m         validate\u001B[38;5;241m=\u001B[39mvalidate,\n\u001B[0;32m    182\u001B[0m     )\n\u001B[1;32m--> 183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result(copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:881\u001B[0m, in \u001B[0;36m_MergeOperation.get_result\u001B[1;34m(self, copy)\u001B[0m\n\u001B[0;32m    879\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_result\u001B[39m(\u001B[38;5;28mself\u001B[39m, copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindicator:\n\u001B[1;32m--> 881\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indicator_pre_merge(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright)\n\u001B[0;32m    883\u001B[0m     join_index, left_indexer, right_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_join_info()\n\u001B[0;32m    885\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_and_concat(\n\u001B[0;32m    886\u001B[0m         join_index, left_indexer, right_indexer, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    887\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:928\u001B[0m, in \u001B[0;36m_MergeOperation._indicator_pre_merge\u001B[1;34m(self, left, right)\u001B[0m\n\u001B[0;32m    923\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indicator_name \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[0;32m    924\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    925\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot use name of an existing column for indicator column\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    926\u001B[0m     )\n\u001B[1;32m--> 928\u001B[0m left \u001B[38;5;241m=\u001B[39m left\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    929\u001B[0m right \u001B[38;5;241m=\u001B[39m right\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    931\u001B[0m left[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_left_indicator\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6685\u001B[0m, in \u001B[0;36mNDFrame.copy\u001B[1;34m(self, deep)\u001B[0m\n\u001B[0;32m   6553\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   6554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcopy\u001B[39m(\u001B[38;5;28mself\u001B[39m, deep: bool_t \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Self:\n\u001B[0;32m   6555\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   6556\u001B[0m \u001B[38;5;124;03m    Make a copy of this object's indices and data.\u001B[39;00m\n\u001B[0;32m   6557\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   6683\u001B[0m \u001B[38;5;124;03m    dtype: int64\u001B[39;00m\n\u001B[0;32m   6684\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 6685\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39mdeep)\n\u001B[0;32m   6686\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clear_item_cache()\n\u001B[0;32m   6687\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor_from_mgr(data, axes\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;241m.\u001B[39maxes)\u001B[38;5;241m.\u001B[39m__finalize__(\n\u001B[0;32m   6688\u001B[0m         \u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcopy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   6689\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:576\u001B[0m, in \u001B[0;36mBaseBlockManager.copy\u001B[1;34m(self, deep)\u001B[0m\n\u001B[0;32m    573\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    574\u001B[0m         new_axes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes)\n\u001B[1;32m--> 576\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcopy\u001B[39m\u001B[38;5;124m\"\u001B[39m, deep\u001B[38;5;241m=\u001B[39mdeep)\n\u001B[0;32m    577\u001B[0m res\u001B[38;5;241m.\u001B[39maxes \u001B[38;5;241m=\u001B[39m new_axes\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;66;03m# Avoid needing to re-compute these\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001B[0m, in \u001B[0;36mBaseBlockManager.apply\u001B[1;34m(self, f, align_keys, **kwargs)\u001B[0m\n\u001B[0;32m    352\u001B[0m         applied \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mapply(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    353\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 354\u001B[0m         applied \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(b, f)(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    355\u001B[0m     result_blocks \u001B[38;5;241m=\u001B[39m extend_blocks(applied, result_blocks)\n\u001B[0;32m    357\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfrom_blocks(result_blocks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:645\u001B[0m, in \u001B[0;36mBlock.copy\u001B[1;34m(self, deep)\u001B[0m\n\u001B[0;32m    643\u001B[0m refs: BlockValuesRefs \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m deep:\n\u001B[1;32m--> 645\u001B[0m     values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    646\u001B[0m     refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    647\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Holdings/Factors---------------------------\\n')\n",
    "df_holding_factor = merge_holding_factor(df_fund_manager, df_asset)\n",
    "df_holding_factor.to_csv(os.path.join(output_path, 'df_holding_factor.csv'))\n",
    "\n",
    "print('\\n---------------Creating Household Sector---------------------------\\n')\n",
    "df_household = create_household_sector(df_holding_factor)\n",
    "df_household.to_csv(os.path.join(output_path, 'df_household.csv'))\n",
    "\n",
    "print('\\n---------------Partitioning Outside Asset---------------------------\\n')\n",
    "df_inside, df_outside = create_outside_asset(df_household)\n",
    "df_inside.to_csv(os.path.join(output_path, 'df_inside.csv'))\n",
    "df_outside.to_csv(os.path.join(output_path, 'df_outside.csv'))\n",
    "\n",
    "print('\\n---------------Calculating Investor AUM---------------------------\\n')\n",
    "df_inv_aum = calc_inv_aum(df_inside, df_outside)\n",
    "df_inv_aum.to_csv(os.path.join(output_path, 'df_inv_aum.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T07:48:25.038044100Z",
     "start_time": "2024-03-17T07:48:21.000887400Z"
    }
   },
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Pooling Investors By Type/Size---------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[89], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Pooling Investors By Type/Size---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m df_holding_binned, df_aum_binned \u001B[38;5;241m=\u001B[39m bin_concentrated_inv(df_inside, df_inv_aum, min_n_holding)\n\u001B[0;32m      3\u001B[0m df_holding_binned\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf_holding_binned.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      4\u001B[0m df_aum_binned\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf_aum_binned.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "Cell \u001B[1;32mIn[87], line 69\u001B[0m, in \u001B[0;36mbin_concentrated_inv\u001B[1;34m(df_inside, df_inv_aum, min_n_holding)\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m typecode \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mbin\u001B[39m)\n\u001B[0;32m     64\u001B[0m df_aum_concentrated \u001B[38;5;241m=\u001B[39m df_valid[\u001B[38;5;241m~\u001B[39mdiversified_mask]\u001B[38;5;241m.\u001B[39massign(\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mbin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtypecode\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mapply(calc_bin)\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m     66\u001B[0m df_concentrated_binned \u001B[38;5;241m=\u001B[39m (df_aum_concentrated\n\u001B[0;32m     67\u001B[0m                           \u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[0;32m     68\u001B[0m                           \u001B[38;5;241m.\u001B[39mgroupby([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 69\u001B[0m                           \u001B[38;5;241m.\u001B[39magg({\n\u001B[0;32m     70\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124min_aum\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     71\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout_aum\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     72\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maum\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     73\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_holding\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;66;03m# incorrect because of double counting\u001B[39;00m\n\u001B[0;32m     74\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtypecode\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlast\u001B[39m\u001B[38;5;124m'\u001B[39m})\n\u001B[0;32m     75\u001B[0m                           \u001B[38;5;241m.\u001B[39mrename_axis(index\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m}))\n\u001B[0;32m     76\u001B[0m df_aum_binned \u001B[38;5;241m=\u001B[39m (pd\u001B[38;5;241m.\u001B[39mconcat([df_aum_diversified, df_concentrated_binned])\n\u001B[0;32m     77\u001B[0m                  \u001B[38;5;241m.\u001B[39msort_index()\n\u001B[0;32m     78\u001B[0m                  \u001B[38;5;241m.\u001B[39massign(\n\u001B[0;32m     79\u001B[0m                    out_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mout_aum\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maum\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     80\u001B[0m                    in_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124min_aum\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maum\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[0;32m     82\u001B[0m df_inside_merged \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[0;32m     83\u001B[0m     left\u001B[38;5;241m=\u001B[39mdf_inside,\n\u001B[0;32m     84\u001B[0m     right\u001B[38;5;241m=\u001B[39mdf_aum_concentrated[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbin\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     85\u001B[0m     how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     86\u001B[0m     left_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     87\u001B[0m     right_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1445\u001B[0m, in \u001B[0;36mDataFrameGroupBy.aggregate\u001B[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1442\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mengine_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m engine_kwargs\n\u001B[0;32m   1444\u001B[0m op \u001B[38;5;241m=\u001B[39m GroupByApply(\u001B[38;5;28mself\u001B[39m, func, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m-> 1445\u001B[0m result \u001B[38;5;241m=\u001B[39m op\u001B[38;5;241m.\u001B[39magg()\n\u001B[0;32m   1446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dict_like(func) \u001B[38;5;129;01mand\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1447\u001B[0m     \u001B[38;5;66;03m# GH #52849\u001B[39;00m\n\u001B[0;32m   1448\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mas_index \u001B[38;5;129;01mand\u001B[39;00m is_list_like(func):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:175\u001B[0m, in \u001B[0;36mApply.agg\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_dict_like(func):\n\u001B[1;32m--> 175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg_dict_like()\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(func):\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;66;03m# we require a list, but not a 'str'\u001B[39;00m\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg_list_like()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:406\u001B[0m, in \u001B[0;36mApply.agg_dict_like\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21magg_dict_like\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m    399\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001B[39;00m\n\u001B[0;32m    401\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;124;03m    Result of aggregation.\u001B[39;00m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 406\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg_or_apply_dict_like(op_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124magg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1390\u001B[0m, in \u001B[0;36mGroupByApply.agg_or_apply_dict_like\u001B[1;34m(self, op_name)\u001B[0m\n\u001B[0;32m   1385\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mengine\u001B[39m\u001B[38;5;124m\"\u001B[39m: engine, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mengine_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: engine_kwargs})\n\u001B[0;32m   1387\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m com\u001B[38;5;241m.\u001B[39mtemp_setattr(\n\u001B[0;32m   1388\u001B[0m     obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas_index\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m, condition\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas_index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1389\u001B[0m ):\n\u001B[1;32m-> 1390\u001B[0m     result_index, result_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dict_like(\n\u001B[0;32m   1391\u001B[0m         op_name, selected_obj, selection, kwargs\n\u001B[0;32m   1392\u001B[0m     )\n\u001B[0;32m   1393\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001B[0;32m   1394\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:479\u001B[0m, in \u001B[0;36mApply.compute_dict_like\u001B[1;34m(self, op_name, selected_obj, selection, kwargs)\u001B[0m\n\u001B[0;32m    476\u001B[0m         results \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m key_data\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;66;03m# key used for column selection and output\u001B[39;00m\n\u001B[1;32m--> 479\u001B[0m     results \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(obj\u001B[38;5;241m.\u001B[39m_gotitem(key, ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), op_name)(how, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m key, how \u001B[38;5;129;01min\u001B[39;00m func\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    482\u001B[0m     ]\n\u001B[0;32m    483\u001B[0m     keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(func\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keys, results\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:480\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    476\u001B[0m         results \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m key_data\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;66;03m# key used for column selection and output\u001B[39;00m\n\u001B[0;32m    479\u001B[0m     results \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 480\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(obj\u001B[38;5;241m.\u001B[39m_gotitem(key, ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), op_name)(how, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    481\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m key, how \u001B[38;5;129;01min\u001B[39;00m func\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    482\u001B[0m     ]\n\u001B[0;32m    483\u001B[0m     keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(func\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keys, results\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:247\u001B[0m, in \u001B[0;36mSeriesGroupBy.aggregate\u001B[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m engine_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    246\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mengine_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m engine_kwargs\n\u001B[1;32m--> 247\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, func)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func, abc\u001B[38;5;241m.\u001B[39mIterable):\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;66;03m# Catch instances of lists / tuples\u001B[39;00m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;66;03m# but not the class list / tuple itself.\u001B[39;00m\n\u001B[0;32m    252\u001B[0m     func \u001B[38;5;241m=\u001B[39m maybe_mangle_lambdas(func)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:3063\u001B[0m, in \u001B[0;36mGroupBy.sum\u001B[1;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001B[0m\n\u001B[0;32m   3058\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3059\u001B[0m     \u001B[38;5;66;03m# If we are grouping on categoricals we want unobserved categories to\u001B[39;00m\n\u001B[0;32m   3060\u001B[0m     \u001B[38;5;66;03m# return zero, rather than the default of NaN which the reindexing in\u001B[39;00m\n\u001B[0;32m   3061\u001B[0m     \u001B[38;5;66;03m# _agg_general() returns. GH #31422\u001B[39;00m\n\u001B[0;32m   3062\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m com\u001B[38;5;241m.\u001B[39mtemp_setattr(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobserved\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m-> 3063\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_agg_general(\n\u001B[0;32m   3064\u001B[0m             numeric_only\u001B[38;5;241m=\u001B[39mnumeric_only,\n\u001B[0;32m   3065\u001B[0m             min_count\u001B[38;5;241m=\u001B[39mmin_count,\n\u001B[0;32m   3066\u001B[0m             alias\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3067\u001B[0m             npfunc\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39msum,\n\u001B[0;32m   3068\u001B[0m         )\n\u001B[0;32m   3070\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_output(result, fill_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1839\u001B[0m, in \u001B[0;36mGroupBy._agg_general\u001B[1;34m(self, numeric_only, min_count, alias, npfunc)\u001B[0m\n\u001B[0;32m   1830\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   1831\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_agg_general\u001B[39m(\n\u001B[0;32m   1832\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1837\u001B[0m     npfunc: Callable,\n\u001B[0;32m   1838\u001B[0m ):\n\u001B[1;32m-> 1839\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cython_agg_general(\n\u001B[0;32m   1840\u001B[0m         how\u001B[38;5;241m=\u001B[39malias,\n\u001B[0;32m   1841\u001B[0m         alt\u001B[38;5;241m=\u001B[39mnpfunc,\n\u001B[0;32m   1842\u001B[0m         numeric_only\u001B[38;5;241m=\u001B[39mnumeric_only,\n\u001B[0;32m   1843\u001B[0m         min_count\u001B[38;5;241m=\u001B[39mmin_count,\n\u001B[0;32m   1844\u001B[0m     )\n\u001B[0;32m   1845\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroupby\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1929\u001B[0m, in \u001B[0;36mGroupBy._cython_agg_general\u001B[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001B[0m\n\u001B[0;32m   1926\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_agg_py_fallback(how, values, ndim\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;241m.\u001B[39mndim, alt\u001B[38;5;241m=\u001B[39malt)\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m-> 1929\u001B[0m new_mgr \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mgrouped_reduce(array_func)\n\u001B[0;32m   1930\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_agged_manager(new_mgr)\n\u001B[0;32m   1931\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_aggregated_output(res)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\base.py:336\u001B[0m, in \u001B[0;36mSingleDataManager.grouped_reduce\u001B[1;34m(self, func)\u001B[0m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrouped_reduce\u001B[39m(\u001B[38;5;28mself\u001B[39m, func):\n\u001B[0;32m    335\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marray\n\u001B[1;32m--> 336\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(arr)\n\u001B[0;32m    337\u001B[0m     index \u001B[38;5;241m=\u001B[39m default_index(\u001B[38;5;28mlen\u001B[39m(res))\n\u001B[0;32m    339\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfrom_array(res, index)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1905\u001B[0m, in \u001B[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001B[1;34m(values)\u001B[0m\n\u001B[0;32m   1903\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21marray_func\u001B[39m(values: ArrayLike) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ArrayLike:\n\u001B[0;32m   1904\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1905\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrouper\u001B[38;5;241m.\u001B[39m_cython_operation(\n\u001B[0;32m   1906\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maggregate\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1907\u001B[0m             values,\n\u001B[0;32m   1908\u001B[0m             how,\n\u001B[0;32m   1909\u001B[0m             axis\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1910\u001B[0m             min_count\u001B[38;5;241m=\u001B[39mmin_count,\n\u001B[0;32m   1911\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1912\u001B[0m         )\n\u001B[0;32m   1913\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m:\n\u001B[0;32m   1914\u001B[0m         \u001B[38;5;66;03m# generally if we have numeric_only=False\u001B[39;00m\n\u001B[0;32m   1915\u001B[0m         \u001B[38;5;66;03m# and non-applicable functions\u001B[39;00m\n\u001B[0;32m   1916\u001B[0m         \u001B[38;5;66;03m# try to python agg\u001B[39;00m\n\u001B[0;32m   1917\u001B[0m         \u001B[38;5;66;03m# TODO: shouldn't min_count matter?\u001B[39;00m\n\u001B[0;32m   1918\u001B[0m         \u001B[38;5;66;03m# TODO: avoid special casing SparseArray here\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(values, SparseArray):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:812\u001B[0m, in \u001B[0;36mBaseGrouper._cython_operation\u001B[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001B[0m\n\u001B[0;32m    807\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;124;03mReturns the values of a cython operation.\u001B[39;00m\n\u001B[0;32m    809\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    810\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m kind \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maggregate\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 812\u001B[0m cy_op \u001B[38;5;241m=\u001B[39m WrappedCythonOp(kind\u001B[38;5;241m=\u001B[39mkind, how\u001B[38;5;241m=\u001B[39mhow, has_dropped_na\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_dropped_na)\n\u001B[0;32m    814\u001B[0m ids, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_info\n\u001B[0;32m    815\u001B[0m ngroups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mngroups\n",
      "File \u001B[1;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:726\u001B[0m, in \u001B[0;36mBaseGrouper.has_dropped_na\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    720\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    721\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[0;32m    722\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhas_dropped_na\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m    723\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;124;03m    Whether grouper has null value(s) that are dropped.\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 726\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_info[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39many())\n",
      "File \u001B[1;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:730\u001B[0m, in \u001B[0;36mBaseGrouper.group_info\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[0;32m    729\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgroup_info\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39mintp], npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39mintp], \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m--> 730\u001B[0m     comp_ids, obs_group_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_compressed_codes()\n\u001B[0;32m    732\u001B[0m     ngroups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(obs_group_ids)\n\u001B[0;32m    733\u001B[0m     comp_ids \u001B[38;5;241m=\u001B[39m ensure_platform_int(comp_ids)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:749\u001B[0m, in \u001B[0;36mBaseGrouper._get_compressed_codes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_compressed_codes\u001B[39m(\n\u001B[0;32m    745\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    746\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger], npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39mintp]]:\n\u001B[0;32m    747\u001B[0m     \u001B[38;5;66;03m# The first returned ndarray may have any signed integer dtype\u001B[39;00m\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroupings) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 749\u001B[0m         group_index \u001B[38;5;241m=\u001B[39m get_group_index(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcodes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, xnull\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    750\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m compress_group_index(group_index, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sort)\n\u001B[0;32m    751\u001B[0m         \u001B[38;5;66;03m# FIXME: compress_group_index's second return value is int64, not intp\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:675\u001B[0m, in \u001B[0;36mBaseGrouper.codes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    674\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger]]:\n\u001B[1;32m--> 675\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [ping\u001B[38;5;241m.\u001B[39mcodes \u001B[38;5;28;01mfor\u001B[39;00m ping \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroupings]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:675\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    674\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger]]:\n\u001B[1;32m--> 675\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [ping\u001B[38;5;241m.\u001B[39mcodes \u001B[38;5;28;01mfor\u001B[39;00m ping \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroupings]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:691\u001B[0m, in \u001B[0;36mGrouping.codes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    689\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger]:\n\u001B[1;32m--> 691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_codes_and_uniques[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:801\u001B[0m, in \u001B[0;36mGrouping._codes_and_uniques\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    796\u001B[0m     uniques \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_uniques\n\u001B[0;32m    797\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    798\u001B[0m     \u001B[38;5;66;03m# GH35667, replace dropna=False with use_na_sentinel=False\u001B[39;00m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;66;03m# error: Incompatible types in assignment (expression has type \"Union[\u001B[39;00m\n\u001B[0;32m    800\u001B[0m     \u001B[38;5;66;03m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001B[39;00m\n\u001B[1;32m--> 801\u001B[0m     codes, uniques \u001B[38;5;241m=\u001B[39m algorithms\u001B[38;5;241m.\u001B[39mfactorize(  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m    802\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrouping_vector, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sort, use_na_sentinel\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dropna\n\u001B[0;32m    803\u001B[0m     )\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m codes, uniques\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:795\u001B[0m, in \u001B[0;36mfactorize\u001B[1;34m(values, sort, use_na_sentinel, size_hint)\u001B[0m\n\u001B[0;32m    792\u001B[0m             \u001B[38;5;66;03m# Don't modify (potentially user-provided) array\u001B[39;00m\n\u001B[0;32m    793\u001B[0m             values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(null_mask, na_value, values)\n\u001B[1;32m--> 795\u001B[0m     codes, uniques \u001B[38;5;241m=\u001B[39m factorize_array(\n\u001B[0;32m    796\u001B[0m         values,\n\u001B[0;32m    797\u001B[0m         use_na_sentinel\u001B[38;5;241m=\u001B[39muse_na_sentinel,\n\u001B[0;32m    798\u001B[0m         size_hint\u001B[38;5;241m=\u001B[39msize_hint,\n\u001B[0;32m    799\u001B[0m     )\n\u001B[0;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    802\u001B[0m     uniques, codes \u001B[38;5;241m=\u001B[39m safe_sort(\n\u001B[0;32m    803\u001B[0m         uniques,\n\u001B[0;32m    804\u001B[0m         codes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m         verify\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    808\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:595\u001B[0m, in \u001B[0;36mfactorize_array\u001B[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001B[0m\n\u001B[0;32m    592\u001B[0m hash_klass, values \u001B[38;5;241m=\u001B[39m _get_hashtable_algo(values)\n\u001B[0;32m    594\u001B[0m table \u001B[38;5;241m=\u001B[39m hash_klass(size_hint \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values))\n\u001B[1;32m--> 595\u001B[0m uniques, codes \u001B[38;5;241m=\u001B[39m table\u001B[38;5;241m.\u001B[39mfactorize(\n\u001B[0;32m    596\u001B[0m     values,\n\u001B[0;32m    597\u001B[0m     na_sentinel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    598\u001B[0m     na_value\u001B[38;5;241m=\u001B[39mna_value,\n\u001B[0;32m    599\u001B[0m     mask\u001B[38;5;241m=\u001B[39mmask,\n\u001B[0;32m    600\u001B[0m     ignore_na\u001B[38;5;241m=\u001B[39muse_na_sentinel,\n\u001B[0;32m    601\u001B[0m )\n\u001B[0;32m    603\u001B[0m \u001B[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001B[39;00m\n\u001B[0;32m    604\u001B[0m uniques \u001B[38;5;241m=\u001B[39m _reconstruct_data(uniques, original\u001B[38;5;241m.\u001B[39mdtype, original)\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7280\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7194\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Pooling Investors By Type/Size---------------------------\\n')\n",
    "df_holding_binned, df_aum_binned = bin_concentrated_inv(df_inside, df_inv_aum, min_n_holding)\n",
    "df_holding_binned.to_csv(os.path.join(output_path, 'df_holding_binned.csv'))\n",
    "df_aum_binned.to_csv(os.path.join(output_path, 'df_aum_binned.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T07:48:27.308319900Z",
     "start_time": "2024-03-17T07:48:26.910382700Z"
    }
   },
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Tracking Investment Universe---------------------------\\n')\n",
    "df_inv_universe = calc_inv_universe(df_holding_binned, n_quarters)\n",
    "df_inv_universe.to_csv(os.path.join(output_path, 'df_inv_universe.csv'))\n",
    "\n",
    "print('\\n---------------Calculating Instrument---------------------------\\n')\n",
    "df_instrument = create_instrument(df_inv_universe, df_aum_binned)\n",
    "df_instrument.to_csv(os.path.join(output_path, 'df_instrument.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:25:20.216641100Z",
     "start_time": "2024-03-17T06:25:20.099383Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Holding Weights---------------------------\\n')\n",
    "df_weights = calc_holding_weights(df_instrument, df_holding_binned, min_n_holding)\n",
    "df_weights.to_csv(os.path.join(output_path, 'df_weights.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.101790500Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\n---------------Estimating Demand System---------------------------\\n')\n",
    "df_model = estimate_model(df_weights, characteristics, params)\n",
    "df_model.to_csv(os.path.join(output_path, 'df_model.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.104966200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Latent Demand---------------------------\\n')\n",
    "df_results = calc_latent_demand(df_model, characteristics, params)\n",
    "df_results.to_csv(os.path.join(output_path, 'df_results.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.106010500Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Moment Condition---------------------------\\n')\n",
    "df_mom = check_moment_condition(df_results[df_results['uni_size'] > df_results['n_holding']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.110217100Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Instrument Validity---------------------------\\n')\n",
    "critical_value_test(df_results, characteristics, figure_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.112444200Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Hypothetical Index Fund---------------------------\\n')\n",
    "test_index_fund(df_results, characteristics, params, figure_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.115639800Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('\\n---------------Creating Figures---------------------------\\n')\n",
    "graph_type_params(df_results, params, figure_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.118821500Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "graph_std_latent_demand(df_results, figure_path)\n",
    "print('\\n---------------Finished---------------------------\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.120974300Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-17T06:25:20.124178300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
