{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.438951200Z",
     "start_time": "2024-03-30T01:37:50.661318400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "import wrds\n",
    "import os\n",
    "from statsmodels.sandbox.regression import gmm\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.479150100Z",
     "start_time": "2024-03-30T01:37:52.454179500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "def request_wrds(path: str,\n",
    "                 start_date: pd.Timestamp,\n",
    "                 end_date: pd.Timestamp) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    with wrds.Connection() as db:\n",
    "        df_s12 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s12\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s12.to_csv(f'{path}s12.csv')\n",
    "\n",
    "        df_s12type5 = db.raw_sql(f'''\n",
    "                    SELECT *\n",
    "                    FROM tfn.s12type5\n",
    "                    WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "                ''', date_cols=['fdate'])\n",
    "        df_s12type5.to_csv(f'{path}df_s12type5.csv')\n",
    "    \n",
    "        df_s34 = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM tfn.s34\n",
    "            WHERE fdate >= '{start_date}' AND fdate <= '{end_date}'\n",
    "        ''', date_cols=['fdate'])\n",
    "        df_s34.to_csv(f'{path}s34.csv')\n",
    "\n",
    "        df_beta = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM beta.ff3\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_beta.to_csv(f'{path}security.csv')\n",
    "\n",
    "        df_security = db.raw_sql(f'''\n",
    "            SELECT *\n",
    "            FROM crspm.crspm_stock\n",
    "            WHERE date >= '{start_date}' AND date <= '{end_date}'\n",
    "        ''', date_cols=['date'])\n",
    "        df_security.to_csv(f'{path}security.csv')\n",
    "\n",
    "        return df_s12, df_s34, df_security\n",
    "\n",
    "\n",
    "def load_wrds(path: str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    try:\n",
    "        df_s12 = pd.read_csv(f'{path}s12.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12(df_s12)\n",
    "\n",
    "        df_s12type5 = pd.read_csv(f'{path}s12type5.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s12type5(df_s12type5)\n",
    "\n",
    "        df_s34 = pd.read_csv(f'{path}s34.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_s34(df_s34)\n",
    "\n",
    "        df_beta = pd.read_csv(f'{path}beta.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_beta(df_beta)\n",
    "\n",
    "        df_security = pd.read_csv(f'{path}security.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "        log_import_security(df_security)\n",
    "\n",
    "        return df_s12, df_s12type5, df_s34, df_beta, df_security\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return request_wrds(path, start_date, end_date)\n",
    "\n",
    "\n",
    "def clean_imports(df_s12,\n",
    "                  df_s12type5,\n",
    "                  df_s34,\n",
    "                  df_beta,\n",
    "                  df_security,\n",
    "                  start_date,\n",
    "                  end_date\n",
    "                  ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "\n",
    "    df_s12_clean = clean_s12(df_s12, start_date, end_date)\n",
    "    log_clean_s12(df_s12_clean)\n",
    "\n",
    "    df_s12type5_clean = clean_s12type5(df_s12type5, start_date, end_date)\n",
    "    log_clean_s12type5(df_s12type5_clean)\n",
    "\n",
    "    df_s34_clean = clean_s34(df_s34, start_date, end_date)\n",
    "    log_clean_s34(df_s34_clean)\n",
    "\n",
    "    df_beta_clean = clean_beta(df_beta, start_date, end_date)\n",
    "    log_clean_beta(df_beta_clean)\n",
    "\n",
    "    df_security_clean = clean_security(df_security, start_date, end_date)\n",
    "    log_clean_security(df_security_clean)\n",
    "\n",
    "    return df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean\n",
    "\n",
    "\n",
    "def clean_s12(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'fundno',\n",
    "        'rdate',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['fundno', 'shares', 'cusip'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'fundno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(date=lambda x: fix_date(x['date']))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_s12type5(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    return (df.rename(columns={'fdate': 'date', 'fundno': 'inv_id'})\n",
    "            .assign(date=lambda x: fix_date(x['date']))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .dropna(how='any', subset=['inv_id', 'date'])\n",
    "            .set_index(['inv_id', 'date']))\n",
    "\n",
    "\n",
    "def clean_s34(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'mgrno',\n",
    "        'rdate',\n",
    "        'typecode',\n",
    "        'cusip',\n",
    "        'shares'\n",
    "    ]\n",
    "    return (df[columns]\n",
    "            .dropna(how='any', subset=['mgrno', 'rdate', 'cusip', 'shares'])\n",
    "            .rename(columns={\n",
    "                'rdate': 'date',\n",
    "                'mgrno': 'inv_id',\n",
    "                'cusip': 'asset_id'})\n",
    "            .assign(date=lambda x: fix_date(x['date']))\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "            .set_index(['inv_id', 'date', 'asset_id']))\n",
    "\n",
    "\n",
    "def clean_beta(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns=[\n",
    "        'PERMNO',\n",
    "        'DATE',\n",
    "        'b_mkt',\n",
    "        'b_smb',\n",
    "        'b_hml'\n",
    "    ]\n",
    "    offset = 6\n",
    "    return (df[columns]\n",
    "            .dropna()\n",
    "            .rename(columns={\n",
    "                'DATE': 'date',\n",
    "                'PERMNO':'permno'})\n",
    "            .assign(\n",
    "                date=lambda x: fix_date(x['date']) + offset)\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "def clean_security(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    columns = [\n",
    "        'LPERMNO',\n",
    "        'cusip',\n",
    "        'datadate',\n",
    "        'prccm',\n",
    "        'trt1m',\n",
    "        'cshoq'\n",
    "    ]\n",
    "    offset = 6\n",
    "    return (df[columns]\n",
    "            .rename(columns={\n",
    "                'LPERMNO': 'permno',\n",
    "                'cusip': 'asset_id',\n",
    "                'prccm': 'prc',\n",
    "                'cshoq': 'shrout',\n",
    "                'datadate': 'date'})\n",
    "            .assign(\n",
    "                shrout=lambda x: x.groupby('asset_id')['shrout'].ffill() * 1000000,\n",
    "                asset_id=lambda x: x['asset_id'].apply(lambda s: s[:-1]),\n",
    "                date=lambda x: fix_date(x['date']) + offset)\n",
    "            .dropna(how='any', subset=['asset_id', 'prc', 'shrout'])\n",
    "            .loc[lambda x: (x['date'] >= start_date) & (x['date'] <= end_date)]\n",
    "            .drop_duplicates(subset=['date', 'permno'], keep='last')\n",
    "            .set_index(['date', 'permno']))\n",
    "\n",
    "\n",
    "# def fix_date(dates: pd.Series) -> pd.Series:\n",
    "#     return pd.to_datetime(dates) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "\n",
    "def fix_date(dates: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(dates).dt.to_period(freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.584882300Z",
     "start_time": "2024-03-30T01:37:52.464451400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stocks Monthly\n",
    "\n",
    "def merge_assets_factors(df_assets: pd.DataFrame, df_factors: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_assets,\n",
    "        right=df_factors,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        indicator=True)\n",
    "\n",
    "    df_merged_indexed = (df_merged\n",
    "                         .reset_index()\n",
    "                         .assign(date=lambda x: x['date'].dt.asfreq('Q'))\n",
    "                         .drop_duplicates(subset=['date', 'asset_id'], keep='last')\n",
    "                         .drop(columns=['permno'])\n",
    "                         .set_index(['date', 'asset_id']))\n",
    "\n",
    "    log_asset_merge(df_merged_indexed)\n",
    "    return df_merged_indexed\n",
    "\n",
    "\n",
    "# Manager / Holdings\n",
    "\n",
    "def match_fund_manager(df_fund: pd.DataFrame, df_manager: pd.DataFrame, df_key: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fund_manager = (df_manager\n",
    "                       .reset_index()\n",
    "                       .assign(date=lambda x: x['date'].dt.asfreq(freq='Q'))\n",
    "                       .drop_duplicates(subset=['inv_id', 'date', 'asset_id'], keep='last')\n",
    "                       .set_index(['inv_id', 'date', 'asset_id']))\n",
    "    \n",
    "    log_holding_merge(df_fund_manager)\n",
    "    return df_fund_manager\n",
    "\n",
    "\n",
    "def construct_zero_holdings(df_fund_manager: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    \n",
    "    def calc_inv_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.groupby('inv_id')['date'].transform('max') - df['date']).apply(lambda x: x.n)\n",
    "        min_diff = np.minimum(date_diff, n_quarters) + 1 \n",
    "        return min_diff\n",
    "    \n",
    "    def calc_asset_obs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        date_diff = (df.sort_values('date')\n",
    "                     .groupby(['inv_id', 'asset_id'])\n",
    "                     ['date']\n",
    "                     .diff(periods=1)\n",
    "                     .fillna(pd.DateOffset(n=1))\n",
    "                     .apply(lambda x: x.n))\n",
    "        min_diff = np.minimum(date_diff, df['inv_obs'])\n",
    "        return min_diff.apply(lambda x: list(range(x)))\n",
    "    \n",
    "    df_holding = (df_fund_manager\n",
    "                  .reset_index()\n",
    "                  .assign(\n",
    "                    inv_obs=lambda x: calc_inv_obs(x),\n",
    "                    asset_obs=lambda x: calc_asset_obs(x))\n",
    "                  .explode('asset_obs')\n",
    "                  .assign(\n",
    "                    mask=lambda x: x['asset_obs'] == 0,\n",
    "                    shares=lambda x: x['shares'] * x['mask'],\n",
    "                    date=lambda x: x['date'] + x['asset_obs'])\n",
    "                  .drop(columns=['inv_obs', 'asset_obs', 'mask'])\n",
    "                  .set_index(['inv_id', 'date', 'asset_id']))\n",
    "        \n",
    "    log_zero_holdings(df_holding)\n",
    "    return df_holding\n",
    "\n",
    "\n",
    "def merge_holding_factor(df_holding: pd.DataFrame, df_asset: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_merged = pd.merge(\n",
    "        left=df_holding,\n",
    "        right=df_asset,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "\n",
    "    df_holding_factor = (df_merged\n",
    "                         .assign(\n",
    "                           shares=lambda x: np.minimum(x['shares'], x['shrout']),\n",
    "                           holding=lambda x: x['prc'] * x['shares'] / 1000000,\n",
    "                           me=lambda x: x['prc'] * x['shrout'] / 1000000,\n",
    "                           typecode=lambda x: x['typecode'].fillna(0).astype('int8'))\n",
    "                         .dropna(subset='holding')\n",
    "                         .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    \n",
    "    log_holding_factor_merge(df_holding_factor)\n",
    "    return df_holding_factor\n",
    "\n",
    "\n",
    "def create_household_sector(df_holding_factor: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_household = (df_holding_factor\n",
    "                      .groupby(['date', 'asset_id'])\n",
    "                      .agg({\n",
    "                        'shares': 'sum',\n",
    "                        'prc': 'last',\n",
    "                        'shrout': 'last',\n",
    "                        'b_mkt': 'last',\n",
    "                        'b_smb': 'last',\n",
    "                        'b_hml': 'last',\n",
    "                        'holding': 'sum',\n",
    "                        'me': 'last',\n",
    "                        '_merge': 'last'})\n",
    "                      .assign(\n",
    "                        shares=lambda x: np.maximum(x['shrout'] - x['shares'], 0),\n",
    "                        holding=lambda x: np.maximum(x['me'] - x['holding'], 0),\n",
    "                        inv_id=0,\n",
    "                        typecode=0)\n",
    "                    .set_index('inv_id', append=True)\n",
    "                    .reorder_levels(['inv_id', 'date', 'asset_id']))\n",
    "    \n",
    "    log_household_sector(df_household)\n",
    "    df_concat = pd.concat([df_holding_factor, df_household])\n",
    "    return df_concat\n",
    "\n",
    "\n",
    "def create_outside_asset(df_household: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    mask = df_household['_merge'] == 'both'\n",
    "    df_inside = df_household[mask].drop(columns='_merge')\n",
    "    df_outside = (df_household[~mask]\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({\n",
    "                    'typecode': 'last',\n",
    "                    'holding': 'sum'})\n",
    "                  .assign(asset_id='-1')\n",
    "                  .set_index('asset_id', append=True))\n",
    "    \n",
    "    log_outside_asset(df_outside)\n",
    "    return df_inside, df_outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.623218Z",
     "start_time": "2024-03-30T01:37:52.497575600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "def calc_inv_aum(df_inside: pd.DataFrame, df_outside: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_in_aum = (df_inside\n",
    "                 .groupby(['inv_id', 'date'])\n",
    "                 .agg({\n",
    "                   'holding': 'sum',\n",
    "                   'shares': 'count',\n",
    "                   'typecode': 'last'})\n",
    "                 .rename(columns={\n",
    "                   'holding': 'in_aum',\n",
    "                   'shares': 'n_holding'}))\n",
    "\n",
    "    df_out_aum = (df_outside\n",
    "                  .groupby(['inv_id', 'date'])\n",
    "                  .agg({\n",
    "                    'holding': 'sum',\n",
    "                    'typecode': 'last'})\n",
    "                  .rename(columns={'holding': 'out_aum'}))\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        left=df_in_aum,\n",
    "        right=df_out_aum,\n",
    "        how='outer',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    \n",
    "    df_inv_aum = (df_merged\n",
    "                  .reset_index()\n",
    "                  .assign(\n",
    "                    typecode=lambda x: x['typecode_x'].fillna(x['typecode_y']),\n",
    "                    n_holding=lambda x: x['n_holding'].fillna(0),\n",
    "                    out_aum=lambda x: x['out_aum'].fillna(0),\n",
    "                    in_aum=lambda x: x['in_aum'].fillna(0),\n",
    "                    aum=lambda x: x['out_aum'] + x['in_aum'],\n",
    "                    out_weight=lambda x: x['out_aum'] / x['aum'],\n",
    "                    hh_mask=lambda x: (x['inv_id'] == 0) | (x['out_aum'] == 0) | (x['in_aum'] == 0) | (x['aum'] < 10),\n",
    "                    inv_id=lambda x: x['hh_mask'] * x['inv_id'],\n",
    "                    equal_alloc=lambda x: x['hh_mask'] * x['aum'] / (1 + x['n_holding']))\n",
    "                  .drop(columns=['typecode_x', 'typecode_y', 'hh_mask'])\n",
    "                  .set_index(['inv_id', 'date']))\n",
    "\n",
    "    log_inv_aum(df_inv_aum)\n",
    "    return df_inv_aum\n",
    "\n",
    "\n",
    "def bin_concentrated_inv(df_inside: pd.DataFrame, df_inv_aum: pd.DataFrame, min_n_holding: int) -> (pd.DataFrame, pd.DataFrame):\n",
    "    log_bins(df_inside, df_inv_aum)\n",
    "    return df_inside, df_inv_aum\n",
    "    household_mask = (df_inv_aum['out_aum'] == 0) | (df_inv_aum['in_aum'] == 0) | (df_inv_aum['aum'] < 10)\n",
    "    df_valid = (df_inv_aum\n",
    "                .reset_index()\n",
    "                .assign(\n",
    "                    inv_id=lambda x: x['inv_id'].mask(household_mask.values, '0'),\n",
    "                    typecode=lambda x: x['typecode'].mask(household_mask.values, 0))\n",
    "                .set_index(['inv_id', 'date']))\n",
    "    \n",
    "    diversified_mask = df_valid['n_holding'] >= min_n_holding\n",
    "    df_aum_diversified = df_valid[diversified_mask]\n",
    "    \n",
    "    def calc_bin(df_date_type: pd.DataFrame) -> pd.Series:\n",
    "        typecode = df_date_type['typecode'].iloc[0]\n",
    "        n_bins = np.ceil(df_date_type['n_holding'].sum() / (2 * min_n_holding)).astype(int)\n",
    "        if (n_bins <= 1) or (typecode == 0):\n",
    "            return pd.Series('0', index=df_date_type.index)\n",
    "        else:\n",
    "            return pd.qcut(x=df_date_type['aum'], q=n_bins, labels=False).apply(fix_qcut_bin, args=(str(typecode),))\n",
    "\n",
    "    def fix_qcut_bin(bin: int, typecode: str) -> str:\n",
    "        return typecode + ':' + str(bin)\n",
    "    \n",
    "    df_aum_concentrated = df_valid[~diversified_mask].assign(\n",
    "        bin = lambda x: x.groupby(['date', 'typecode']).apply(calc_bin).reset_index(drop=True))\n",
    "    df_concentrated_binned = (df_aum_concentrated\n",
    "                              .groupby(['bin', 'date'])\n",
    "                              .agg({\n",
    "                                'in_aum': 'sum',\n",
    "                                'out_aum': 'sum',\n",
    "                                'aum': 'sum',\n",
    "                                'typecode': 'last'})\n",
    "                              .rename_axis(index={'bin': 'inv_id'}))\n",
    "    df_aum_binned = (pd.concat([\n",
    "                        df_aum_diversified, \n",
    "                        df_concentrated_binned])\n",
    "                     .assign(\n",
    "                       out_weight=lambda x: x['out_aum'] / x['aum']))\n",
    "    df_inside_merged = pd.merge(\n",
    "        left=df_inside,\n",
    "        right=df_aum_concentrated['bin'],\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    df_inside_binned = (df_inside_merged\n",
    "                        .reset_index()\n",
    "                        .assign(bin=lambda x: x['bin'].fillna(x['inv_id']))\n",
    "                        .groupby(['bin', 'date', 'asset_id'])\n",
    "                        .agg({\n",
    "                          'holding': 'sum',\n",
    "                          'prc': 'last',\n",
    "                          'shrout': 'last',\n",
    "                          'me': 'last',\n",
    "                          'b_mkt': 'last',\n",
    "                          'b_smb': 'last',\n",
    "                          'b_hml': 'last'})\n",
    "                        .rename_axis(index={'bin': 'inv_id'}))\n",
    "    \n",
    "    log_bins(df_inside_binned, df_aum_binned)\n",
    "    return df_inside_binned, df_aum_binned\n",
    "\n",
    "\n",
    "def assetid_byinv(df_holding: pd.DataFrame) -> pd.Series:\n",
    "    return df_holding.reset_index('asset_id')['asset_id']\n",
    "\n",
    "\n",
    "def calc_inv_universe(df_holding: pd.DataFrame, n_quarters: int) -> pd.DataFrame:\n",
    "    df_assetid_byinv = assetid_byinv(df_holding).sort_index()\n",
    "    idx_inv_universe = df_assetid_byinv.index.unique()\n",
    "    df_inv_universe = pd.DataFrame(index=idx_inv_universe, columns=['inv_universe'])\n",
    "    offset = pd.DateOffset(months=3 * n_quarters)\n",
    "\n",
    "    def calc_past_quarters(i: int, d: pd.Timestamp) -> np.array:\n",
    "        prev_date = d - offset\n",
    "        asset_id_within_range = df_assetid_byinv.loc[i].loc[prev_date:d]\n",
    "        inv_uni = asset_id_within_range.unique().tolist()\n",
    "        return inv_uni\n",
    "\n",
    "    for (inv_id, date) in idx_inv_universe.to_flat_index():\n",
    "        inv_uni = calc_past_quarters(inv_id, date)\n",
    "        df_inv_universe.loc[(inv_id, date), 'inv_universe'] = inv_uni\n",
    "\n",
    "    df_inv_universe = df_inv_universe.assign(uni_size=lambda x: x['inv_universe'].apply(len))\n",
    "    \n",
    "    log_inv_universe(df_inv_universe)\n",
    "    return df_inv_universe\n",
    "\n",
    "\n",
    "def create_equal_allocation(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_alloc = pd.merge(\n",
    "        left=df_inv_universe,\n",
    "        right=df_aum_binned,\n",
    "        how='inner',\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    return (df_equal_alloc\n",
    "            .assign(\n",
    "                valid=lambda x: x.index.get_level_values('inv_id') != '0',\n",
    "                allocation=lambda x: x['aum'] / (x['uni_size'] + 1))\n",
    "            .explode('inv_universe')\n",
    "            .rename(columns={'inv_universe':'asset_id'})\n",
    "            .set_index('asset_id', append=True))\n",
    "\n",
    "\n",
    "def create_total_allocation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df['allocation']\n",
    "            .groupby('asset_id')\n",
    "            .sum())\n",
    "\n",
    "\n",
    "def create_instrument(df_inv_universe: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_equal_allocation = create_equal_allocation(df_inv_universe, df_aum_binned)\n",
    "    total_allocation = create_total_allocation(df_equal_allocation)\n",
    "    df_instrument = (df_equal_allocation\n",
    "                     .assign(iv_me=lambda x: total_allocation - x['allocation'])\n",
    "                     .drop(columns=['allocation', 'valid', 'typecode']))\n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument\n",
    "\n",
    "\n",
    "def calc_instrument(df_inside_binned: pd.DataFrame, df_aum_binned: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_inside_binned = dd.from_pandas(df_inside_binned.reset_index())\n",
    "    df_aum_binned = dd.from_pandas(df_aum_binned.reset_index())\n",
    "    \n",
    "    df_merged = df_inside_binned.merge(\n",
    "        right=df_aum_binned.drop(columns=['typecode']),\n",
    "        how='inner',\n",
    "        on=['inv_id', 'date'])\n",
    "    \n",
    "    # df_merged = pd.merge(\n",
    "    #     left=df_inside_binned,\n",
    "    #     right=df_aum_binned.drop(columns=['typecode']),\n",
    "    #     how='inner',\n",
    "    #     on=['inv_id', 'date'])\n",
    "        \n",
    "    df_instrument = (df_merged\n",
    "                     .assign(\n",
    "                        total_alloc=lambda x: x.groupby(['date', 'asset_id'])['equal_alloc'].transform('sum', meta=pd.Series(dtype='int8')),\n",
    "                        iv_me=lambda x: x['total_alloc'] - x['equal_alloc'])\n",
    "                     .drop(columns=['total_alloc'])\n",
    "                     .compute()\n",
    "                     .set_index(['inv_id', 'date', 'asset_id']))\n",
    "    \n",
    "    log_instrument(df_instrument)\n",
    "    return df_instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.669457500Z",
     "start_time": "2024-03-30T01:37:52.516410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimation\n",
    "\n",
    "def calc_holding_weights(df_instrument: pd.DataFrame, min_n_holding: int) -> pd.DataFrame:\n",
    "    mask = (df_instrument['n_holding'] >= min_n_holding) & (df_instrument['iv_me'] > 0)\n",
    "\n",
    "    df_weights = (df_instrument.loc[mask]\n",
    "                 .assign(\n",
    "                   ln_me=lambda x: np.log(x['me']),\n",
    "                   ln_iv_me=lambda x: np.log(x['iv_me']),\n",
    "                   weight=lambda x: x['holding'] / x['aum'],\n",
    "                   rweight=lambda x: x['weight'] / x['out_weight'],\n",
    "                   ln_rweight=lambda x: np.log(x['rweight']),\n",
    "                   mean_ln_rweight=lambda x: x['ln_rweight'].groupby(['inv_id', 'date']).transform('mean'),\n",
    "                   const=1)\n",
    "                .drop(columns=['me', 'iv_me', 'weight', 'rweight'])\n",
    "                .reset_index('asset_id'))\n",
    "    \n",
    "    log_holding_weights(df_weights)\n",
    "    return df_weights\n",
    "\n",
    "\n",
    "def unpack_result(result: gmm.GMMResults) -> dict:\n",
    "    return result.params\n",
    "\n",
    "\n",
    "def momcond_1(params, exog):\n",
    "    upper_bound = 0.999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    mean_ln_rweight = exog[1]\n",
    "    arr_characteristics = exog[2:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = ln_me_term + characteristics_term + mean_ln_rweight\n",
    "    \n",
    "    return pred_weight\n",
    "\n",
    "\n",
    "def momcond_2(params, exog):\n",
    "    upper_bound = 0.9999\n",
    "    exog = exog.T\n",
    "    \n",
    "    beta_ln_me = params[0]\n",
    "    beta_characteristics = params[1:]\n",
    "    \n",
    "    ln_me = exog[0]\n",
    "    rweight = exog[1]\n",
    "    mean_ln_rweight = exog[2]\n",
    "    arr_characteristics = exog[3:]\n",
    "    \n",
    "    ln_me_term = (upper_bound - np.exp(-1 * beta_ln_me)) * ln_me\n",
    "    characteristics_term = np.dot(beta_characteristics, arr_characteristics)\n",
    "    pred_weight = np.exp(-1 * (ln_me_term + characteristics_term + mean_ln_rweight))\n",
    "    \n",
    "    return rweight * pred_weight\n",
    "\n",
    "\n",
    "def estimate_model(df_weights: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    \n",
    "    def fit_inv_date_1(df_inv_date: pd.DataFrame) -> gmm.GMMResults:\n",
    "        exog = np.asarray(df_inv_date[['ln_me', 'mean_ln_rweight'] + characteristics])\n",
    "        instrument = np.asarray(df_inv_date[['ln_iv_me', 'mean_ln_rweight'] + characteristics])\n",
    "        n = exog.shape[0]\n",
    "        endog = np.asarray(df_inv_date['ln_rweight'])\n",
    "        start_params = np.zeros(len(params))\n",
    "        w0inv = np.dot(instrument.T, instrument) / n\n",
    "        \n",
    "        try:\n",
    "            model = gmm.NonlinearIVGMM(\n",
    "                endog=endog,\n",
    "                exog=exog,\n",
    "                instrument=instrument, \n",
    "                func=momcond_1)\n",
    "            result = model.fit(\n",
    "                start_params=start_params,\n",
    "                maxiter=0,\n",
    "                inv_weights=w0inv)\n",
    "            # log_results(result, params)\n",
    "            return result\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Linear Algebra Error')\n",
    "            print(f'Investor Id:  {df_inv_date.index.get_level_values(0).unique()}')\n",
    "            print(f'Date:  {df_inv_date.index.get_level_values(1).unique()}')\n",
    "            return np.NaN\n",
    "    \n",
    "    \n",
    "    def fit_inv_date_2(df_inv_date: pd.DataFrame) -> gmm.GMMResults:\n",
    "        exog = np.asarray(df_inv_date[['ln_me', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "        instrument = np.asarray(df_inv_date[['ln_iv_me', 'rweight', 'mean_ln_rweight'] + characteristics])\n",
    "        n = exog.shape[0]\n",
    "        endog = np.ones(n)\n",
    "        \n",
    "        try:\n",
    "            model = gmm.NonlinearIVGMM(\n",
    "                endog=endog,\n",
    "                exog=exog,\n",
    "                instrument=instrument, \n",
    "                func=momcond_1)\n",
    "            w0inv = np.dot(instrument.T, instrument) / n\n",
    "            start_params = np.zeros(len(params))\n",
    "            result = model.fit(\n",
    "                start_params=start_params,\n",
    "                maxiter=0,\n",
    "                inv_weights=w0inv)\n",
    "            log_results(result, params)\n",
    "            return result\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular Matrix Error')\n",
    "            print(f'Investor Id:  {df_inv_date.index.get_level_values(0).unique()}')\n",
    "            print(f'Date:  {df_inv_date.index.get_level_values(1).unique()}')\n",
    "            return np.NaN\n",
    "    \n",
    "    \n",
    "    df_model = (df_weights\n",
    "                .assign(\n",
    "                    gmm_result=lambda x: x.groupby(['inv_id', 'date']).apply(fit_inv_date_2),\n",
    "                    lst_params=lambda x: x['gmm_result'].apply(unpack_result)))\n",
    "    df_model[params] = pd.DataFrame(df_model['lst_params'].tolist(), index=df_model.index)\n",
    "    df_model = df_model.drop(columns='lst_params')\n",
    "    \n",
    "    log_params(df_model)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def calc_latent_demand(df_model: pd.DataFrame, characteristics: list, params: list) -> pd.DataFrame:\n",
    "    upper_bound = 0.9999\n",
    "    \n",
    "    df_results = df_model.assign(\n",
    "        beta_ln_me=lambda x: upper_bound - np.exp(-1 * x['beta_ln_me']),\n",
    "        beta_const=lambda x: x['beta_const'] + x['mean_ln_rweight'],\n",
    "        pred_ln_rweight=lambda x: np.einsum('ij,ij->i', x[['ln_me'] + characteristics], x[params]),\n",
    "        latent_demand=lambda x: x['ln_rweight'] - x['pred_ln_rweight'])\n",
    "    log_latent_demand(df_results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Liquidity\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.670717200Z",
     "start_time": "2024-03-30T01:37:52.524784500Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.672856400Z",
     "start_time": "2024-03-30T01:37:52.546569900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figures\n",
    "\n",
    "def clean_figures(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df_results\n",
    "            .assign(\n",
    "                date=lambda x: x['date'].dt.to_timestamp(),\n",
    "                typecode=lambda x: x['typecode'].apply(get_readable_typecode)))\n",
    "\n",
    "\n",
    "def check_moment_condition(df_results: pd.DataFrame):\n",
    "    df_mom = (df_results\n",
    "              .groupby(['inv_id', 'date'])\n",
    "              .agg({'latent_demand': 'mean'}))\n",
    "    \n",
    "    epsilon = 0.001\n",
    "    mask = (df_mom['latent_demand'] > 1 - epsilon) & (df_mom['latent_demand'] < 1 + epsilon)\n",
    "    valid_rate = len(df_mom[mask]) / len(df_mom)\n",
    "    print(f'Percentage of valid portfolios:  {100*valid_rate:.4f}%')\n",
    "    \n",
    "    g = sns.displot(\n",
    "        data=df_mom,\n",
    "        x='latent_demand'\n",
    "    )\n",
    "    g.set_axis_labels('Log Latent Demand', 'Density')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'moment_condition.png'))\n",
    "\n",
    "\n",
    "def critical_value_test(df_results: pd.DataFrame, characteristics: list, figure_path: str):\n",
    "    \n",
    "    def iv_reg(df_inv_date: pd.DataFrame):\n",
    "        y = df_inv_date['ln_me']\n",
    "        X = df_inv_date[['ln_iv_me'] + characteristics]\n",
    "        model = OLS(y, X)\n",
    "        result = model.fit()\n",
    "        t_stat = result.tvalues.iloc[0]\n",
    "        return t_stat\n",
    "    \n",
    "    df_iv = (df_results\n",
    "             .groupby(['inv_id', 'date'])\n",
    "             .apply(iv_reg)\n",
    "             .to_frame('t_stat')\n",
    "             .groupby('date')\n",
    "             .min()\n",
    "             .reset_index())\n",
    "    \n",
    "    g = sns.relplot(\n",
    "        data=df_iv, \n",
    "        x='date', \n",
    "        y='t_stat', \n",
    "        kind='line')\n",
    "    g.refline(\n",
    "        y=4.05,\n",
    "        linestyle='--')\n",
    "    g.set_axis_labels('Date', 'First stage t-statistic')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'instrument_validity.png'))\n",
    "\n",
    "\n",
    "def typecode_share_counts(df_inv_aum: pd.DataFrame, figure_path: str):\n",
    "    df_type_share = (df_inv_aum\n",
    "                     .assign(typecode=lambda x: x['typecode'].apply(get_readable_typecode))\n",
    "                     .groupby(['typecode', 'date'])\n",
    "                     .agg({'aum': 'sum', 'n_holding': 'count'})\n",
    "                     .assign(share=lambda x: x['aum'] / x['aum'].groupby('date').transform('sum')))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, x='date', y='n_holding', hue='typecode', kind='line')\n",
    "    g.set_axis_labels('Date', 'Number of Investors')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_count.png'))\n",
    "    \n",
    "    g = sns.relplot(data=df_type_share, x='date', y='share', hue='typecode', kind='line')\n",
    "    g.set_axis_labels('Date', 'Share of AUM')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'typecode_share.png'))\n",
    "\n",
    "\n",
    "def test_index_fund(df_results: pd.DataFrame, characteristics: list, params: list, figure_path: str):\n",
    "    index_id = '90457'\n",
    "    mask = df_results.index.get_level_values(0) == index_id\n",
    "    df_index_fund = (df_results\n",
    "                     .assign(\n",
    "                        ln_rweight=lambda x: x['ln_me'] + x['mean_ln_rweight'])\n",
    "                     .loc[mask, ['ln_me', 'ln_iv_me', 'ln_rweight', 'mean_ln_rweight'] + characteristics + params])\n",
    "    \n",
    "    df_index_fund_model = estimate_model(df_index_fund, characteristics, params)\n",
    "    df_index_fund_result = calc_latent_demand(df_index_fund_model, characteristics, params)\n",
    "    cols = params + ['latent_demand']\n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_index_fund_result[[param]],\n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.despine()\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.savefig(os.path.join(figure_path, f'index_fund_{param}.png'))\n",
    "    \n",
    "\n",
    "def graph_type_params(df_results: pd.DataFrame, params: list, figure_path: str):\n",
    "    cols = params + ['latent_demand']\n",
    "\n",
    "    df_types = df_results.copy()\n",
    "    df_types[cols] = df_types[cols].apply(lambda x: x * df_results['aum'])\n",
    "    df_types = (df_types\n",
    "                .groupby(['typecode', 'date'])\n",
    "                [cols]\n",
    "                .sum()\n",
    "                .apply(lambda x: x / df_results.groupby(['typecode', 'date'])['aum'].sum())\n",
    "                .reset_index()\n",
    "                .assign(typecode=lambda x: x['typecode'].apply(get_readable_typecode)))\n",
    "    \n",
    "    for param in cols:\n",
    "        g = sns.relplot(data=df_types, \n",
    "                        x='date',\n",
    "                        y=param,\n",
    "                        hue='typecode',\n",
    "                        kind='line')\n",
    "        g.set_axis_labels('Date', f'{get_readable_param(param)}')\n",
    "        g.legend.set_title('Institution Type')\n",
    "        g.despine()\n",
    "        plt.savefig(os.path.join(figure_path, f'{param}.png'))\n",
    "\n",
    "\n",
    "def graph_std_latent_demand(df_results: pd.DataFrame, figure_path: str):\n",
    "    df_ld = (df_results\n",
    "         .groupby(['inv_id', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'std',\n",
    "            'aum': 'last',\n",
    "            'typecode': 'last'})\n",
    "         .assign(latent_demand=lambda x: x['latent_demand'] * x['aum'])\n",
    "         .groupby(['typecode', 'date'])\n",
    "         .agg({\n",
    "            'latent_demand': 'mean',\n",
    "            'aum': 'sum'})\n",
    "         .assign(\n",
    "            latent_demand=lambda x: x['latent_demand'] / x['aum'], \n",
    "            typecode=lambda x: x['typecode'].apply(get_readable_typecode)))\n",
    "    \n",
    "    g = sns.relplot(data=df_ld, \n",
    "                    x='date',\n",
    "                    y='latent_demand',\n",
    "                    hue='typecode',\n",
    "                    kind='line')\n",
    "    g.set_axis_labels('Date', 'Standard Deviation of Latent Demand')\n",
    "    g.legend.set_title('Institution Type')\n",
    "    g.despine()\n",
    "    plt.savefig(os.path.join(figure_path, f'std_latent_demand.png'))\n",
    "\n",
    "\n",
    "def create_tables(df_instrument: pd.DataFrame, df_inv_uni: pd.DataFrame):\n",
    "    df_pctile = df_inv_uni.assign(pctile=0)\n",
    "    arr_dates = df_pctile.index.get_level_values('date').unique()\n",
    "    \n",
    "    for date in arr_dates:\n",
    "        data = df_pctile.loc[date, 'aum']\n",
    "        df_pctile.loc[date, 'pctile'] = pd.qcut(data, q=100)\n",
    "        \n",
    "    df_grouped_pctile = (df_pctile['uni_persistence']\n",
    "                         .groupby(['pctile'])\n",
    "                         .median())\n",
    "    print(df_grouped_pctile.head(10))\n",
    "\n",
    "    \n",
    "def get_param_cols(cols: list) -> list:\n",
    "    return ['beta_' + col for col in cols]\n",
    "\n",
    "\n",
    "def get_readable_param(name: str) -> str:\n",
    "    return name.replace('_', ' ').title()\n",
    "\n",
    "\n",
    "def get_readable_typecode(typecode: int):\n",
    "    return dict_typecode[typecode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.723178400Z",
     "start_time": "2024-03-30T01:37:52.553878100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log\n",
    "\n",
    "def log_import_s12(df_s12: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12['rdate']\n",
    "    print('Imported s12')\n",
    "    print('Number of holdings:  ', len(df_s12))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_import_s12type5(df_s12type5: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5['fdate']\n",
    "    print()\n",
    "    print('Imported s12type5')\n",
    "    print('Number of holdings:  ', len(df_s12type5))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_import_s34(df_s34: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34['rdate']\n",
    "    print()\n",
    "    print('Imported s34')\n",
    "    print('Number of holdings:  ', len(df_s34))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_import_beta(df_beta: pd.DataFrame):\n",
    "    dateindex_ffm = df_beta['DATE']\n",
    "    print()\n",
    "    print('Imported betas')\n",
    "    print('Number of dates:  ', len(df_beta))\n",
    "    print('Earliest date:  ', min(dateindex_ffm))\n",
    "    print('Latest date:  ', max(dateindex_ffm))\n",
    "\n",
    "\n",
    "def log_import_security(df_security: pd.DataFrame):\n",
    "    dateindex_security = df_security['datadate']\n",
    "    print()\n",
    "    print('Imported security')\n",
    "    print('Number of holdings:  ', len(df_security))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_clean_s12(df_s12_clean: pd.DataFrame):\n",
    "    dateindex_s12 = df_s12_clean.index.get_level_values('date')\n",
    "    print('Cleaned s12')\n",
    "    print('Number of holdings:  ', len(df_s12_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12))\n",
    "    print('Latest date:  ', max(dateindex_s12))\n",
    "\n",
    "\n",
    "def log_clean_s12type5(df_s12type5_clean: pd.DataFrame):\n",
    "    dateindex_s12type5 = df_s12type5_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s12type5')\n",
    "    print('Number of firm/dates:  ', len(df_s12type5_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s12type5))\n",
    "    print('Latest date:  ', max(dateindex_s12type5))\n",
    "\n",
    "\n",
    "def log_clean_s34(df_s34_clean: pd.DataFrame):\n",
    "    dateindex_s34 = df_s34_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned s34')\n",
    "    print('Number of holdings:  ', len(df_s34_clean))\n",
    "    print('Earliest date:  ', min(dateindex_s34))\n",
    "    print('Latest date:  ', max(dateindex_s34))\n",
    "\n",
    "\n",
    "def log_clean_beta(df_beta_clean: pd.DataFrame):\n",
    "    dateindex_beta = df_beta_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned beta')\n",
    "    print('Number of dates:  ', len(df_beta_clean))\n",
    "    print('Earliest date:  ', min(dateindex_beta))\n",
    "    print('Latest date:  ', max(dateindex_beta))\n",
    "\n",
    "\n",
    "def log_clean_security(df_security_clean: pd.DataFrame):\n",
    "    dateindex_security = df_security_clean.index.get_level_values('date')\n",
    "    print()\n",
    "    print('Cleaned security')\n",
    "    print('Number of asset/dates:  ', len(df_security_clean))\n",
    "    print('Earliest date:  ', min(dateindex_security))\n",
    "    print('Latest date:  ', max(dateindex_security))\n",
    "\n",
    "\n",
    "def log_holding_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged s12 and s34')\n",
    "    print('Number of holdings:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_asset_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged assets and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_holding_factor_merge(df_merged: pd.DataFrame):\n",
    "    print('Merged holdings and factors')\n",
    "    print('Number of assets/dates:  ', len(df_merged))\n",
    "    print()\n",
    "\n",
    "\n",
    "def log_household_sector(df_household: pd.DataFrame):\n",
    "    print('Created household sector')\n",
    "    print('Number of holdings:  ', len(df_household))\n",
    "\n",
    "\n",
    "def log_outside_asset(df_outside: pd.DataFrame):\n",
    "    print('Created outside asset')\n",
    "    print('Number of holdings:  ', len(df_outside))\n",
    "\n",
    "\n",
    "def log_zero_holdings(df_holding: pd.DataFrame):\n",
    "    print('Constructed zero holdings')\n",
    "    print('Number of non-zero holdings:  ', sum(df_holding['shares'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_holding['shares'] == 0))\n",
    "\n",
    "\n",
    "def log_inv_aum(df_inv_aum: pd.DataFrame):\n",
    "    idx = df_inv_aum.index.get_level_values('inv_id').unique()\n",
    "    print('Calculated investor AUM')\n",
    "    print('Number of investors:  ', len(idx))\n",
    "\n",
    "\n",
    "def log_bins(df_inside_binned: pd.DataFrame, df_aum_binned: pd.DataFrame):\n",
    "    print('Binned investors')\n",
    "    print('Number of investors by holding:  ', len(df_inside_binned.index.unique('inv_id')))\n",
    "    print('Number of investors by aum:  ', len(df_aum_binned.index.unique('inv_id')))\n",
    "\n",
    "\n",
    "def log_inv_universe(df_inv_uni: pd.DataFrame):\n",
    "    idx = df_inv_uni.index.get_level_values('inv_id').unique()\n",
    "    print('Created investment universe')\n",
    "    print('Number of investors:  ', len(idx))\n",
    "    print('Average investment universe size:  ', len(df_inv_uni['uni_size'].mean()))\n",
    "    \n",
    "\n",
    "def log_instrument(df_instrument: pd.DataFrame):\n",
    "    print('Created market equity instrument')\n",
    "    print('Number of valid instruments:  ', sum(df_instrument['iv_me'] > 0))\n",
    "    print('Number of invalid instruments:  ', sum(df_instrument['iv_me'] == 0))\n",
    "    \n",
    "\n",
    "def log_holding_weights(df_model: pd.DataFrame):\n",
    "    print('Calculated holding weights')\n",
    "    print('Number of non-zero holdings:  ', sum(df_model['holding'] > 0))\n",
    "    print('Number of zero holdings:  ', sum(df_model['holding'] == 0))\n",
    "\n",
    "\n",
    "def log_results(result, params):\n",
    "    print(result.summary(yname='Latent demand', xname=params))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def log_params(df_params: pd.DataFrame):\n",
    "    print('Estimated parameters')\n",
    "    print('Number of converged estimations:  ', sum(df_params['gmm_result'].notna()))\n",
    "    \n",
    "    \n",
    "def log_latent_demand(df_results: pd.DataFrame):\n",
    "    print('Calculated latent demand')\n",
    "    print('Number of investors:  ', len(df_results.index.get_level_values('inv_id').unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:37:52.724178800Z",
     "start_time": "2024-03-30T01:37:52.571071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "sns.set_theme(style='ticks', palette=sns.color_palette('hls', 6))\n",
    "\n",
    "input_path = 'data/'\n",
    "output_path = 'output/'\n",
    "figure_path = 'figures/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "os.makedirs(figure_path, exist_ok=True)\n",
    "\n",
    "start_date = pd.Period('2012-01')\n",
    "end_date = pd.Period('2017-12')\n",
    "\n",
    "characteristics = [\n",
    "    'b_mkt',\n",
    "    'b_smb',\n",
    "    'b_hml'\n",
    "] + ['const']\n",
    "params = ['beta_ln_me'] + get_param_cols(characteristics)\n",
    "dict_typecode = {\n",
    "     0: 'Households',\n",
    "     1: 'Banks',\n",
    "     2: 'Insurance companies',\n",
    "     3: 'Investment advisors',\n",
    "     4: 'Mutual funds',\n",
    "     5: 'Pension funds',\n",
    "}\n",
    "\n",
    "min_n_holding = 1000\n",
    "n_quarters = 11"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    dfs = load_wrds(input_path)\n",
    "    df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean = clean_imports(\n",
    "        *dfs,\n",
    "        start_date,\n",
    "        end_date\n",
    "    )\n",
    "\n",
    "    df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "    df_fund_manager = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "    df_holding = construct_zero_holdings(df_fund_manager, n_quarters)\n",
    "    df_holding_factor = merge_holding_factor(df_holding, df_asset)\n",
    "    df_household = create_household_sector(df_holding_factor)\n",
    "    df_inside, df_outside = create_outside_asset(df_household)\n",
    "    df_inv_aum = calc_inv_aum(df_inside, df_outside)\n",
    "    df_holding_binned, df_aum_binned = bin_concentrated_inv(df_inside, df_inv_aum, min_n_holding)\n",
    "    df_instrument = calc_instrument(df_holding_binned, df_aum_binned)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:39:37.253906300Z",
     "start_time": "2024-03-30T01:37:52.581784300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Imports---------------------------\n",
      "\n",
      "Imported s12\n",
      "Number of holdings:   30987\n",
      "Earliest date:   2014-06-30\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported s12type5\n",
      "Number of holdings:   572518\n",
      "Earliest date:   12/31/1994\n",
      "Latest date:   9/30/2022\n",
      "\n",
      "Imported s34\n",
      "Number of holdings:   22707709\n",
      "Earliest date:   2012-03-31\n",
      "Latest date:   2017-12-31\n",
      "\n",
      "Imported betas\n",
      "Number of dates:   432458\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-29\n",
      "\n",
      "Imported security\n",
      "Number of holdings:   402452\n",
      "Earliest date:   2012-01-31\n",
      "Latest date:   2017-12-31\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "print('\\n---------------Starting Imports---------------------------\\n')\n",
    "dfs = load_wrds(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Starting Cleaning---------------------------\n",
      "Cleaned s12\n",
      "Number of holdings:   27327\n",
      "Earliest date:   2014-06\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned s12type5\n",
      "Number of firm/dates:   49372\n",
      "Earliest date:   2012-03\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned s34\n",
      "Number of holdings:   22267070\n",
      "Earliest date:   2012-03\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned beta\n",
      "Number of dates:   394535\n",
      "Earliest date:   2012-07\n",
      "Latest date:   2017-12\n",
      "\n",
      "Cleaned security\n",
      "Number of asset/dates:   348924\n",
      "Earliest date:   2012-07\n",
      "Latest date:   2017-12\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Starting Cleaning---------------------------\\n')\n",
    "df_s12_clean, df_s12type5_clean, df_s34_clean, df_beta_clean, df_security_clean = clean_imports(\n",
    "    *dfs,\n",
    "    start_date,\n",
    "    end_date\n",
    ")\n",
    "\n",
    "# df_s12_clean.to_csv(os.path.join(output_path, 'df_s12_clean.csv'))\n",
    "# df_s12type5_clean.to_csv(os.path.join(output_path, 'df_s12type5_clean.csv'))\n",
    "# df_s34_clean.to_csv(os.path.join(output_path, 'df_s34_clean.csv'))\n",
    "# df_beta_clean.to_csv(os.path.join(output_path, 'df_beta_clean.csv'))\n",
    "# df_security_clean.to_csv(os.path.join(output_path, 'df_security_clean.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:41:18.201983600Z",
     "start_time": "2024-03-30T01:39:37.272192700Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Assets/Factors---------------------------\n",
      "Merged assets and factors\n",
      "Number of assets/dates:   118413\n",
      "\n",
      "\n",
      "---------------Merging s12/s34 Holdings---------------------------\n",
      "Merged s12 and s34\n",
      "Number of holdings:   22267070\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Assets/Factors---------------------------\\n')\n",
    "df_asset = merge_assets_factors(df_security_clean, df_beta_clean)\n",
    "# df_asset.to_csv(os.path.join(output_path, 'df_asset.csv'))\n",
    "\n",
    "print('\\n---------------Merging s12/s34 Holdings---------------------------\\n')\n",
    "df_fund_manager = match_fund_manager(df_s12_clean, df_s34_clean, df_s12type5_clean)\n",
    "# df_fund_manager.to_csv(os.path.join(output_path, 'df_fund_manager.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:42:09.488732500Z",
     "start_time": "2024-03-30T01:41:18.204122400Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:48:34.615519500Z",
     "start_time": "2024-03-30T01:42:09.488732500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Constructing Zero Holdings---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyatt\\AppData\\Local\\Temp\\ipykernel_1452\\556469650.py:62: PerformanceWarning: Adding/subtracting object-dtype array to PeriodArray not vectorized.\n",
      "  date=lambda x: x['date'] + x['asset_obs'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed zero holdings\n",
      "Number of non-zero holdings:   22267070\n",
      "Number of zero holdings:   2478516\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Constructing Zero Holdings---------------------------\\n')\n",
    "df_holding = construct_zero_holdings(df_fund_manager, n_quarters)\n",
    "# df_holding.to_csv(os.path.join(output_path, 'df_holding.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:49:21.294648300Z",
     "start_time": "2024-03-30T01:48:34.639502200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Merging Holdings/Factors---------------------------\n",
      "Merged holdings and factors\n",
      "Number of assets/dates:   18074588\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Merging Holdings/Factors---------------------------\\n')\n",
    "df_holding_factor = merge_holding_factor(df_holding, df_asset)\n",
    "# df_holding_factor.to_csv(os.path.join(output_path, 'df_holding_factor.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Creating Household Sector---------------------------\n",
      "Created household sector\n",
      "Number of holdings:   90925\n",
      "\n",
      "---------------Partitioning Outside Asset---------------------------\n",
      "Created outside asset\n",
      "Number of holdings:   67481\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Creating Household Sector---------------------------\\n')\n",
    "df_household = create_household_sector(df_holding_factor)\n",
    "# df_household.to_csv(os.path.join(output_path, 'df_household.csv'))\n",
    "\n",
    "print('\\n---------------Partitioning Outside Asset---------------------------\\n')\n",
    "df_inside, df_outside = create_outside_asset(df_household)\n",
    "# df_inside.to_csv(os.path.join(output_path, 'df_inside.csv'))\n",
    "# df_outside.to_csv(os.path.join(output_path, 'df_outside.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:49:27.281396300Z",
     "start_time": "2024-03-30T01:49:21.233866500Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Investor AUM---------------------------\n",
      "Calculated investor AUM\n",
      "Number of investors:   3410\n",
      "\n",
      "---------------Pooling Investors By Type/Size---------------------------\n",
      "\n",
      "Binned investors\n",
      "Number of investors by holding:   5895\n",
      "Number of investors by aum:   3410\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------Calculating Investor AUM---------------------------\\n')\n",
    "df_inv_aum = calc_inv_aum(df_inside, df_outside)\n",
    "# df_inv_aum.to_csv(os.path.join(output_path, 'df_inv_aum.csv'))\n",
    "\n",
    "print('\\n---------------Pooling Investors By Type/Size---------------------------\\n')\n",
    "df_holding_binned, df_aum_binned = bin_concentrated_inv(df_inside, df_inv_aum, min_n_holding)\n",
    "# df_holding_binned.to_csv(os.path.join(output_path, 'df_holding_binned.csv'))\n",
    "# df_aum_binned.to_csv(os.path.join(output_path, 'df_aum_binned.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:49:30.839195Z",
     "start_time": "2024-03-30T01:49:27.283543200Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T01:49:42.210281700Z",
     "start_time": "2024-03-30T01:49:30.839195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------Calculating Instrument---------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exactly one of npartitions and chunksize must be specified.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# print('\\n---------------Tracking Investment Universe---------------------------\\n')\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# df_inv_universe = calc_inv_universe(df_holding_binned, n_quarters)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# # df_inv_universe.to_csv(os.path.join(output_path, 'df_inv_universe.csv'))\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------Calculating Instrument---------------------------\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m df_instrument \u001B[38;5;241m=\u001B[39m calc_instrument(df_holding_binned, df_aum_binned)\n",
      "Cell \u001B[1;32mIn[4], line 170\u001B[0m, in \u001B[0;36mcalc_instrument\u001B[1;34m(df_inside_binned, df_aum_binned)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalc_instrument\u001B[39m(df_inside_binned: pd\u001B[38;5;241m.\u001B[39mDataFrame, df_aum_binned: pd\u001B[38;5;241m.\u001B[39mDataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m--> 170\u001B[0m     df_inside_binned \u001B[38;5;241m=\u001B[39m dd\u001B[38;5;241m.\u001B[39mfrom_pandas(df_inside_binned\u001B[38;5;241m.\u001B[39mreset_index())\n\u001B[0;32m    171\u001B[0m     df_aum_binned \u001B[38;5;241m=\u001B[39m dd\u001B[38;5;241m.\u001B[39mfrom_pandas(df_aum_binned\u001B[38;5;241m.\u001B[39mreset_index())\n\u001B[0;32m    173\u001B[0m     df_merged \u001B[38;5;241m=\u001B[39m df_inside_binned\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[0;32m    174\u001B[0m         right\u001B[38;5;241m=\u001B[39mdf_aum_binned\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtypecode\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[0;32m    175\u001B[0m         how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    176\u001B[0m         on\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minv_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\io.py:264\u001B[0m, in \u001B[0;36mfrom_pandas\u001B[1;34m(data, npartitions, chunksize, sort, name)\u001B[0m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput must be a pandas DataFrame or Series.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (npartitions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m (chunksize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExactly one of npartitions and chunksize must be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    266\u001B[0m nrows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mValueError\u001B[0m: Exactly one of npartitions and chunksize must be specified."
     ]
    }
   ],
   "source": [
    "# print('\\n---------------Tracking Investment Universe---------------------------\\n')\n",
    "# df_inv_universe = calc_inv_universe(df_holding_binned, n_quarters)\n",
    "# # df_inv_universe.to_csv(os.path.join(output_path, 'df_inv_universe.csv'))\n",
    "\n",
    "print('\\n---------------Calculating Instrument---------------------------\\n')\n",
    "df_instrument = calc_instrument(df_holding_binned, df_aum_binned)\n",
    "# df_instrument.to_csv(os.path.join(output_path, 'df_instrument.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.950535800Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Holding Weights---------------------------\\n')\n",
    "df_weights = calc_holding_weights(df_instrument, min_n_holding)\n",
    "# df_weights.to_csv(os.path.join(output_path, 'df_weights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.956033900Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Estimating Demand System---------------------------\\n')\n",
    "df_model = estimate_model(df_weights, characteristics, params)\n",
    "# df_model.to_csv(os.path.join(output_path, 'df_model.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.961679300Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Calculating Latent Demand---------------------------\\n')\n",
    "df_results = calc_latent_demand(df_model, characteristics, params)\n",
    "# df_results.to_csv(os.path.join(output_path, 'df_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.969530300Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Moment Condition---------------------------\\n')\n",
    "df_figures = clean_figures(df_results)\n",
    "check_moment_condition(df_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.975482Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Investors by Typecode---------------------------\\n')\n",
    "typecode_share_counts(df_inv_aum, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.980488600Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Instrument Validity---------------------------\\n')\n",
    "critical_value_test(df_figures, characteristics, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.985051500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Testing Hypothetical Index Fund---------------------------\\n')\n",
    "test_index_fund(df_figures, characteristics, params, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.989558800Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Parameters By Typecode---------------------------\\n')\n",
    "graph_type_params(df_figures, params, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:41.996484400Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Standard Deviation of Latent Demand By Typecode---------------------------\\n')\n",
    "graph_std_latent_demand(df_figures, figure_path)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:42.001178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-30T01:49:42.005566100Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\n---------------Finished---------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
